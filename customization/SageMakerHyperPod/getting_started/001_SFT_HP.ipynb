{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6b0946",
   "metadata": {},
   "source": [
    "## Supervised Fine Tuning usimng Hyperpod Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040c9c3",
   "metadata": {},
   "source": [
    "You can customize Amazon Nova models through base recipes using Amazon SageMaker Hyperpod jobs. These recipes support Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), with both Full-Rank and Low-Rank Adaptation (LoRA) options.\n",
    "\n",
    "The end-to-end customization workflow involves stages like model training, model evaluation, and deployment for inference. This model customization approach on SageMaker AI provides greater flexibility and control to fine-tune its supported Amazon Nova models, optimize hyperparameters with precision, and implement techniques including LoRA Parameter-Efficient Fine-Tuning (PEFT), Full-Rank Supervised Fine-Tuning, and Direct Preference Optimization (DPO).\n",
    "\n",
    "This notebook demonstrates Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) of Amazon Nova using Amazon SageMaker Training Job. SFT is a technique that allows fine-tuning language models on specific tasks using labeled examples, while PEFT enables efficient fine-tuning by updating only a small subset of the model's parameters.\n",
    "\n",
    "> _**Note:** This notebook demonstrates fine-tuning using Nova Lite, but the same techniques can be applied to Nova Pro or Nova Micro models with appropriate adjustments to the configuration._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c121ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple\n",
      "Requirement already satisfied: sagemaker in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from -r ./requirements.txt (line 1)) (2.245.1.dev0)\n",
      "Collecting sagemaker (from -r ./requirements.txt (line 1))\n",
      "  Using cached sagemaker-2.248.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: boto3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from -r ./requirements.txt (line 2)) (1.39.4)\n",
      "Collecting boto3 (from -r ./requirements.txt (line 2))\n",
      "  Downloading boto3-1.39.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: datasets==3.5.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from -r ./requirements.txt (line 3)) (3.5.1)\n",
      "Requirement already satisfied: sagemaker-mlflow==0.1.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from -r ./requirements.txt (line 4)) (0.1.0)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from -r ./requirements.txt (line 5)) (1.6.1)\n",
      "Requirement already satisfied: psutil in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from -r ./requirements.txt (line 6)) (7.0.0)\n",
      "Requirement already satisfied: py7zr in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from -r ./requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: filelock in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.5.1->-r ./requirements.txt (line 3)) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (0.26.2)\n",
      "Requirement already satisfied: packaging in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from datasets==3.5.1->-r ./requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: mlflow>=2.8 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from scikit-learn==1.6.1->-r ./requirements.txt (line 5)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from scikit-learn==1.6.1->-r ./requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from scikit-learn==1.6.1->-r ./requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: attrs<26,>=24 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (24.2.0)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: docker in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (0.115.9)\n",
      "Requirement already satisfied: google-pasta in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: pathos in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (4.3.7)\n",
      "Requirement already satisfied: protobuf<6.32,>=3.12 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (5.28.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (1.0.42)\n",
      "Requirement already satisfied: schema in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: uvicorn in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker->-r ./requirements.txt (line 1)) (0.29.0)\n",
      "Collecting botocore<1.40.0,>=1.39.6 (from boto3->-r ./requirements.txt (line 2))\n",
      "  Downloading botocore-1.39.6-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from boto3->-r ./requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from boto3->-r ./requirements.txt (line 2)) (0.13.0)\n",
      "Requirement already satisfied: texttable in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.20.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from py7zr->-r ./requirements.txt (line 7)) (3.23.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: pyzstd>=0.16.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from py7zr->-r ./requirements.txt (line 7)) (0.17.0)\n",
      "Requirement already satisfied: pyppmd<1.3.0,>=1.1.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.0.6)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from py7zr->-r ./requirements.txt (line 7)) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.0.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from botocore<1.40.0,>=1.39.6->boto3->-r ./requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from aiohttp->datasets==3.5.1->-r ./requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from aiohttp->datasets==3.5.1->-r ./requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from aiohttp->datasets==3.5.1->-r ./requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from aiohttp->datasets==3.5.1->-r ./requirements.txt (line 3)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from aiohttp->datasets==3.5.1->-r ./requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from aiohttp->datasets==3.5.1->-r ./requirements.txt (line 3)) (1.18.0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from graphene<4,>=3->sagemaker->-r ./requirements.txt (line 1)) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from graphene<4,>=3->sagemaker->-r ./requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from graphene<4,>=3->sagemaker->-r ./requirements.txt (line 1)) (4.13.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r ./requirements.txt (line 1)) (3.21.0)\n",
      "Requirement already satisfied: mlflow-skinny==3.1.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: Flask<4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.15.2)\n",
      "Requirement already satisfied: gunicorn<24 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (22.0.0)\n",
      "Requirement already satisfied: matplotlib<4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.9.2)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.0.35)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (8.1.8)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.58.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.43)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.31.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.11.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from fastapi->sagemaker->-r ./requirements.txt (line 1)) (0.41.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from omegaconf<3,>=2.2->sagemaker->-r ./requirements.txt (line 1)) (4.9.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pandas->datasets==3.5.1->-r ./requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pandas->datasets==3.5.1->-r ./requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from requests>=2.32.2->datasets==3.5.1->-r ./requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from requests>=2.32.2->datasets==3.5.1->-r ./requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from requests>=2.32.2->datasets==3.5.1->-r ./requirements.txt (line 3)) (2025.1.31)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r ./requirements.txt (line 1)) (14.0.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r ./requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jsonschema->sagemaker->-r ./requirements.txt (line 1)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jsonschema->sagemaker->-r ./requirements.txt (line 1)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jsonschema->sagemaker->-r ./requirements.txt (line 1)) (0.21.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from uvicorn->sagemaker->-r ./requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: six in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from google-pasta->sagemaker->-r ./requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pathos->sagemaker->-r ./requirements.txt (line 1)) (1.7.7)\n",
      "Requirement already satisfied: pox>=0.3.4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pathos->sagemaker->-r ./requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: Mako in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.3.9)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r ./requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r ./requirements.txt (line 1)) (2.18.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from starlette<0.46.0,>=0.40.0->fastapi->sagemaker->-r ./requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.46.0,>=0.40.0->fastapi->sagemaker->-r ./requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.36.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker->-r ./requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.2.15)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.52b0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.17.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.6.1)\n",
      "Downloading sagemaker-2.248.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.39.6-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.39.6-py3-none-any.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore, boto3, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.39.4\n",
      "    Uninstalling botocore-1.39.4:\n",
      "      Successfully uninstalled botocore-1.39.4\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.39.4\n",
      "    Uninstalling boto3-1.39.4:\n",
      "      Successfully uninstalled boto3-1.39.4\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.245.1.dev0\n",
      "    Uninstalling sagemaker-2.245.1.dev0:\n",
      "      Successfully uninstalled sagemaker-2.245.1.dev0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.37.7 requires botocore==1.36.7, but you have botocore 1.39.6 which is incompatible.\n",
      "awscli 1.37.7 requires s3transfer<0.12.0,>=0.11.0, but you have s3transfer 0.13.0 which is incompatible.\n",
      "aiobotocore 2.18.0 requires botocore<1.36.2,>=1.36.0, but you have botocore 1.39.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.36.1 botocore-1.39.6 sagemaker-2.248.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5e30f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d82f53f",
   "metadata": {},
   "source": [
    "## Prerequisite: \n",
    "\n",
    "This notebook assumes that the Cluster Setup and Cluster RIG (restricted instance group) setup is complete. If you have not followed the cluster creation and RIG Creation Step, please follow the docuemntation instructions on how to setup a Hyperpod Cluster and how to add RIG to that Hyperpod cluster. [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-hp-cluster.html)\n",
    "\n",
    "You can also make use of Cluster Creation and RIG Setup Helper Utility command line tool which can be [found here](/Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/cli_utility/00_setup)\n",
    "\n",
    "To verify you have a cluster running, head to SageMaker AI on AWS Console and head to under \"Hyperpod Section\" or you can run below command as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d3c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! List all the clusters available !!!!\n",
      "[\n",
      "  {\n",
      "    \"ClusterArn\": \"arn:aws:sagemaker:us-east-1:905418197933:cluster/m9bbgk2h4tx7\",\n",
      "    \"ClusterName\": \"hp-cluster\",\n",
      "    \"CreationTime\": 1751954458.774,\n",
      "    \"ClusterStatus\": \"InService\",\n",
      "    \"TrainingPlanArns\": []\n",
      "  }\n",
      "]\n",
      "\n",
      "Cluster name:\n",
      "hp-cluster\n",
      "\n",
      "Describe the Restricted Instance Group in the cluster\n",
      "{\n",
      "  \"CurrentCount\": 2,\n",
      "  \"TargetCount\": 2,\n",
      "  \"InstanceGroupName\": \"restricted-instance-group3\",\n",
      "  \"InstanceType\": \"ml.p5.48xlarge\",\n",
      "  \"ExecutionRole\": \"arn:aws:iam::905418197933:role/hp-eks-test-SMHP-Exec-Role-us-east-1\",\n",
      "  \"ThreadsPerCore\": 1,\n",
      "  \"InstanceStorageConfigs\": [\n",
      "    {\n",
      "      \"EbsVolumeConfig\": {\n",
      "        \"VolumeSizeInGB\": 500\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"Status\": \"InService\",\n",
      "  \"TrainingPlanStatus\": \"NotApplicable\",\n",
      "  \"OverrideVpcConfig\": {\n",
      "    \"SecurityGroupIds\": [\n",
      "      \"sg-0d292fc98e9374ba9\"\n",
      "    ],\n",
      "    \"Subnets\": [\n",
      "      \"subnet-009c17e0bbdf35342\"\n",
      "    ]\n",
      "  },\n",
      "  \"TrustedEnvironment\": {\n",
      "    \"FSxLustreConfig\": {\n",
      "      \"SizeInGiB\": 12000,\n",
      "      \"PerUnitStorageThroughput\": 125\n",
      "    },\n",
      "    \"S3OutputPath\": \"s3://customer-escrow-905418197933-hp-66af7af3\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"!!!! List all the clusters available !!!!\"\n",
    "aws sagemaker list-clusters | jq '.ClusterSummaries'\n",
    "CLUSTER_NAME=$(aws sagemaker list-clusters | jq -r '.ClusterSummaries[0].ClusterName')\n",
    "CLUSTER_ID=$(aws sagemaker list-clusters | jq -r '.ClusterSummaries[0].ClusterArn | split(\"/\")[-1]')\n",
    "\n",
    "\n",
    "echo \"\"\n",
    "echo \"Cluster name:\"\n",
    "echo $CLUSTER_NAME\n",
    "\n",
    "\n",
    "echo \"\"\n",
    "\n",
    "echo \"Describe the Restricted Instance Group in the cluster\"\n",
    "aws sagemaker describe-cluster --cluster-name $CLUSTER_NAME | jq -r '.RestrictedInstanceGroups[0]'\n",
    "RIG_NAME=$(aws sagemaker describe-cluster --cluster-name $CLUSTER_NAME | jq -r '.RestrictedInstanceGroups[0].InstanceGroupName')\n",
    "\n",
    "cat > .env << EOF\n",
    "export CLUSTER_NAME=$CLUSTER_NAME\n",
    "export CLUSTER_ID=$CLUSTER_ID\n",
    "export RIG_NAME=$RIG_NAME\n",
    "EOF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f700d8a",
   "metadata": {},
   "source": [
    "As you can see in above cell output, the describe cluster has RIG setup as well \n",
    "\n",
    "```\n",
    "\"RestrictedInstanceGroups\": [\n",
    "        {\n",
    "            ....\n",
    "```\n",
    "\n",
    "This indicates that this cluster has RIG setup with p5.48xlarge instances which are needed to kick off the training jobs on hyperpod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f0335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128902e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/dewanup/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::905418197933:role/IibsAdminAccess-DO-NOT-DELETE\n",
      "sagemaker bucket: sagemaker-us-east-1-905418197933\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77093873",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the dataset\n",
    "\n",
    "In this example, we are going to load [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) dataset, an open-source dataset and model suite focused on enabling and improving function calling capabilities for large language models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149abd65",
   "metadata": {},
   "source": [
    "### Step 1.1: Data Loading\n",
    "\n",
    "This code loads the first 10,000 examples from the glaive-function-calling-v2 dataset from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d1c0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'chat'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glaiveai/glaive-function-calling-v2\", split=\"train[:10000]\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcc7ec",
   "metadata": {},
   "source": [
    "Converting the dataset to a pandas DataFrame makes it easier to work with and manipulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa726332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import glaive_to_standard_format\n",
    "\n",
    "processed_dataset = glaive_to_standard_format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8ba30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system</th>\n",
       "      <th>chat</th>\n",
       "      <th>messages</th>\n",
       "      <th>tools</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SYSTEM: You are a helpful assistant with acces...</td>\n",
       "      <td>USER: Can you book a flight for me from New Yo...</td>\n",
       "      <td>[{'content': 'You are a helpful assistant.', '...</td>\n",
       "      <td>[{\"type\": \"function\", \"function\": {\"name\": \"ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SYSTEM: You are a helpful assistant with acces...</td>\n",
       "      <td>USER: Can you tell me the latest news headline...</td>\n",
       "      <td>[{'content': 'You are a helpful assistant.', '...</td>\n",
       "      <td>[{\"type\": \"function\", \"function\": {\"name\": \"ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SYSTEM: You are a helpful assistant with acces...</td>\n",
       "      <td>USER: I need a new password. Can you generate ...</td>\n",
       "      <td>[{'content': 'You are a helpful assistant.', '...</td>\n",
       "      <td>[{\"type\": \"function\", \"function\": {\"name\": \"ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SYSTEM: You are a helpful assistant with acces...</td>\n",
       "      <td>USER: I need a new password. Can you generate ...</td>\n",
       "      <td>[{'content': 'You are a helpful assistant.', '...</td>\n",
       "      <td>[{\"type\": \"function\", \"function\": {\"name\": \"ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYSTEM: You are a helpful assistant with acces...</td>\n",
       "      <td>USER: Hi, I have a list of numbers and I need ...</td>\n",
       "      <td>[{'content': 'You are a helpful assistant.', '...</td>\n",
       "      <td>[{\"type\": \"function\", \"function\": {\"name\": \"ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              system  \\\n",
       "0  SYSTEM: You are a helpful assistant with acces...   \n",
       "1  SYSTEM: You are a helpful assistant with acces...   \n",
       "2  SYSTEM: You are a helpful assistant with acces...   \n",
       "3  SYSTEM: You are a helpful assistant with acces...   \n",
       "4  SYSTEM: You are a helpful assistant with acces...   \n",
       "\n",
       "                                                chat  \\\n",
       "0  USER: Can you book a flight for me from New Yo...   \n",
       "1  USER: Can you tell me the latest news headline...   \n",
       "2  USER: I need a new password. Can you generate ...   \n",
       "3  USER: I need a new password. Can you generate ...   \n",
       "4  USER: Hi, I have a list of numbers and I need ...   \n",
       "\n",
       "                                            messages  \\\n",
       "0  [{'content': 'You are a helpful assistant.', '...   \n",
       "1  [{'content': 'You are a helpful assistant.', '...   \n",
       "2  [{'content': 'You are a helpful assistant.', '...   \n",
       "3  [{'content': 'You are a helpful assistant.', '...   \n",
       "4  [{'content': 'You are a helpful assistant.', '...   \n",
       "\n",
       "                                               tools  \n",
       "0  [{\"type\": \"function\", \"function\": {\"name\": \"ge...  \n",
       "1  [{\"type\": \"function\", \"function\": {\"name\": \"ge...  \n",
       "2  [{\"type\": \"function\", \"function\": {\"name\": \"ge...  \n",
       "3  [{\"type\": \"function\", \"function\": {\"name\": \"ge...  \n",
       "4  [{\"type\": \"function\", \"function\": {\"name\": \"ca...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(processed_dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82817dfe",
   "metadata": {},
   "source": [
    "### Step 1.2: Train/Val/Test Split\n",
    "\n",
    "The dataset is split into training (72%), validation (18%), and test (10%) sets to properly evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "382799aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train elements:  7178\n",
      "Number of test elements:  998\n",
      "Number of val elements:  1795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(temp, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Number of train elements: \", len(train))\n",
    "print(\"Number of test elements: \", len(test))\n",
    "print(\"Number of val elements: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa744330",
   "metadata": {},
   "source": [
    "### Understanding the Nova Format\n",
    "\n",
    "Let's format the dataset by using the prompt style for Amazon Nova:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": [{\"text\": Content of the System prompt}],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\"text\": Content of the user prompt]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\"text\": Content of the answer]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2f4c7",
   "metadata": {},
   "source": [
    "### Step 1.3: Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a449ef",
   "metadata": {},
   "source": [
    "The notebook defines utility functions to clean the dataset content by removing prefixes and handling special cases:\n",
    "\n",
    "```python\n",
    "def clean_prefix(content):\n",
    "    # Removes prefixes like \"USER:\", \"ASSISTANT:\", etc.\n",
    "    ...\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "\n",
    "def clean_numbered_conversation(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9d4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_prefix(content):\n",
    "    \"\"\"Remove prefixes from content, according to Nova data_validator\"\"\"\n",
    "    prefixes = [\n",
    "        \"SYSTEM:\",\n",
    "        \"System:\",\n",
    "        \"USER:\",\n",
    "        \"User:\",\n",
    "        \"ASSISTANT:\",\n",
    "        \"Assistant:\",\n",
    "        \"Bot:\",\n",
    "        \"BOT:\",\n",
    "    ]\n",
    "\n",
    "    # Handle array case (list of content items)\n",
    "    if hasattr(content, \"__iter__\") and not isinstance(content, str):\n",
    "        for i, item in enumerate(content):\n",
    "            if isinstance(item, dict) and \"text\" in item:\n",
    "                text = item[\"text\"]\n",
    "                if isinstance(text, str):\n",
    "                    # Clean line by line for multi-line text\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    cleaned_lines = []\n",
    "                    for line in lines:\n",
    "                        cleaned_line = line.strip()\n",
    "                        for prefix in prefixes:\n",
    "                            if cleaned_line.startswith(prefix):\n",
    "                                cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                                break\n",
    "                        cleaned_lines.append(cleaned_line)\n",
    "                    item[\"text\"] = \"\\n\".join(cleaned_lines)\n",
    "        return content\n",
    "\n",
    "    # Handle string case\n",
    "    if isinstance(content, str):\n",
    "        lines = content.split(\"\\n\")\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            cleaned_line = line.strip()\n",
    "            for prefix in prefixes:\n",
    "                if cleaned_line.startswith(prefix):\n",
    "                    cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                    break\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    \"\"\"Clean message list from None values and convert to list of dicts if needed.\"\"\"\n",
    "    if isinstance(message_list, str):\n",
    "        message_list = json.loads(message_list)\n",
    "\n",
    "    tmp_cleaned = []\n",
    "    for msg in message_list:\n",
    "        new_msg = {}\n",
    "        for key, value in msg.items():\n",
    "            if key in [\"content\"]:\n",
    "                if value is None or str(value).lower() == \"None\":\n",
    "                    continue\n",
    "            new_msg[key] = value\n",
    "        tmp_cleaned.append(new_msg)\n",
    "\n",
    "    cleaned = []\n",
    "    for item in tmp_cleaned:\n",
    "        content = item[\"content\"]\n",
    "        for content_item in content:\n",
    "            if isinstance(content_item, dict) and \"text\" in content_item:\n",
    "                text = clean_numbered_conversation(content_item[\"text\"])\n",
    "                content_item[\"text\"] = clean_prefix(text)\n",
    "        cleaned.append({\"role\": item[\"role\"], \"content\": content})\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Additional function to specifically handle the numbered conversation format\n",
    "def clean_numbered_conversation(text):\n",
    "    \"\"\"Clean numbered conversation format like '1. User: ...'\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Pattern to match numbered items with User: or Assistant: prefixes\n",
    "    pattern = r\"(\\d+\\.\\s*)(User:|Assistant:)\\s*\"\n",
    "\n",
    "    # Replace the pattern, keeping the number but removing the role prefix\n",
    "    cleaned_text = re.sub(pattern, r\"\\1\", text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc3784",
   "metadata": {},
   "source": [
    "These functions transform the dataset into the format required by Nova models, handling tool calls and formatting:\n",
    "\n",
    "```python\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    # Transforms tool format to Nova's expected format\n",
    "    ...\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    # Prepares dataset in the required format for Nova models\n",
    "    ...\n",
    "\n",
    "def prepare_dataset_test(sample):\n",
    "    # Formats validation dataset for evaluation\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ea4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    \"\"\"Transform tool from old format to Nova format.\"\"\"\n",
    "    if \"function\" not in tool:\n",
    "        return tool\n",
    "\n",
    "    function = tool[\"function\"]\n",
    "    return {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": function[\"name\"],\n",
    "            \"description\": function[\"description\"],\n",
    "            \"inputSchema\": {\"json\": function[\"parameters\"]},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    \"\"\"Prepare dataset in the required format for Nova models\"\"\"\n",
    "    messages = {\"system\": [], \"messages\": []}\n",
    "\n",
    "    # Process tools upfront if they exist\n",
    "    tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    formatted_text = (\n",
    "        \"\"  # Initialize outside the loop to avoid undefined variable issues\n",
    "    )\n",
    "\n",
    "    for message in sample[\"messages\"]:\n",
    "        role = message[\"role\"]\n",
    "\n",
    "        if role == \"system\" and tools:\n",
    "            # Build system message with tools\n",
    "            system_text = (\n",
    "                f\"{message['content']}\\n\"\n",
    "                \"You may call one or more functions to assist with the user query.\\n\\n\"\n",
    "                \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "                \"<tools>\\n\"\n",
    "                f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "                \"</tools>\\n\\n\"\n",
    "                \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "                '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "            )\n",
    "            messages[\"system\"] = [{\"text\": system_text.lower()}]\n",
    "\n",
    "        elif role == \"user\":\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": message[\"content\"].lower()}]}\n",
    "            )\n",
    "\n",
    "        elif role == \"tool\":\n",
    "            formatted_text += message[\"content\"]\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": formatted_text.lower()}]}\n",
    "            )\n",
    "\n",
    "        elif role == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Process tool calls\n",
    "                tool_calls_text = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    tool_call_json = {\n",
    "                        \"name\": function_data[\"name\"],\n",
    "                        \"parameters\": arguments,\n",
    "                    }\n",
    "                    tool_calls_text.append(json.dumps(tool_call_json))\n",
    "\n",
    "                messages[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [{\"text\": \"\".join(tool_calls_text).lower()}],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                messages[\"messages\"].append(\n",
    "                    {\"role\": \"assistant\", \"content\": [{\"text\": message[\"content\"].lower()}]}\n",
    "                )\n",
    "\n",
    "    # Remove the last message if it's not from assistant\n",
    "    if messages[\"messages\"] and messages[\"messages\"][-1][\"role\"] != \"assistant\":\n",
    "        messages[\"messages\"].pop()\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c043ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_test(sample):\n",
    "    \"\"\"Parse sample and format it for validation dataset.\"\"\"\n",
    "    # Process tools\n",
    "    tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    # Initialize result\n",
    "    result = []\n",
    "    conversation_history = []\n",
    "\n",
    "    # Extract system message\n",
    "    system_content = \"\"\n",
    "    for message in sample[\"messages\"]:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            system_content = message[\"content\"]\n",
    "            if tools:\n",
    "                system_content += (\n",
    "                    \"\\nYou may call one or more functions to assist with the user query.\\n\\n\"\n",
    "                    \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "                    \"<tools>\\n\"\n",
    "                    f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "                    \"</tools>\\n\\n\"\n",
    "                    \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "                    '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "                )\n",
    "            break\n",
    "\n",
    "    # Process conversation turns\n",
    "    for i, message in enumerate(sample[\"messages\"]):\n",
    "        if message[\"role\"] == \"system\":\n",
    "            continue\n",
    "\n",
    "        # Add message to conversation history\n",
    "        if message[\"role\"] == \"user\":\n",
    "            conversation_history.append(f\"## User: {message['content']}\")\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Format tool calls\n",
    "                target_parts = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    target_parts.append(\n",
    "                        json.dumps(\n",
    "                            {\"name\": function_data[\"name\"], \"parameters\": arguments}\n",
    "                        )\n",
    "                    )\n",
    "                target = \"\".join(target_parts)\n",
    "\n",
    "                conversation_history.append(f\"## Assistant: {target}\")\n",
    "            else:\n",
    "                conversation_history.append(f\"## Assistant: {message['content']}\")\n",
    "        elif message[\"role\"] == \"tool\":\n",
    "            conversation_history.append(f\"## Function: {message['content']}\")\n",
    "\n",
    "        # Create input-target pair when we have an assistant message\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            # Input is system message + all previous conversation\n",
    "            input_text = \"\\n\".join(conversation_history[:-1])\n",
    "\n",
    "            # Target is the assistant's response\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Format tool calls\n",
    "                target_parts = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    target_parts.append(\n",
    "                        json.dumps(\n",
    "                            {\"name\": function_data[\"name\"], \"parameters\": arguments}\n",
    "                        )\n",
    "                    )\n",
    "                target = \"\".join(target_parts)\n",
    "            else:\n",
    "                target = message[\"content\"]\n",
    "\n",
    "            result.append({\"system\": system_content.lower(), \"query\": input_text.lower(), \"response\": target.lower()})\n",
    "\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67897c01",
   "metadata": {},
   "source": [
    "### Step 1.4: Data Preperation in Converse Format for Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a10f4dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a087be38cc24b168acb7b15c31865cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"system\":[{\"text\":\"you are a helpful assistant.\\nyou may call one or more functions to assist with the user query.\\n\\nyou are provided with function signatures within <tools><\\/tools> xml tags:\\n<tools>\\n{\\\"tools\\\": [{\\\"toolspec\\\": {\\\"name\\\": \\\"search_restaurant\\\", \\\"description\\\": \\\"search for restaurants based on location and cuisine\\\", \\\"inputschema\\\": {\\\"json\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"location\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"the location for restaurant search\\\"}, \\\"cuisine\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"the preferred cuisine type\\\"}}, \\\"required\\\": [\\\"location\\\"]}}}}, {\\\"toolspec\\\": {\\\"name\\\": \\\"get_lyrics\\\", \\\"description\\\": \\\"get the lyrics of a song\\\", \\\"inputschema\\\": {\\\"json\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"artist\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"the artist of the song\\\"}, \\\"title\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"the title of the song\\\"}}, \\\"required\\\": [\\\"artist\\\", \\\"title\\\"]}}}}]}\\n<\\/tools>\\n\\nfor each function call, return a json object with function name and parameters:\\n{\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}\"}],\"messages\":[{\"role\":\"user\",\"content\":[{\"text\":\"i'm in new york and i'm craving for some italian food. can you help me find a restaurant?\"}]},{\"role\":\"assistant\",\"content\":[{\"text\":\"{\\\"name\\\": \\\"search_restaurant\\\", \\\"parameters\\\": {\\\"location\\\": \\\"new york\\\", \\\"cuisine\\\": \\\"italian\\\"}}\"}]},{\"role\":\"user\",\"content\":[{\"text\":\"{\\\"restaurants\\\": [{\\\"name\\\": \\\"carbone\\\", \\\"address\\\": \\\"181 thompson st, new york, ny 10012, united states\\\", \\\"rating\\\": 4.7}, {\\\"name\\\": \\\"l'artusi\\\", \\\"address\\\": \\\"228 w 10th st, new york, ny 10014, united states\\\", \\\"rating\\\": 4.6}]}\"}]},{\"role\":\"assistant\",\"content\":[{\"text\":\"i found a couple of italian restaurants for you in new york. the first one is carbone, located at 181 thompson st, new york, ny 10012, united states with a rating of 4.7. the second one is l'artusi, located at 228 w 10th st, new york, ny 10014, united states with a rating of 4.6. enjoy your meal!\"}]},{\"role\":\"user\",\"content\":[{\"text\":\"that's great! thank you for your help.\"}]},{\"role\":\"assistant\",\"content\":[{\"text\":\"you're welcome! if you need any more help, feel free to ask.\"}]}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd97224d054f4fb6ba45cb46a6a7439e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"system\":[{\"text\":\"you are a helpful assistant.\\nyou may call one or more functions to assist with the user query.\\n\\nyou are provided with function signatures within <tools><\\/tools> xml tags:\\n<tools>\\n{\\\"tools\\\": [{\\\"toolspec\\\": {\\\"name\\\": \\\"calculate_age\\\", \\\"description\\\": \\\"calculate age based on date of birth\\\", \\\"inputschema\\\": {\\\"json\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"date_of_birth\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"the date of birth in yyyy-mm-dd format\\\"}}, \\\"required\\\": [\\\"date_of_birth\\\"]}}}}]}\\n<\\/tools>\\n\\nfor each function call, return a json object with function name and parameters:\\n{\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}\"}],\"messages\":[{\"role\":\"user\",\"content\":[{\"text\":\"hi, i was born on 1990-05-15. can you tell me how old i am today?\"}]},{\"role\":\"assistant\",\"content\":[{\"text\":\"{\\\"name\\\": \\\"calculate_age\\\", \\\"parameters\\\": {\\\"date_of_birth\\\": \\\"1990-05-15\\\"}}\"}]},{\"role\":\"user\",\"content\":[{\"text\":\"{\\\"age\\\": 31}\"}]},{\"role\":\"assistant\",\"content\":[{\"text\":\"according to my calculations, you are 31 years old today.\"}]},{\"role\":\"user\",\"content\":[{\"text\":\"wow, time flies! thank you for the information.\"}]},{\"role\":\"assistant\",\"content\":[{\"text\":\"you're welcome! if you have any other questions, feel free to ask.\"}]}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760ac3697691418591c846dc654aa44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'query': '## user: tell me an example of a website that uses artificial intelligence and machine learning.', 'response': \"one example of a website that uses artificial intelligence and machine learning is netflix. netflix uses an algorithm to personalize the viewing experience for each user, by recommending content based on the user's viewing history, preferences, and ratings. the more a user watches, the more accurate the algorithm becomes in predicting what they might enjoy watching next. additionally, netflix uses machine learning to optimize video encoding, improving the streaming quality for users on slower internet connections. \", 'system': 'you are a helpful assistant.'}, {'query': \"## user: tell me an example of a website that uses artificial intelligence and machine learning.\\n## assistant: one example of a website that uses artificial intelligence and machine learning is netflix. netflix uses an algorithm to personalize the viewing experience for each user, by recommending content based on the user's viewing history, preferences, and ratings. the more a user watches, the more accurate the algorithm becomes in predicting what they might enjoy watching next. additionally, netflix uses machine learning to optimize video encoding, improving the streaming quality for users on slower internet connections. \\n## user: how does machine learning improve video encoding on netflix?\", 'response': 'machine learning improves video encoding on netflix by analyzing each video and determining the optimal encoding settings based on its specific characteristics. for example, a video with a lot of movement requires a higher bitrate than a static image to maintain picture quality. by using machine learning algorithms, netflix can determine the ideal settings for each video, resulting in better image quality and faster streaming speeds for users. ', 'system': 'you are a helpful assistant.'}, {'query': \"## user: tell me an example of a website that uses artificial intelligence and machine learning.\\n## assistant: one example of a website that uses artificial intelligence and machine learning is netflix. netflix uses an algorithm to personalize the viewing experience for each user, by recommending content based on the user's viewing history, preferences, and ratings. the more a user watches, the more accurate the algorithm becomes in predicting what they might enjoy watching next. additionally, netflix uses machine learning to optimize video encoding, improving the streaming quality for users on slower internet connections. \\n## user: how does machine learning improve video encoding on netflix?\\n## assistant: machine learning improves video encoding on netflix by analyzing each video and determining the optimal encoding settings based on its specific characteristics. for example, a video with a lot of movement requires a higher bitrate than a static image to maintain picture quality. by using machine learning algorithms, netflix can determine the ideal settings for each video, resulting in better image quality and faster streaming speeds for users. \\n## user: can you explain the difference between artificial intelligence and machine learning?\", 'response': 'artificial intelligence (ai) is a broad term that encompasses any computer system designed to perform tasks that would normally require human intelligence, such as natural language processing, speech recognition, and decision making. machine learning, on the other hand, is a subset of ai that involves training algorithms to make predictions or decisions based on patterns in data. in other words, machine learning is a way of teaching computers to learn from experience, without being explicitly programmed to do so. ', 'system': 'you are a helpful assistant.'}, {'query': \"## user: tell me an example of a website that uses artificial intelligence and machine learning.\\n## assistant: one example of a website that uses artificial intelligence and machine learning is netflix. netflix uses an algorithm to personalize the viewing experience for each user, by recommending content based on the user's viewing history, preferences, and ratings. the more a user watches, the more accurate the algorithm becomes in predicting what they might enjoy watching next. additionally, netflix uses machine learning to optimize video encoding, improving the streaming quality for users on slower internet connections. \\n## user: how does machine learning improve video encoding on netflix?\\n## assistant: machine learning improves video encoding on netflix by analyzing each video and determining the optimal encoding settings based on its specific characteristics. for example, a video with a lot of movement requires a higher bitrate than a static image to maintain picture quality. by using machine learning algorithms, netflix can determine the ideal settings for each video, resulting in better image quality and faster streaming speeds for users. \\n## user: can you explain the difference between artificial intelligence and machine learning?\\n## assistant: artificial intelligence (ai) is a broad term that encompasses any computer system designed to perform tasks that would normally require human intelligence, such as natural language processing, speech recognition, and decision making. machine learning, on the other hand, is a subset of ai that involves training algorithms to make predictions or decisions based on patterns in data. in other words, machine learning is a way of teaching computers to learn from experience, without being explicitly programmed to do so. \\n## user: what are some real-world applications of machine learning?\", 'response': 'there are many real-world applications of machine learning, ranging from image and speech recognition to fraud detection and self-driving cars. some other examples include:\\n\\n- personalized product recommendations on e-commerce sites\\n- credit risk analysis in the banking industry\\n- predictive maintenance in manufacturing\\n- medical diagnosis and treatment planning\\n- natural language processing for chatbots and virtual assistants\\n\\nthe possibilities are truly endless, and as machine learning technology continues to advance, we can expect even more innovative applications to emerge. ', 'system': 'you are a helpful assistant.'}, {'query': \"## user: tell me an example of a website that uses artificial intelligence and machine learning.\\n## assistant: one example of a website that uses artificial intelligence and machine learning is netflix. netflix uses an algorithm to personalize the viewing experience for each user, by recommending content based on the user's viewing history, preferences, and ratings. the more a user watches, the more accurate the algorithm becomes in predicting what they might enjoy watching next. additionally, netflix uses machine learning to optimize video encoding, improving the streaming quality for users on slower internet connections. \\n## user: how does machine learning improve video encoding on netflix?\\n## assistant: machine learning improves video encoding on netflix by analyzing each video and determining the optimal encoding settings based on its specific characteristics. for example, a video with a lot of movement requires a higher bitrate than a static image to maintain picture quality. by using machine learning algorithms, netflix can determine the ideal settings for each video, resulting in better image quality and faster streaming speeds for users. \\n## user: can you explain the difference between artificial intelligence and machine learning?\\n## assistant: artificial intelligence (ai) is a broad term that encompasses any computer system designed to perform tasks that would normally require human intelligence, such as natural language processing, speech recognition, and decision making. machine learning, on the other hand, is a subset of ai that involves training algorithms to make predictions or decisions based on patterns in data. in other words, machine learning is a way of teaching computers to learn from experience, without being explicitly programmed to do so. \\n## user: what are some real-world applications of machine learning?\\n## assistant: there are many real-world applications of machine learning, ranging from image and speech recognition to fraud detection and self-driving cars. some other examples include:\\n\\n- personalized product recommendations on e-commerce sites\\n- credit risk analysis in the banking industry\\n- predictive maintenance in manufacturing\\n- medical diagnosis and treatment planning\\n- natural language processing for chatbots and virtual assistants\\n\\nthe possibilities are truly endless, and as machine learning technology continues to advance, we can expect even more innovative applications to emerge. \\n## user: how does machine learning help with fraud detection?\", 'response': 'machine learning helps with fraud detection by analyzing large amounts of data and identifying patterns that may indicate fraudulent activity. for example, a credit card company could use machine learning algorithms to detect unusual spending behavior, such as multiple purchases in different locations within a short period of time. the system could then flag these transactions for further investigation, potentially preventing fraudulent charges before they occur. machine learning can also be used to detect insider threats within organizations, such as employees attempting to steal sensitive data or commit financial fraud. ', 'system': 'you are a helpful assistant.'}, {'query': \"## user: tell me an example of a website that uses artificial intelligence and machine learning.\\n## assistant: one example of a website that uses artificial intelligence and machine learning is netflix. netflix uses an algorithm to personalize the viewing experience for each user, by recommending content based on the user's viewing history, preferences, and ratings. the more a user watches, the more accurate the algorithm becomes in predicting what they might enjoy watching next. additionally, netflix uses machine learning to optimize video encoding, improving the streaming quality for users on slower internet connections. \\n## user: how does machine learning improve video encoding on netflix?\\n## assistant: machine learning improves video encoding on netflix by analyzing each video and determining the optimal encoding settings based on its specific characteristics. for example, a video with a lot of movement requires a higher bitrate than a static image to maintain picture quality. by using machine learning algorithms, netflix can determine the ideal settings for each video, resulting in better image quality and faster streaming speeds for users. \\n## user: can you explain the difference between artificial intelligence and machine learning?\\n## assistant: artificial intelligence (ai) is a broad term that encompasses any computer system designed to perform tasks that would normally require human intelligence, such as natural language processing, speech recognition, and decision making. machine learning, on the other hand, is a subset of ai that involves training algorithms to make predictions or decisions based on patterns in data. in other words, machine learning is a way of teaching computers to learn from experience, without being explicitly programmed to do so. \\n## user: what are some real-world applications of machine learning?\\n## assistant: there are many real-world applications of machine learning, ranging from image and speech recognition to fraud detection and self-driving cars. some other examples include:\\n\\n- personalized product recommendations on e-commerce sites\\n- credit risk analysis in the banking industry\\n- predictive maintenance in manufacturing\\n- medical diagnosis and treatment planning\\n- natural language processing for chatbots and virtual assistants\\n\\nthe possibilities are truly endless, and as machine learning technology continues to advance, we can expect even more innovative applications to emerge. \\n## user: how does machine learning help with fraud detection?\\n## assistant: machine learning helps with fraud detection by analyzing large amounts of data and identifying patterns that may indicate fraudulent activity. for example, a credit card company could use machine learning algorithms to detect unusual spending behavior, such as multiple purchases in different locations within a short period of time. the system could then flag these transactions for further investigation, potentially preventing fraudulent charges before they occur. machine learning can also be used to detect insider threats within organizations, such as employees attempting to steal sensitive data or commit financial fraud. \\n## user: how does natural language processing work?\", 'response': 'natural language processing (nlp) is a field of ai that focuses on teaching computers to understand and interpret human language. this involves breaking down sentences and analyzing their structure and meaning in order to extract information or generate responses. nlp systems typically use a combination of machine learning algorithms and rule-based approaches to achieve this functionality. some common applications of nlp include chatbots, sentiment analysis, and language translation. ', 'system': 'you are a helpful assistant.'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from random import randint\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\"train\": train_dataset, \"test\": test_dataset, \"val\": val_dataset}\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset, remove_columns=train_dataset.features\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.to_pandas()\n",
    "\n",
    "train_dataset[\"messages\"] = train_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(train_dataset.iloc[randint(0, len(train_dataset))].to_json())\n",
    "\n",
    "val_dataset = dataset[\"val\"].map(prepare_dataset, remove_columns=val_dataset.features)\n",
    "\n",
    "val_dataset = val_dataset.to_pandas()\n",
    "\n",
    "val_dataset[\"messages\"] = val_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(val_dataset.iloc[randint(0, len(val_dataset))].to_json())\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset_test, remove_columns=test_dataset.features\n",
    ")\n",
    "print(test_dataset[randint(0, len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80bdec",
   "metadata": {},
   "source": [
    "### Step 1.5: Data Preperation on test data for Offline Evaluation post fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b84168",
   "metadata": {},
   "source": [
    "Let's format the test dataset in the format:\n",
    "\n",
    "Required Fields:\n",
    "\n",
    "* query: String containing the question or instruction that needs an answer\n",
    "* response: String containing the expected model output\n",
    "\n",
    "Optional Fields:\n",
    "\n",
    "* system: String containing the system prompt that sets the behavior, role, or personality of the AI model before it processes the query\n",
    "\n",
    "Example Entry\n",
    "```\n",
    "\n",
    "{\n",
    "   \"system\":\"You are a english major with top marks in class who likes to give minimal word responses: \",\n",
    "   \"query\":\"What is the symbol that ends the sentence as a question\",\n",
    "   \"response\":\"?\"\n",
    "}\n",
    "{\n",
    "   \"system\":\"You are a pattern analysis specialist that provides succinct answers: \",\n",
    "   \"query\":\"What is the next number in this series? 1, 2, 4, 8, 16, ?\",\n",
    "   \"response\":\"32\"\n",
    "}\n",
    "{\n",
    "   \"system\":\"You have great attention to detail that follows instructions accurately: \",\n",
    "   \"query\":\"Repeat only the last two words of the following: I ate a hamburger today and it was kind of dry\",\n",
    "   \"response\":\"of dry\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45087011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'you are a helpful assistant.\\nyou may call one or more functions to assist with the user query.\\n\\nyou are provided with function signatures within <tools></tools> xml tags:\\n<tools>\\n{\"tools\": [{\"toolspec\": {\"name\": \"calculate_age\", \"description\": \"calculate the age based on the birth date\", \"inputschema\": {\"json\": {\"type\": \"object\", \"properties\": {\"birth_date\": {\"type\": \"string\", \"description\": \"the birth date in format yyyy-mm-dd\"}}, \"required\": [\"birth_date\"]}}}}]}\\n</tools>\\n\\nfor each function call, return a json object with function name and parameters:\\n{\"name\": function name, \"parameters\": dictionary of argument name and its value}', 'query': '## user: can you order a pizza for me?', 'response': \"i'm sorry, but i'm unable to perform external tasks like ordering a pizza. my current capabilities are limited to the functions provided to me. in this case, i can help you calculate age based on a birth date. \"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Flatten the dataset\n",
    "all_examples = []\n",
    "for examples_list in test_dataset:\n",
    "    # The first column contains the list of examples\n",
    "    column_name = test_dataset.column_names[0]\n",
    "    examples = examples_list[column_name]\n",
    "    all_examples.extend(examples)\n",
    "\n",
    "# Create a new dataset with the desired structure\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"system\": [example[\"system\"] for example in all_examples],\n",
    "        \"query\": [example[\"query\"] for example in all_examples],\n",
    "        \"response\": [example[\"response\"] for example in all_examples],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(test_dataset[randint(0, len(val_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4a505",
   "metadata": {},
   "source": [
    "### Step 1.6: Upload all 3 curated datasets (train, test, val) to Amazon S3\n",
    "\n",
    "The notebook applies the functions to transform the datasets into the required formats\n",
    "\n",
    "\n",
    "The processed datasets are saved locally and then uploaded to Amazon S3 for use in SageMaker training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b52fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8abda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/nova-sft-peft\"\n",
    "else:\n",
    "    input_path = f\"datasets/nova-sft-peft\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.jsonl\"\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/dataset.jsonl\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/gen_qa.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bb404ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe72b95203947b5b4a240544b092fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-905418197933/datasets/nova-sft-peft/train/dataset.jsonl\n",
      "s3://sagemaker-us-east-1-905418197933/datasets/nova-sft-peft/test/gen_qa.jsonl\n",
      "s3://sagemaker-us-east-1-905418197933/datasets/nova-sft-peft/val/dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save datasets to s3\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "os.makedirs(\"./data/val\", exist_ok=True)\n",
    "\n",
    "train_dataset.to_json(\"./data/train/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "val_dataset.to_json(\"./data/val/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "test_dataset.to_json(\"./data/test/gen_qa.jsonl\")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/train/dataset.jsonl\", bucket_name, f\"{input_path}/train/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/val/dataset.jsonl\", bucket_name, f\"{input_path}/val/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/test/gen_qa.jsonl\", bucket_name, f\"{input_path}/test/gen_qa.jsonl\"\n",
    ")\n",
    "\n",
    "shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(val_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80768dc0",
   "metadata": {},
   "source": [
    "## Step 2: Model fine-tuning\n",
    "\n",
    "We now define the parameters to kick off a Hyperpod Pytorch Training Job to run the supervised fine-tuning on a tool-calling dataset for our Amazon Nova model\n",
    "\n",
    "This section sets up and runs the fine-tuning job using SageMaker Hyperpod. It uses Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) to efficiently train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b5c19",
   "metadata": {},
   "source": [
    "#### Image URI\n",
    "\n",
    "This specifies the pre-built container for SFT fine-tuning, which is different from the DPO container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4c1afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri_map = {\n",
    "   \"sft\":\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\",\n",
    "    \"dpo\": \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-DPO-latest\",\n",
    "    \"ppo\": \"078496829476.dkr.ecr.us-west-2.amazonaws.com/nova-fine-tune-repo:HP-PPO-latest\",\n",
    "    \"cpt\": \"078496829476.dkr.ecr.us-west-2.amazonaws.com/nova-fine-tune-repo:HP-CPT-latest\",\n",
    "    \"eval\": \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-HP-Eval-latest\"\n",
    " }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed1b17",
   "metadata": {},
   "source": [
    "#### Configuring the Model and Recipe\n",
    "\n",
    "This specifies which model to fine-tune and the recipe to use. The recipe includes \"lora\" indicating parameter-efficient fine-tuning, and \"sft\" indicating supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b232fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RECIPE_PATH = \"fine-tuning/nova/nova_micro_p5_gpu_lora_sft\"\n",
    "INSTANCE =\"p5.48xlarge\"\n",
    "RUN_NAME = \"demo-sft-hp-nova-micro-run\"\n",
    "CONTAINER = image_uri_map[\"sft\"]\n",
    "OUTPUT_PATH=\"s3://sagemaker-us-east-1-905418197933/HP-SFT-RUNS/\"\n",
    "NAMESPACE = \"kubeflow\"\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['NAMESPACE'] = NAMESPACE\n",
    "os.environ['RECIPE_PATH'] = RECIPE_PATH\n",
    "os.environ['INSTANCE'] = INSTANCE\n",
    "os.environ['RUN_NAME'] = RUN_NAME\n",
    "os.environ['CONTAINER'] = CONTAINER\n",
    "os.environ['OUTPUT_PATH'] = OUTPUT_PATH\n",
    "os.environ['TRAIN_DATA_PATH'] = train_dataset_s3_path\n",
    "os.environ['VAL_DATA_PATH'] = val_dataset_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdf137ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HP CLI Installation.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'sagemaker-hyperpod-cli' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple\n",
      "Processing /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/sagemaker-hyperpod-cli\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting click==8.1.7 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: awscli>=1.34.9 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (1.37.7)\n",
      "Collecting awscli-cwlogs>=1.4.6 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached awscli-cwlogs-1.4.6.tar.gz (37 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: boto3<2.0,>=1.35.3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (1.36.1)\n",
      "Requirement already satisfied: botocore>=1.35.6 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (1.39.6)\n",
      "Collecting kubernetes==33.1.0 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyyaml==6.0.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (6.0.2)\n",
      "Collecting ratelimit==2.2.1 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached ratelimit-2.2.1.tar.gz (5.3 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tabulate==0.9.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (0.9.0)\n",
      "Collecting itables>=2.2.2 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached itables-2.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (3.1.4)\n",
      "Collecting ipywidgets>=8.1.7 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting hydra-core==1.3.2 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: omegaconf==2.3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (2.3.0)\n",
      "Collecting pynvml==11.4.1 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached pynvml-11.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting requests==2.32.4 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm==4.66.5 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting zstandard==0.15.2 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached zstandard-0.15.2.tar.gz (1.0 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pytest==8.3.2 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached pytest-8.3.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pytest-cov==5.0.0 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached pytest_cov-5.0.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting pytest-order==1.3.0 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached pytest_order-1.3.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting tox==4.18.0 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached tox-4.18.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting ruff==0.6.2 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached ruff-0.6.2-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting hera-workflows==5.16.3 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached hera_workflows-5.16.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (1.0.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.6 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-hyperpod==3.0.0) (2.11.3)\n",
      "Collecting hyperpod-pytorch-job-template<2.0.0,>=1.0.0 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached hyperpod_pytorch_job_template-1.0-py3-none-any.whl.metadata (519 bytes)\n",
      "Collecting hyperpod-custom-inference-template<2.0.0,>=1.0.0 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached hyperpod_custom_inference_template-1.0-py3-none-any.whl.metadata (498 bytes)\n",
      "Collecting hyperpod-jumpstart-inference-template<2.0.0,>=1.0.0 (from sagemaker-hyperpod==3.0.0)\n",
      "  Using cached hyperpod_jumpstart_inference_template-1.0-py3-none-any.whl.metadata (512 bytes)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from hydra-core==1.3.2->sagemaker-hyperpod==3.0.0) (4.9.3)\n",
      "Requirement already satisfied: packaging in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from hydra-core==1.3.2->sagemaker-hyperpod==3.0.0) (24.2)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (0.9)\n",
      "Collecting iniconfig (from pytest==8.3.2->sagemaker-hyperpod==3.0.0)\n",
      "  Using cached iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest==8.3.2->sagemaker-hyperpod==3.0.0)\n",
      "  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting coverage>=5.2.1 (from coverage[toml]>=5.2.1->pytest-cov==5.0.0->sagemaker-hyperpod==3.0.0)\n",
      "  Downloading coverage-7.9.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from requests==2.32.4->sagemaker-hyperpod==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from requests==2.32.4->sagemaker-hyperpod==3.0.0) (3.10)\n",
      "Requirement already satisfied: cachetools>=5.4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from tox==4.18.0->sagemaker-hyperpod==3.0.0) (5.5.0)\n",
      "Requirement already satisfied: chardet>=5.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from tox==4.18.0->sagemaker-hyperpod==3.0.0) (5.2.0)\n",
      "Requirement already satisfied: colorama>=0.4.6 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from tox==4.18.0->sagemaker-hyperpod==3.0.0) (0.4.6)\n",
      "Requirement already satisfied: filelock>=3.15.4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from tox==4.18.0->sagemaker-hyperpod==3.0.0) (3.16.1)\n",
      "Requirement already satisfied: platformdirs>=4.2.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from tox==4.18.0->sagemaker-hyperpod==3.0.0) (4.3.7)\n",
      "Collecting pyproject-api>=1.7.1 (from tox==4.18.0->sagemaker-hyperpod==3.0.0)\n",
      "  Using cached pyproject_api-1.9.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting virtualenv>=20.26.3 (from tox==4.18.0->sagemaker-hyperpod==3.0.0)\n",
      "  Using cached virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting botocore>=1.35.6 (from sagemaker-hyperpod==3.0.0)\n",
      "  Downloading botocore-1.36.7-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from awscli>=1.34.9->sagemaker-hyperpod==3.0.0) (0.16)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from awscli>=1.34.9->sagemaker-hyperpod==3.0.0)\n",
      "  Downloading s3transfer-0.11.5-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from awscli>=1.34.9->sagemaker-hyperpod==3.0.0) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from botocore>=1.35.6->sagemaker-hyperpod==3.0.0) (1.0.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0)\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from itables>=2.2.2->sagemaker-hyperpod==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from itables>=2.2.2->sagemaker-hyperpod==3.0.0) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jinja2>=3.1.2->sagemaker-hyperpod==3.0.0) (3.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.10.6->sagemaker-hyperpod==3.0.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.10.6->sagemaker-hyperpod==3.0.0) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.10.6->sagemaker-hyperpod==3.0.0) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.10.6->sagemaker-hyperpod==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (4.23.0)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (14.0.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (4.0.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (6.11.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes==33.1.0->sagemaker-hyperpod==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (3.21.0)\n",
      "Requirement already satisfied: decorator in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (0.6.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jsonschema<5.0.0->sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jsonschema<5.0.0->sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jsonschema<5.0.0->sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jsonschema<5.0.0->sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (0.21.0)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pydantic[email]<3.0,>=1.7->hera-workflows==5.16.3->sagemaker-hyperpod==3.0.0) (2.2.0)\n",
      "Collecting packaging (from hydra-core==1.3.2->sagemaker-hyperpod==3.0.0)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from rsa<4.8,>=3.1.2->awscli>=1.34.9->sagemaker-hyperpod==3.0.0) (0.6.1)\n",
      "INFO: pip is looking at multiple versions of s3transfer to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from awscli>=1.34.9->sagemaker-hyperpod==3.0.0)\n",
      "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Downloading s3transfer-0.11.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.26.3->tox==4.18.0->sagemaker-hyperpod==3.0.0)\n",
      "  Using cached distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pandas->itables>=2.2.2->sagemaker-hyperpod==3.0.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pandas->itables>=2.2.2->sagemaker-hyperpod==3.0.0) (2024.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from email-validator>=2.0.0->pydantic[email]<3.0,>=1.7->hera-workflows==5.16.3->sagemaker-hyperpod==3.0.0) (2.7.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0->sagemaker-hyperpod==3.0.0) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/dewanup/.pyenv/versions/3.11.10/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.1.7->sagemaker-hyperpod==3.0.0) (0.2.3)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached hera_workflows-5.16.3-py3-none-any.whl (339 kB)\n",
      "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
      "Using cached pytest-8.3.2-py3-none-any.whl (341 kB)\n",
      "Using cached pytest_cov-5.0.0-py3-none-any.whl (21 kB)\n",
      "Using cached pytest_order-1.3.0-py3-none-any.whl (14 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached ruff-0.6.2-py3-none-macosx_11_0_arm64.whl (9.2 MB)\n",
      "Using cached tox-4.18.0-py3-none-any.whl (156 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading botocore-1.36.7-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached hyperpod_custom_inference_template-1.0-py3-none-any.whl (11 kB)\n",
      "Using cached hyperpod_jumpstart_inference_template-1.0-py3-none-any.whl (9.2 kB)\n",
      "Using cached hyperpod_pytorch_job_template-1.0-py3-none-any.whl (13 kB)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached itables-2.4.4-py3-none-any.whl (2.3 MB)\n",
      "Downloading coverage-7.9.2-cp311-cp311-macosx_11_0_arm64.whl (212 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Using cached pyproject_api-1.9.1-py3-none-any.whl (13 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading s3transfer-0.11.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Using cached iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Using cached distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Building wheels for collected packages: sagemaker-hyperpod, ratelimit, zstandard, awscli-cwlogs\n",
      "  Building wheel for sagemaker-hyperpod (pyproject.toml): started\n",
      "  Building wheel for sagemaker-hyperpod (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sagemaker-hyperpod: filename=sagemaker_hyperpod-3.0.0-py3-none-any.whl size=161417 sha256=7878e748764fa6c819c534cbf9933f8c6944a729351f8b9a93c1fe40e2443360\n",
      "  Stored in directory: /Users/dewanup/Library/Caches/pip/wheels/50/53/7b/1585117fd2512222239df80eb694a6e77417f04f35739cea91\n",
      "  Building wheel for ratelimit (pyproject.toml): started\n",
      "  Building wheel for ratelimit (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5972 sha256=8aadb7a0487c0e53c083dc459c72523e25c1ae9118db7bb00039bbdf608e3dcf\n",
      "  Stored in directory: /Users/dewanup/Library/Caches/pip/wheels/ee/d5/e5/8fbffe089140fb498987b7709becf861086daace105d243475\n",
      "  Building wheel for zstandard (pyproject.toml): started\n",
      "  Building wheel for zstandard (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for zstandard: filename=zstandard-0.15.2-cp311-cp311-macosx_14_0_arm64.whl size=343094 sha256=59234e40ada5b0e31b50f2abe9189ab29b6708abfed60edf09bc5f7ea6c68cdf\n",
      "  Stored in directory: /Users/dewanup/Library/Caches/pip/wheels/e3/4b/85/b58d1687dc5b043b2279a47eee51ddd31a9932166eccf34a20\n",
      "  Building wheel for awscli-cwlogs (pyproject.toml): started\n",
      "  Building wheel for awscli-cwlogs (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for awscli-cwlogs: filename=awscli_cwlogs-1.4.6-py3-none-any.whl size=41231 sha256=2091713d9943f9526f570fa2f913ac4b6b0a6eeb02b8b2e6829a9f948fc7f749\n",
      "  Stored in directory: /Users/dewanup/Library/Caches/pip/wheels/ca/12/57/6b9cdff6c02fa62733d18fef905f9dd110800cc67be88afa0a\n",
      "Successfully built sagemaker-hyperpod ratelimit zstandard awscli-cwlogs\n",
      "Installing collected packages: ratelimit, distlib, zstandard, widgetsnbextension, virtualenv, tqdm, ruff, requests, pynvml, pluggy, packaging, jupyterlab_widgets, iniconfig, hyperpod-jumpstart-inference-template, hyperpod-custom-inference-template, coverage, click, pytest, pyproject-api, hydra-core, botocore, tox, s3transfer, pytest-order, pytest-cov, kubernetes, itables, ipywidgets, hyperpod-pytorch-job-template, hera-workflows, awscli-cwlogs, sagemaker-hyperpod\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.23.0\n",
      "    Uninstalling zstandard-0.23.0:\n",
      "      Successfully uninstalled zstandard-0.23.0\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.13\n",
      "    Uninstalling widgetsnbextension-4.0.13:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.13\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: jupyterlab_widgets\n",
      "    Found existing installation: jupyterlab_widgets 3.0.13\n",
      "    Uninstalling jupyterlab_widgets-3.0.13:\n",
      "      Successfully uninstalled jupyterlab_widgets-3.0.13\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.8\n",
      "    Uninstalling click-8.1.8:\n",
      "      Successfully uninstalled click-8.1.8\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.39.6\n",
      "    Uninstalling botocore-1.39.6:\n",
      "      Successfully uninstalled botocore-1.39.6\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.13.0\n",
      "    Uninstalling s3transfer-0.13.0:\n",
      "      Successfully uninstalled s3transfer-0.13.0\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 31.0.0\n",
      "    Uninstalling kubernetes-31.0.0:\n",
      "      Successfully uninstalled kubernetes-31.0.0\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.1.5\n",
      "    Uninstalling ipywidgets-8.1.5:\n",
      "      Successfully uninstalled ipywidgets-8.1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "crewai 0.108.0 requires litellm==1.60.2, but you have litellm 1.66.3 which is incompatible.\n",
      "langchain-core 0.3.51 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "instructor 1.7.2 requires rich<14.0.0,>=13.7.0, but you have rich 14.0.0 which is incompatible.\n",
      "sagemaker 2.248.1 requires packaging<25,>=23.0, but you have packaging 25.0 which is incompatible.\n",
      "aiobotocore 2.18.0 requires botocore<1.36.2,>=1.36.0, but you have botocore 1.36.7 which is incompatible.\n",
      "langsmith 0.3.24 requires zstandard<0.24.0,>=0.23.0, but you have zstandard 0.15.2 which is incompatible.\n",
      "streamlit 1.40.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.40.1 requires rich<14,>=10.14.0, but you have rich 14.0.0 which is incompatible.\n",
      "langfuse 2.60.2 requires packaging<25.0,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "langchain-aws 0.2.18 requires boto3>=1.37.0, but you have boto3 1.36.1 which is incompatible.\n",
      "langchain-pinecone 0.2.0 requires aiohttp<3.10,>=3.9.5, but you have aiohttp 3.11.16 which is incompatible.\n",
      "crewai-tools 0.38.1 requires click>=8.1.8, but you have click 8.1.7 which is incompatible.\n",
      "embedchain 0.1.128 requires rich<14.0.0,>=13.7.0, but you have rich 14.0.0 which is incompatible.\n",
      "unstructured-client 0.28.0 requires pydantic<2.10.0,>=2.9.2, but you have pydantic 2.11.3 which is incompatible.\n",
      "unstructured-client 0.28.0 requires python-dateutil==2.8.2, but you have python-dateutil 2.9.0.post0 which is incompatible.\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed awscli-cwlogs-1.4.6 botocore-1.36.7 click-8.1.7 coverage-7.9.2 distlib-0.3.9 hera-workflows-5.16.3 hydra-core-1.3.2 hyperpod-custom-inference-template-1.0 hyperpod-jumpstart-inference-template-1.0 hyperpod-pytorch-job-template-1.0 iniconfig-2.1.0 ipywidgets-8.1.7 itables-2.4.4 jupyterlab_widgets-3.0.15 kubernetes-33.1.0 packaging-25.0 pluggy-1.6.0 pynvml-11.4.1 pyproject-api-1.9.1 pytest-8.3.2 pytest-cov-5.0.0 pytest-order-1.3.0 ratelimit-2.2.1 requests-2.32.4 ruff-0.6.2 s3transfer-0.11.3 sagemaker-hyperpod-3.0.0 tox-4.18.0 tqdm-4.66.5 virtualenv-20.31.2 widgetsnbextension-4.0.14 zstandard-0.15.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Find more information at: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html\n",
      "\n",
      "Basic Commands:\n",
      "  * get-clusters    Get clusters information for HyperPod EKS clusters.\n",
      "  * connect-cluster Creates a connection from users local terminal to the HyperPod cluster \n",
      "                    allowing user to start and preform other basic operations with training jobs.\n",
      "  * start-job       Start a training job from a file on HyperPod cluster.\n",
      "  * get-job         Show details of a specific training job submitted on HyperPod cluster.\n",
      "  * list-jobs       List training job on a HyperPod cluster.\n",
      "  * cancel-job      Cancel training job on a HyperPod cluster.\n",
      "  * patch-job       Patch a job with specific operation on a HyperPod cluster.\n",
      "Troubleshooting and Debugging Commands:\n",
      "  * get-log         Get logs for a pod of training job running on HyperPod cluster.\n",
      "  * list-pods       List all pods associated with a training job on HyperPod cluster.\n",
      "  * exec            Execute a command on a pod of a training job on HyperPod cluster.\n",
      "\n",
      "Usage:\n",
      "  hyperpod [command] [options]\n",
      "\n",
      "Use \"hyperpod <command> --help\" for more information about a given command.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Starting HP CLI Installation.....\"\n",
    "git clone https://github.com/aws/sagemaker-hyperpod-cli.git\n",
    "cd sagemaker-hyperpod-cli && pip install .\n",
    "hyperpod --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9795c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF > runner.sh\n",
    "hyperpod start-job --namespace ${NAMESPACE} --recipe ${RECIPE_PATH} --override-parameters \\\\\n",
    "     '{\"instance_type\": \"${INSTANCE}\",\n",
    "       \"container\": \"${CONTAINER}\", \n",
    "       \"recipes.run.name\": \"${RUN_NAME}\",\n",
    "        \"recipes.run.data_s3_path\": \"${TRAIN_DATA_PATH}\", \n",
    "        \"recipes.run.output_s3_path\": \"${OUTPUT_PATH}\",\n",
    "        \"recipes.run.validation_data_s3_path\": \"${VAL_DATA_PATH}\"}'\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cfa9e",
   "metadata": {},
   "source": [
    "## Step 2: Model fine-tuning\n",
    "\n",
    "We now define the PyTorch estimator to run the supervised fine-tuning on a tool-calling dataset for our Amazon Nova model\n",
    "\n",
    "This section sets up and runs the fine-tuning job using SageMaker. It uses Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) to efficiently train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990bb02",
   "metadata": {},
   "source": [
    "### Launch a Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5b432eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final command: python3 /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/NovaRecipeConfigs/src/hyperpod_cli/sagemaker_hyperpod_recipes/main.py recipes=fine-tuning/nova/nova_micro_p5_gpu_lora_sft cluster_type=k8s cluster=k8s base_results_dir=/Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/results cluster.pullPolicy=\"IfNotPresent\" cluster.restartPolicy=\"OnFailure\" cluster.namespace=\"kubeflow\" instance_type=\"p5.48xlarge\" container=\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\" recipes.run.name=\"demo-sft-hp-nova-micro-run\" recipes.run.data_s3_path=\"s3://sagemaker-us-east-1-905418197933/datasets/nova-sft-peft/train/dataset.jsonl\" recipes.run.output_s3_path=\"s3://sagemaker-us-east-1-905418197933/HP-SFT-RUNS/\" recipes.run.validation_data_s3_path=\"s3://sagemaker-us-east-1-905418197933/datasets/nova-sft-peft/val/dataset.jsonl\"\n",
      "Prepared output directory at /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/results/demo-sft-hp-nova-micro-run-alnr5/k8s_templates\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "Helm script created at /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/results/demo-sft-hp-nova-micro-run-alnr5/demo-sft-hp-nova-micro-run-alnr5_launch.sh\n",
      "Running Helm script: /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/results/demo-sft-hp-nova-micro-run-alnr5/demo-sft-hp-nova-micro-run-alnr5_launch.sh\n",
      "NAME: demo-sft-hp-nova-micro-run-alnr5\n",
      "LAST DEPLOYED: Wed Jul 16 08:07:49 2025\n",
      "NAMESPACE: kubeflow\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "Launcher successfully generated: /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/NovaRecipeConfigs/src/hyperpod_cli/sagemaker_hyperpod_recipes/launcher/nova/k8s_templates/SFT\n",
      "\n",
      "{\n",
      " \"Console URL\": \"https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/cluster-management/hp-cluster\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!bash runner.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4c3a4",
   "metadata": {},
   "source": [
    "### View the Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b30b7972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest folder found: ./results/demo-sft-hp-nova-micro-run-alnr5/\n",
      "Successfully loaded: ./results/demo-sft-hp-nova-micro-run-alnr5/k8s_templates/config/demo-sft-hp-nova-micro-run-alnr5_hydra.yaml, converted to JSON and stored as a job_manifest variable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "def latest_job_manifest(base_directory=\"./results\"):\n",
    "    \"\"\"\n",
    "    Find the latest created folder in the base directory and load YAML file as JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all folders in the base directory\n",
    "        folders = glob.glob(os.path.join(base_directory, \"*/\"))\n",
    "        \n",
    "        if not folders:\n",
    "            print(f\"No folders found in {base_directory}\")\n",
    "            return None\n",
    "        \n",
    "        # Get the latest folder based on creation time\n",
    "        latest_folder = max(folders, key=os.path.getctime)\n",
    "        print(f\"Latest folder found: {latest_folder}\")\n",
    "        \n",
    "        # Construct the path to the YAML file\n",
    "        yaml_path = os.path.join(latest_folder, \"k8s_templates\", \"config\")\n",
    "        \n",
    "        # Check if the k8s_template/config directory exists\n",
    "        if not os.path.exists(yaml_path):\n",
    "            print(f\"Directory {yaml_path} does not exist\")\n",
    "            return None\n",
    "        \n",
    "        # Find YAML files in the config directory\n",
    "        yaml_files = glob.glob(os.path.join(yaml_path, \"*.yaml\")) + glob.glob(os.path.join(yaml_path, \"*.yml\"))\n",
    "        \n",
    "        if not yaml_files:\n",
    "            print(f\"No YAML files found in {yaml_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Load each YAML file and convert to JSON\n",
    "        loaded_data = {}\n",
    "        \n",
    "        for yaml_file in yaml_files:\n",
    "            try:\n",
    "                with open(yaml_file, 'r', encoding='utf-8') as file:\n",
    "                    yaml_content = yaml.safe_load(file)\n",
    "                    \n",
    "                    # Convert to JSON string if needed for parsing\n",
    "                    json_content = json.dumps(yaml_content, indent=2)\n",
    "                    \n",
    "                    # Store both the parsed object and JSON string\n",
    "                    file_name = os.path.basename(yaml_file)\n",
    "                    loaded_data = yaml_content\n",
    "                    \n",
    "                    print(f\"Successfully loaded: {yaml_file}, converted to JSON and stored as a job_manifest variable\")\n",
    "                    \n",
    "            except yaml.YAMLError as e:\n",
    "                print(f\"Error parsing YAML file {yaml_file}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {yaml_file}: {e}\")\n",
    "        \n",
    "        return loaded_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "job_manifest = latest_job_manifest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3961fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_job_name = job_manifest['run']['name']\n",
    "os.environ['JOB_NAME'] = unique_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77c446",
   "metadata": {},
   "source": [
    "### List Hyperpod Training Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0a7ecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\u001b[0;34m   HyperPod Jobs\u001b[0m\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\n",
      "{\n",
      "    \"jobs\": [\n",
      "        {\n",
      "            \"Name\": \"demo-sft-hp-nova-micro-run-alnr5\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-16T15:07:50Z\",\n",
      "            \"State\": \"Running\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-jdebk\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-10T22:52:47Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-vvee7\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-10T19:25:28Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-wsd4j\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T15:47:22Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"eval-g0zj5\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-11T21:28:21Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"my-test-job-djr92\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-11T03:22:12Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"sft-hp-nova-micro-run-beqzh\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T22:12:14Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"test-dewanup-nova-micro-lora-sft-fixed-560ko\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T21:51:02Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bash ../cli_utility/01_manager/hyperpod_job_manager.sh --action list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541e8e9",
   "metadata": {},
   "source": [
    "\n",
    "### Cancel a Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5423a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\u001b[0;34m   Cancel Job: sft-hp-nova-micro-run-apksv\u001b[0m\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[0;32m✓ Job cancellation requested.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bash ../cli_utility/01_manager/hyperpod_job_manager.sh --job_name $JOB_NAME --action cancel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e875c",
   "metadata": {},
   "source": [
    "### Describe a Cluster details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c15eeb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ClusterArn\": \"arn:aws:sagemaker:us-east-1:905418197933:cluster/m9bbgk2h4tx7\",\n",
      "    \"ClusterName\": \"hp-cluster\",\n",
      "    \"ClusterStatus\": \"InService\",\n",
      "    \"CreationTime\": 1751954458.774,\n",
      "    \"InstanceGroups\": [\n",
      "        {\n",
      "            \"CurrentCount\": 1,\n",
      "            \"TargetCount\": 1,\n",
      "            \"InstanceGroupName\": \"accelerated-worker-group-1\",\n",
      "            \"InstanceType\": \"ml.g5.8xlarge\",\n",
      "            \"LifeCycleConfig\": {\n",
      "                \"SourceS3Uri\": \"s3://hp-eks-test-bucket-905418197933-us-east-1\",\n",
      "                \"OnCreate\": \"on_create.sh\"\n",
      "            },\n",
      "            \"ExecutionRole\": \"arn:aws:iam::905418197933:role/hp-eks-test-SMHP-Exec-Role-us-east-1\",\n",
      "            \"ThreadsPerCore\": 1,\n",
      "            \"InstanceStorageConfigs\": [\n",
      "                {\n",
      "                    \"EbsVolumeConfig\": {\n",
      "                        \"VolumeSizeInGB\": 500\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"OnStartDeepHealthChecks\": [\n",
      "                \"InstanceConnectivity\",\n",
      "                \"InstanceStress\"\n",
      "            ],\n",
      "            \"Status\": \"InService\",\n",
      "            \"TrainingPlanStatus\": \"NotApplicable\"\n",
      "        },\n",
      "        {\n",
      "            \"CurrentCount\": 1,\n",
      "            \"TargetCount\": 1,\n",
      "            \"InstanceGroupName\": \"test-new\",\n",
      "            \"InstanceType\": \"ml.m5.xlarge\",\n",
      "            \"LifeCycleConfig\": {\n",
      "                \"SourceS3Uri\": \"s3://hp-eks-test-bucket-905418197933-us-east-1\",\n",
      "                \"OnCreate\": \"on_create.sh\"\n",
      "            },\n",
      "            \"ExecutionRole\": \"arn:aws:iam::905418197933:role/hp-eks-test-SMHP-Exec-Role-us-east-1\",\n",
      "            \"ThreadsPerCore\": 2,\n",
      "            \"InstanceStorageConfigs\": [],\n",
      "            \"Status\": \"InService\",\n",
      "            \"TrainingPlanStatus\": \"NotApplicable\"\n",
      "        }\n",
      "    ],\n",
      "    \"RestrictedInstanceGroups\": [\n",
      "        {\n",
      "            \"CurrentCount\": 2,\n",
      "            \"TargetCount\": 2,\n",
      "            \"InstanceGroupName\": \"restricted-instance-group3\",\n",
      "            \"InstanceType\": \"ml.p5.48xlarge\",\n",
      "            \"ExecutionRole\": \"arn:aws:iam::905418197933:role/hp-eks-test-SMHP-Exec-Role-us-east-1\",\n",
      "            \"ThreadsPerCore\": 1,\n",
      "            \"InstanceStorageConfigs\": [\n",
      "                {\n",
      "                    \"EbsVolumeConfig\": {\n",
      "                        \"VolumeSizeInGB\": 500\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"Status\": \"InService\",\n",
      "            \"TrainingPlanStatus\": \"NotApplicable\",\n",
      "            \"OverrideVpcConfig\": {\n",
      "                \"SecurityGroupIds\": [\n",
      "                    \"sg-0d292fc98e9374ba9\"\n",
      "                ],\n",
      "                \"Subnets\": [\n",
      "                    \"subnet-009c17e0bbdf35342\"\n",
      "                ]\n",
      "            },\n",
      "            \"TrustedEnvironment\": {\n",
      "                \"FSxLustreConfig\": {\n",
      "                    \"SizeInGiB\": 12000,\n",
      "                    \"PerUnitStorageThroughput\": 125\n",
      "                },\n",
      "                \"S3OutputPath\": \"s3://customer-escrow-905418197933-hp-66af7af3\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"VpcConfig\": {\n",
      "        \"SecurityGroupIds\": [\n",
      "            \"sg-0d292fc98e9374ba9\"\n",
      "        ],\n",
      "        \"Subnets\": [\n",
      "            \"subnet-0ddd197a60c9b2195\"\n",
      "        ]\n",
      "    },\n",
      "    \"Orchestrator\": {\n",
      "        \"Eks\": {\n",
      "            \"ClusterArn\": \"arn:aws:eks:us-east-1:905418197933:cluster/my-eks-cluster\"\n",
      "        }\n",
      "    },\n",
      "    \"NodeRecovery\": \"None\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source .env\n",
    "\n",
    "aws sagemaker describe-cluster --cluster-name $CLUSTER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3edeb",
   "metadata": {},
   "source": [
    "### Monitor the Job and CloudWatch Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39b03174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\u001b[0;34m   Job Status\u001b[0m\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\n",
      "{\n",
      "    \"jobs\": [\n",
      "        {\n",
      "            \"Name\": \"demo-sft-hp-nova-micro-run-alnr5\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-16T15:07:50Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-jdebk\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-10T22:52:47Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-vvee7\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-10T19:25:28Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-wsd4j\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T15:47:22Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"eval-g0zj5\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-11T21:28:21Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"my-test-job-djr92\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-11T03:22:12Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"sft-hp-nova-micro-run-beqzh\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T22:12:14Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"test-dewanup-nova-micro-lora-sft-fixed-560ko\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T21:51:02Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "\u001b[0;36m▶ Pod Status: demo-sft-hp-nova-micro-run-alnr5\u001b[0m\n",
      "\n",
      "{\n",
      " \"pods\": [\n",
      "  {\n",
      "   \"PodName\": \"demo-sft-hp-nova-micro-run-alnr5-master-0\",\n",
      "   \"Namespace\": \"kubeflow\",\n",
      "   \"Status\": \"Succeeded\",\n",
      "   \"CreationTime\": \"2025-07-16 15:07:50+00:00\"\n",
      "  },\n",
      "  {\n",
      "   \"PodName\": \"demo-sft-hp-nova-micro-run-alnr5-worker-0\",\n",
      "   \"Namespace\": \"kubeflow\",\n",
      "   \"Status\": \"Succeeded\",\n",
      "   \"CreationTime\": \"2025-07-16 15:07:50+00:00\"\n",
      "  }\n",
      " ]\n",
      "}\n",
      "\n",
      "\u001b[0;36m▶ Pod Details\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[0;36m▶ Pod: demo-sft-hp-nova-micro-run-alnr5-master-0\u001b[0m\n",
      "\n",
      "Name:             demo-sft-hp-nova-micro-run-alnr5-master-0\n",
      "Namespace:        kubeflow\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             hyperpod-i-0110f799aed1ff4f3/172.31.4.115\n",
      "Start Time:       Wed, 16 Jul 2025 08:07:52 -0700\n",
      "Labels:           training.kubeflow.org/job-name=demo-sft-hp-nova-micro-run-alnr5\n",
      "                  training.kubeflow.org/job-role=master\n",
      "                  training.kubeflow.org/operator-name=pytorchjob-controller\n",
      "                  training.kubeflow.org/replica-index=0\n",
      "                  training.kubeflow.org/replica-type=master\n",
      "Annotations:      <none>\n",
      "Status:           Succeeded\n",
      "IP:               172.31.61.99\n",
      "IPs:\n",
      "  IP:           172.31.61.99\n",
      "Controlled By:  PyTorchJob/demo-sft-hp-nova-micro-run-alnr5\n",
      "Init Containers:\n",
      "  init-container:\n",
      "    Container ID:   containerd://a80120f60b1b85d73b47edef06bd92c9fa31cecea65ed1ae8c944cac9dfdaf70\n",
      "    Image:          708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\n",
      "    Image ID:       708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo@sha256:c126d5d1632ba2ade3daf30624798339ee43159b1d840a20b5a0d8a23c267676\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Wed, 16 Jul 2025 08:07:53 -0700\n",
      "      Finished:     Wed, 16 Jul 2025 08:07:55 -0700\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /config from training-config (rw)\n",
      "      /sagemaker/fsx/init from fsx-path-init (rw)\n",
      "      /var/log/aws/clusters from logs (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cb4dl (ro)\n",
      "Containers:\n",
      "  pytorch:\n",
      "    Container ID:   containerd://5f7d41657ac283be50d18706f8aedc876f14990bbdf06947bec0db4ca717f919\n",
      "    Image:          708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\n",
      "    Image ID:       708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo@sha256:7b25e6ad326621ed300d314ddf61837ee4cc5d9f09021c0106ccb58f3bd27fdf\n",
      "    Port:           23456/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Wed, 16 Jul 2025 08:08:00 -0700\n",
      "      Finished:     Wed, 16 Jul 2025 08:54:21 -0700\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      nvidia.com/gpu:  8\n",
      "    Requests:\n",
      "      nvidia.com/gpu:  8\n",
      "    Environment:\n",
      "      X_AMZ_SOURCE_ACCOUNT:  905418197933\n",
      "      PYTHONUNBUFFERED:      1\n",
      "      MASTER_PORT:           23456\n",
      "      PET_MASTER_PORT:       23456\n",
      "      MASTER_ADDR:           demo-sft-hp-nova-micro-run-alnr5-master-0\n",
      "      PET_MASTER_ADDR:       demo-sft-hp-nova-micro-run-alnr5-master-0\n",
      "      WORLD_SIZE:            2\n",
      "      RANK:                  0\n",
      "      PET_NPROC_PER_NODE:    auto\n",
      "      PET_NODE_RANK:         0\n",
      "      PET_NNODES:            2\n",
      "    Mounts:\n",
      "      /config from training-config (rw)\n",
      "      /dev/shm from dshm (rw)\n",
      "      /sagemaker/fsx/main from fsx-path-main (rw)\n",
      "      /sagemaker/fsx/output_artifacts from fsx-path-output-artifacts (rw)\n",
      "      /tmp from tmp-storage (rw)\n",
      "      /var/log/aws/clusters from logs (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cb4dl (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   False \n",
      "  Initialized                 True \n",
      "  Ready                       False \n",
      "  ContainersReady             False \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  fsx-path-main:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/main\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  fsx-path-init:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/init\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  training-config:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      training-config-demo-sft-hp-nova-micro-run-alnr5\n",
      "    Optional:  false\n",
      "  dshm:\n",
      "    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n",
      "    Medium:     Memory\n",
      "    SizeLimit:  <unset>\n",
      "  logs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/aws/clusters\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  fsx-path-output-artifacts:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/output_artifacts\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  tmp-storage:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/dlami/nvme\n",
      "    HostPathType:  Directory\n",
      "  kube-api-access-cb4dl:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "                             nvidia.com/gpu:NoSchedule op=Exists\n",
      "                             sagemaker.amazonaws.com/RestrictedNode=Worker:NoSchedule\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  55m   default-scheduler  Successfully assigned kubeflow/demo-sft-hp-nova-micro-run-alnr5-master-0 to hyperpod-i-0110f799aed1ff4f3\n",
      "  Normal  Pulling    55m   kubelet            Pulling image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\"\n",
      "  Normal  Pulled     55m   kubelet            Successfully pulled image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\" in 220ms (220ms including waiting). Image size: 246304701 bytes.\n",
      "  Normal  Created    55m   kubelet            Created container init-container\n",
      "  Normal  Started    55m   kubelet            Started container init-container\n",
      "  Normal  Pulling    54m   kubelet            Pulling image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\"\n",
      "  Normal  Pulled     54m   kubelet            Successfully pulled image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\" in 3.823s (3.823s including waiting). Image size: 20797145303 bytes.\n",
      "  Normal  Created    54m   kubelet            Created container pytorch\n",
      "  Normal  Started    54m   kubelet            Started container pytorch\n",
      "\n",
      "\u001b[0;36m▶ Pod: demo-sft-hp-nova-micro-run-alnr5-worker-0\u001b[0m\n",
      "\n",
      "Name:             demo-sft-hp-nova-micro-run-alnr5-worker-0\n",
      "Namespace:        kubeflow\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             hyperpod-i-0f98b43f2eb4bf47a/172.31.9.136\n",
      "Start Time:       Wed, 16 Jul 2025 08:07:52 -0700\n",
      "Labels:           training.kubeflow.org/job-name=demo-sft-hp-nova-micro-run-alnr5\n",
      "                  training.kubeflow.org/operator-name=pytorchjob-controller\n",
      "                  training.kubeflow.org/replica-index=0\n",
      "                  training.kubeflow.org/replica-type=worker\n",
      "Annotations:      <none>\n",
      "Status:           Succeeded\n",
      "IP:               172.31.33.84\n",
      "IPs:\n",
      "  IP:           172.31.33.84\n",
      "Controlled By:  PyTorchJob/demo-sft-hp-nova-micro-run-alnr5\n",
      "Init Containers:\n",
      "  init-container:\n",
      "    Container ID:   containerd://c6bfc89eaf4c325e07ca4a71e9c678eeefaa542c90dcddb6afd0e88d6abb3988\n",
      "    Image:          708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\n",
      "    Image ID:       708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo@sha256:c126d5d1632ba2ade3daf30624798339ee43159b1d840a20b5a0d8a23c267676\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Wed, 16 Jul 2025 08:07:53 -0700\n",
      "      Finished:     Wed, 16 Jul 2025 08:07:55 -0700\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /config from training-config (rw)\n",
      "      /sagemaker/fsx/init from fsx-path-init (rw)\n",
      "      /var/log/aws/clusters from logs (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-czkgs (ro)\n",
      "  init-pytorch:\n",
      "    Container ID:  containerd://5ef65db2d06b4ff425d7486049d5f3cc760f603cba318a2f4020f8b5946f65f3\n",
      "    Image:         alpine:3.10\n",
      "    Image ID:      708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo@sha256:e515aad2ed234a5072c4d2ef86a1cb77d5bfe4b11aa865d9214875734c4eeb3c\n",
      "    Port:          <none>\n",
      "    Host Port:     <none>\n",
      "    Command:\n",
      "      sh\n",
      "      -c\n",
      "      err=1;for i in $(seq 100); do if nslookup demo-sft-hp-nova-micro-run-alnr5-master-0; then err=0 && break; fi;echo waiting for master; sleep 2; done; exit $err\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Wed, 16 Jul 2025 08:07:56 -0700\n",
      "      Finished:     Wed, 16 Jul 2025 08:08:17 -0700\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      cpu:     100m\n",
      "      memory:  20Mi\n",
      "    Requests:\n",
      "      cpu:        50m\n",
      "      memory:     10Mi\n",
      "    Environment:  <none>\n",
      "    Mounts:\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-czkgs (ro)\n",
      "Containers:\n",
      "  pytorch:\n",
      "    Container ID:   containerd://add58b1d6bfdd59e052dfb7120c8fd622ecdacb19612c976db115200706be660\n",
      "    Image:          708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\n",
      "    Image ID:       708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo@sha256:7b25e6ad326621ed300d314ddf61837ee4cc5d9f09021c0106ccb58f3bd27fdf\n",
      "    Port:           23456/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Wed, 16 Jul 2025 08:08:22 -0700\n",
      "      Finished:     Wed, 16 Jul 2025 08:45:24 -0700\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      nvidia.com/gpu:  8\n",
      "    Requests:\n",
      "      nvidia.com/gpu:  8\n",
      "    Environment:\n",
      "      X_AMZ_SOURCE_ACCOUNT:  905418197933\n",
      "      PYTHONUNBUFFERED:      1\n",
      "      MASTER_PORT:           23456\n",
      "      PET_MASTER_PORT:       23456\n",
      "      MASTER_ADDR:           demo-sft-hp-nova-micro-run-alnr5-master-0\n",
      "      PET_MASTER_ADDR:       demo-sft-hp-nova-micro-run-alnr5-master-0\n",
      "      WORLD_SIZE:            2\n",
      "      RANK:                  1\n",
      "      PET_NPROC_PER_NODE:    auto\n",
      "      PET_NODE_RANK:         1\n",
      "      PET_NNODES:            2\n",
      "    Mounts:\n",
      "      /config from training-config (rw)\n",
      "      /dev/shm from dshm (rw)\n",
      "      /sagemaker/fsx/main from fsx-path-main (rw)\n",
      "      /sagemaker/fsx/output_artifacts from fsx-path-output-artifacts (rw)\n",
      "      /tmp from tmp-storage (rw)\n",
      "      /var/log/aws/clusters from logs (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-czkgs (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   False \n",
      "  Initialized                 True \n",
      "  Ready                       False \n",
      "  ContainersReady             False \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  fsx-path-main:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/main\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  fsx-path-init:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/init\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  training-config:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      training-config-demo-sft-hp-nova-micro-run-alnr5\n",
      "    Optional:  false\n",
      "  dshm:\n",
      "    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n",
      "    Medium:     Memory\n",
      "    SizeLimit:  <unset>\n",
      "  logs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/aws/clusters\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  fsx-path-output-artifacts:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/output_artifacts\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  tmp-storage:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/dlami/nvme\n",
      "    HostPathType:  Directory\n",
      "  kube-api-access-czkgs:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "                             nvidia.com/gpu:NoSchedule op=Exists\n",
      "                             sagemaker.amazonaws.com/RestrictedNode=Worker:NoSchedule\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  55m   default-scheduler  Successfully assigned kubeflow/demo-sft-hp-nova-micro-run-alnr5-worker-0 to hyperpod-i-0f98b43f2eb4bf47a\n",
      "  Normal  Pulling    55m   kubelet            Pulling image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\"\n",
      "  Normal  Pulled     55m   kubelet            Successfully pulled image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\" in 211ms (211ms including waiting). Image size: 246304701 bytes.\n",
      "  Normal  Created    55m   kubelet            Created container init-container\n",
      "  Normal  Started    55m   kubelet            Started container init-container\n",
      "  Normal  Pulled     54m   kubelet            Container image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:alpine-3.10\" already present on machine\n",
      "  Normal  Created    54m   kubelet            Created container init-pytorch\n",
      "  Normal  Started    54m   kubelet            Started container init-pytorch\n",
      "  Normal  Pulling    54m   kubelet            Pulling image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\"\n",
      "  Normal  Pulled     54m   kubelet            Successfully pulled image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\" in 3.217s (3.217s including waiting). Image size: 20797145303 bytes.\n",
      "  Normal  Created    54m   kubelet            Created container pytorch\n",
      "  Normal  Started    54m   kubelet            Started container pytorch\n",
      "\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\u001b[0;34m   CloudWatch Logs\u001b[0m\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[0;36m▶ Log Configuration\u001b[0m\n",
      "\n",
      "Log Group: /aws/sagemaker/Clusters/hp-cluster/m9bbgk2h4tx7\n",
      "Stream prefix: SagemakerHyperPodTrainingJob/restricted-instance-group3\n",
      "\n",
      "\u001b[0;36m▶ Log Streams\u001b[0m\n",
      "\n",
      "\u001b[1;33m⚠ No streams found with prefix, trying broader search...\u001b[0m\n",
      "\u001b[0;32m✓ Found log streams:\u001b[0m\n",
      "  - SagemakerHyperPodTrainingJob/restricted-instance-group3/i-0f98b43f2eb4bf47a\n",
      "  - SagemakerHyperPodTrainingJob/restricted-instance-group3/i-0110f799aed1ff4f3\n",
      "  - \n",
      "\n",
      "\u001b[0;36m▶ Recent Logs (Last 100 lines per stream)\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[0;36m▶ Log Stream: SagemakerHyperPodTrainingJob/restricted-instance-group3/i-0f98b43f2eb4bf47a\u001b[0m\n",
      "\n",
      "Fetching recent logs...\n",
      "Creating parquet from Arrow format: 100%|##########| 1/1 [00:00<00:00, 628.64ba/s]\t[2025-07-11 21:36:27,126] Finished evaluation, Closing the inference server\t[2025-07-11 21:36:27,146] ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmprwerp4vb'> (in /usr/local/lib/python3.12/tempfile.py:936)n': {'name': 'eval-g0zj5', 'model_type': 'amazon.nova-micro-v1:0:128k', 'model_name_or_path': 's3://customer-escrow-905418197933-hp-66af7af3/dewanup-nova-micro-lora-sft-fixed-jdebk/outputs/checkpoints', 'replicas': 1, 'data_s3_path': '', 'tensorboard_s3_path': '', 'output_s3_path': 's3://905418197933-realm-data/HF/ace_reason_math/1750697426/output-artifacts/eval-results/'}, 'evaluation': {'task': 'mmlu', 'strategy': 'zs_cot', 'metric': 'accuracy', 'subtask': 'anatomy'}, 'inference': {'max_new_tokens': 8196, 'top_k': -1, 'top_p': 1.0, 'temperature': 0}}\t[2025-07-11 21:32:08,537] Run name: eval-g0zj5\t[2025-07-11 21:32:08,570] Found the model, Loading model weights\t[2025-07-11 21:32:08,570] Starting Inferencing server\t[2025-07-11 21:32:08,585] Inferencing server still starting...\t[2025-07-11 21:32:08,796] INFO 07-11 21:32:08 [importing.py:53] Triton module has been replaced with a placeholder.\t[2025-07-11 21:32:08,841] INFO 07-11 21:32:08 [__init__.py:239] Automatically detected platform cuda.\t[2025-07-11 21:32:38,587] Inferencing server still starting...\t[2025-07-11 21:33:08,589] Inferencing server still starting...\t[2025-07-11 21:33:38,590] Inferencing server still starting...\t[2025-07-11 21:34:08,592] Inferencing server still starting...\t[2025-07-11 21:34:38,594] Inferencing server still starting...\t[2025-07-11 21:35:10,595] Inferencing server still starting...\t[2025-07-11 21:36:14,598] Inferencing server has started.\t[2025-07-11 21:36:14,598] Starting evaluation with task mmlu\t[2025-07-11 21:36:17,720] INFO 07-11 21:36:17 [importing.py:53] Triton module has been replaced with a placeholder.\t[2025-07-11 21:36:17,870] INFO 07-11 21:36:17 [__init__.py:239] Automatically detected platform cuda.\t[2025-07-11 21:36:21,270] \n",
      "\n",
      "\u001b[0;36m▶ Log Stream: SagemakerHyperPodTrainingJob/restricted-instance-group3/i-0110f799aed1ff4f3\u001b[0m\n",
      "\n",
      "Fetching recent logs...\n",
      "Epoch 1: : 523it [38:06,  4.37s/it, v_num=, reduced_train_loss=0.874, global_step=1141.0, consumed_samples=73088.0, train_step_timing in s=3.340, batch_time=4.440, samples/sec=14.40]train loss calculated 0.8742266893386841\n",
      "Epoch 1: : 524it [38:10,  4.37s/it, v_num=, reduced_train_loss=0.888, global_step=1142.0, consumed_samples=73152.0, train_step_timing in s=3.320, batch_time=4.340, samples/sec=14.70]train loss calculated 0.8881533145904541\n",
      "Epoch 1: : 525it [38:15,  4.37s/it, v_num=, reduced_train_loss=0.857, global_step=1143.0, consumed_samples=73216.0, train_step_timing in s=3.380, batch_time=4.280, samples/sec=14.90]train loss calculated 0.8571255207061768\n",
      "Epoch 1: : 526it [38:19,  4.37s/it, v_num=, reduced_train_loss=0.961, global_step=1144.0, consumed_samples=73280.0, train_step_timing in s=3.290, batch_time=4.310, samples/sec=14.90]train loss calculated 0.9605405330657959\n",
      "Epoch 1: : 527it [38:23,  4.37s/it, v_num=, reduced_train_loss=0.827, global_step=1145.0, consumed_samples=73344.0, train_step_timing in s=3.210, batch_time=4.280, samples/sec=15.00]train loss calculated 0.8268367052078247\n",
      "Epoch 1: : 528it [38:27,  4.37s/it, v_num=, reduced_train_loss=0.834, global_step=1146.0, consumed_samples=73408.0, train_step_timing in s=3.310, batch_time=4.230, samples/sec=15.10]train loss calculated 0.8337791562080383\n",
      "Epoch 1: : 529it [38:32,  4.37s/it, v_num=, reduced_train_loss=0.788, global_step=1147.0, consumed_samples=73472.0, train_step_timing in s=3.340, batch_time=4.340, samples/sec=14.80]train loss calculated 0.78849196434021\n",
      "Epoch 1: : 530it [38:36,  4.37s/it, v_num=, reduced_train_loss=0.849, global_step=1148.0, consumed_samples=73536.0, train_step_timing in s=2.990, batch_time=4.150, samples/sec=15.40]train loss calculated 0.8493067026138306\n",
      "Epoch 1: : 531it [38:40,  4.37s/it, v_num=, reduced_train_loss=0.865, global_step=1149.0, consumed_samples=73600.0, train_step_timing in s=3.480, batch_time=4.020, samples/sec=15.90]train loss calculated 0.8653340339660645\n",
      "Epoch 1: : 532it [38:45,  4.37s/it, v_num=, reduced_train_loss=0.841, global_step=1150.0, consumed_samples=73664.0, train_step_timing in s=3.430, batch_time=4.500, samples/sec=14.20]train loss calculated 0.8407489061355591\n",
      "Epoch 1: : 533it [38:49,  4.37s/it, v_num=, reduced_train_loss=0.950, global_step=1151.0, consumed_samples=73728.0, train_step_timing in s=3.390, batch_time=4.370, samples/sec=14.60]train loss calculated 0.9502102136611938\n",
      "Epoch 1: : 534it [38:53,  4.37s/it, v_num=, reduced_train_loss=0.858, global_step=1152.0, consumed_samples=73792.0, train_step_timing in s=3.290, batch_time=4.440, samples/sec=14.40]train loss calculated 0.8581693172454834\n",
      "Epoch 1: : 535it [38:58,  4.37s/it, v_num=, reduced_train_loss=0.890, global_step=1153.0, consumed_samples=73856.0, train_step_timing in s=3.390, batch_time=4.340, samples/sec=14.70]train loss calculated 0.8895231485366821\n",
      "Epoch 1: : 536it [39:02,  4.37s/it, v_num=, reduced_train_loss=0.914, global_step=1154.0, consumed_samples=73920.0, train_step_timing in s=3.390, batch_time=4.370, samples/sec=14.60]train loss calculated 0.9141672849655151\n",
      "Epoch 1: : 537it [39:06,  4.37s/it, v_num=, reduced_train_loss=0.914, global_step=1155.0, consumed_samples=7.4e+4, train_step_timing in s=3.270, batch_time=4.360, samples/sec=14.70] train loss calculated 0.91437828540802\n",
      "Epoch 1: : 538it [39:11,  4.37s/it, v_num=, reduced_train_loss=0.877, global_step=1156.0, consumed_samples=7.4e+4, train_step_timing in s=3.410, batch_time=4.260, samples/sec=15.00]train loss calculated 0.8772664070129395\n",
      "Epoch 1: : 539it [39:15,  4.37s/it, v_num=, reduced_train_loss=0.908, global_step=1157.0, consumed_samples=74112.0, train_step_timing in s=3.400, batch_time=4.340, samples/sec=14.70]train loss calculated 0.9077670574188232\n",
      "Epoch 1: : 540it [39:19,  4.37s/it, v_num=, reduced_train_loss=0.875, global_step=1158.0, consumed_samples=74176.0, train_step_timing in s=3.380, batch_time=4.370, samples/sec=14.60]train loss calculated 0.8754771947860718\n",
      "Epoch 1: : 541it [39:24,  4.37s/it, v_num=, reduced_train_loss=0.818, global_step=1159.0, consumed_samples=74240.0, train_step_timing in s=3.380, batch_time=4.370, samples/sec=14.60]train loss calculated 0.8175592422485352\n",
      "Epoch 1: : 542it [39:28,  4.37s/it, v_num=, reduced_train_loss=0.927, global_step=1160.0, consumed_samples=74304.0, train_step_timing in s=3.350, batch_time=4.290, samples/sec=14.90]train loss calculated 0.9265929460525513\n",
      "Epoch 1: : 543it [39:32,  4.37s/it, v_num=, reduced_train_loss=0.827, global_step=1161.0, consumed_samples=74368.0, train_step_timing in s=3.290, batch_time=4.330, samples/sec=14.80]train loss calculated 0.8274103403091431\n",
      "Epoch 1: : 544it [39:37,  4.37s/it, v_num=, reduced_train_loss=0.941, global_step=1162.0, consumed_samples=74432.0, train_step_timing in s=3.290, batch_time=4.350, samples/sec=14.70]train loss calculated 0.9414460062980652\n",
      "Epoch 1: : 545it [39:41,  4.37s/it, v_num=, reduced_train_loss=0.807, global_step=1163.0, consumed_samples=74496.0, train_step_timing in s=3.400, batch_time=4.300, samples/sec=14.90]train loss calculated 0.8070048093795776\n",
      "Epoch 1: : 546it [39:45,  4.37s/it, v_num=, reduced_train_loss=0.820, global_step=1164.0, consumed_samples=74560.0, train_step_timing in s=3.310, batch_time=4.450, samples/sec=14.40]train loss calculated 0.820473313331604\n",
      "Epoch 1: : 547it [39:50,  4.37s/it, v_num=, reduced_train_loss=0.950, global_step=1165.0, consumed_samples=74624.0, train_step_timing in s=3.340, batch_time=4.310, samples/sec=14.80]train loss calculated 0.9501339197158813\n",
      "Epoch 1: : 548it [39:54,  4.37s/it, v_num=, reduced_train_loss=0.937, global_step=1166.0, consumed_samples=74688.0, train_step_timing in s=3.360, batch_time=4.320, samples/sec=14.80]train loss calculated 0.9365205764770508\n",
      "Epoch 1: : 549it [39:58,  4.37s/it, v_num=, reduced_train_loss=0.777, global_step=1167.0, consumed_samples=74752.0, train_step_timing in s=3.320, batch_time=4.390, samples/sec=14.60]train loss calculated 0.7774803638458252\n",
      "Epoch 1: : 550it [40:03,  4.37s/it, v_num=, reduced_train_loss=0.785, global_step=1168.0, consumed_samples=74816.0, train_step_timing in s=3.300, batch_time=4.300, samples/sec=14.90]train loss calculated 0.7846362590789795\n",
      "Epoch 1: : 551it [40:07,  4.37s/it, v_num=, reduced_train_loss=0.864, global_step=1169.0, consumed_samples=74880.0, train_step_timing in s=3.260, batch_time=4.290, samples/sec=14.90]train loss calculated 0.864311933517456\n",
      "Epoch 1: : 552it [40:11,  4.37s/it, v_num=, reduced_train_loss=0.770, global_step=1170.0, consumed_samples=74944.0, train_step_timing in s=3.350, batch_time=4.270, samples/sec=15.00]train loss calculated 0.7704010009765625\n",
      "Epoch 1: : 553it [40:16,  4.37s/it, v_num=, reduced_train_loss=0.938, global_step=1171.0, consumed_samples=7.5e+4, train_step_timing in s=3.380, batch_time=4.320, samples/sec=14.80] train loss calculated 0.9375468492507935\n",
      "Epoch 1: : 554it [40:20,  4.37s/it, v_num=, reduced_train_loss=0.951, global_step=1172.0, consumed_samples=75072.0, train_step_timing in s=3.440, batch_time=4.320, samples/sec=14.80]train loss calculated 0.9509410262107849\n",
      "Epoch 1: : 555it [40:24,  4.37s/it, v_num=, reduced_train_loss=0.777, global_step=1173.0, consumed_samples=75136.0, train_step_timing in s=3.450, batch_time=4.390, samples/sec=14.60]train loss calculated 0.7774874567985535\n",
      "Epoch 1: : 556it [40:29,  4.37s/it, v_num=, reduced_train_loss=0.895, global_step=1174.0, consumed_samples=75200.0, train_step_timing in s=3.360, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8945023417472839\n",
      "Epoch 1: : 557it [40:33,  4.37s/it, v_num=, reduced_train_loss=0.850, global_step=1175.0, consumed_samples=75264.0, train_step_timing in s=3.280, batch_time=4.330, samples/sec=14.80]train loss calculated 0.8498396277427673\n",
      "Epoch 1: : 558it [40:37,  4.37s/it, v_num=, reduced_train_loss=0.798, global_step=1176.0, consumed_samples=75328.0, train_step_timing in s=3.250, batch_time=4.340, samples/sec=14.70]train loss calculated 0.7983971238136292\n",
      "Epoch 1: : 559it [40:42,  4.37s/it, v_num=, reduced_train_loss=0.802, global_step=1177.0, consumed_samples=75392.0, train_step_timing in s=3.410, batch_time=4.240, samples/sec=15.10]train loss calculated 0.8024945259094238\n",
      "Epoch 1: : 560it [40:46,  4.37s/it, v_num=, reduced_train_loss=0.838, global_step=1178.0, consumed_samples=75456.0, train_step_timing in s=3.390, batch_time=4.380, samples/sec=14.60]train loss calculated 0.8378608226776123\n",
      "Epoch 1: : 561it [40:50,  4.37s/it, v_num=, reduced_train_loss=0.921, global_step=1179.0, consumed_samples=75520.0, train_step_timing in s=3.270, batch_time=4.370, samples/sec=14.70]train loss calculated 0.9207227230072021\n",
      "Epoch 1: : 562it [40:55,  4.37s/it, v_num=, reduced_train_loss=0.963, global_step=1180.0, consumed_samples=75584.0, train_step_timing in s=3.420, batch_time=4.280, samples/sec=15.00]train loss calculated 0.9632612466812134\n",
      "Epoch 1: : 563it [40:59,  4.37s/it, v_num=, reduced_train_loss=0.864, global_step=1181.0, consumed_samples=75648.0, train_step_timing in s=3.370, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8644891977310181\n",
      "Epoch 1: : 564it [41:04,  4.37s/it, v_num=, reduced_train_loss=0.947, global_step=1182.0, consumed_samples=75712.0, train_step_timing in s=3.430, batch_time=4.330, samples/sec=14.80]train loss calculated 0.94734787940979\n",
      "Epoch 1: : 565it [41:08,  4.37s/it, v_num=, reduced_train_loss=0.861, global_step=1183.0, consumed_samples=75776.0, train_step_timing in s=3.650, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8614346981048584\n",
      "Epoch 1: : 566it [41:13,  4.37s/it, v_num=, reduced_train_loss=0.898, global_step=1184.0, consumed_samples=75840.0, train_step_timing in s=3.350, batch_time=4.610, samples/sec=13.90]train loss calculated 0.8982287049293518\n",
      "Epoch 1: : 567it [41:17,  4.37s/it, v_num=, reduced_train_loss=0.864, global_step=1185.0, consumed_samples=75904.0, train_step_timing in s=3.300, batch_time=4.400, samples/sec=14.50]train loss calculated 0.8639969229698181\n",
      "Epoch 1: : 568it [41:21,  4.37s/it, v_num=, reduced_train_loss=0.812, global_step=1186.0, consumed_samples=7.6e+4, train_step_timing in s=3.420, batch_time=4.310, samples/sec=14.80] train loss calculated 0.8119203448295593\n",
      "Epoch 1: : 569it [41:26,  4.37s/it, v_num=, reduced_train_loss=1.050, global_step=1187.0, consumed_samples=7.6e+4, train_step_timing in s=3.750, batch_time=4.440, samples/sec=14.40]train loss calculated 1.0535669326782227\n",
      "Epoch 1: : 570it [41:30,  4.37s/it, v_num=, reduced_train_loss=0.852, global_step=1188.0, consumed_samples=76096.0, train_step_timing in s=3.360, batch_time=4.670, samples/sec=13.70]train loss calculated 0.851875901222229\n",
      "Epoch 1: : 571it [41:35,  4.37s/it, v_num=, reduced_train_loss=0.897, global_step=1189.0, consumed_samples=76160.0, train_step_timing in s=3.330, batch_time=4.330, samples/sec=14.80]train loss calculated 0.8968481421470642\n",
      "Epoch 1: : 572it [41:39,  4.37s/it, v_num=, reduced_train_loss=0.918, global_step=1190.0, consumed_samples=76224.0, train_step_timing in s=3.360, batch_time=4.370, samples/sec=14.60]train loss calculated 0.9178856611251831\n",
      "Epoch 1: : 573it [41:43,  4.37s/it, v_num=, reduced_train_loss=0.846, global_step=1191.0, consumed_samples=76288.0, train_step_timing in s=3.270, batch_time=4.300, samples/sec=14.90]train loss calculated 0.8460735082626343\n",
      "Epoch 1: : 574it [41:48,  4.37s/it, v_num=, reduced_train_loss=0.860, global_step=1192.0, consumed_samples=76352.0, train_step_timing in s=3.340, batch_time=4.290, samples/sec=14.90]train loss calculated 0.8600265979766846\n",
      "Epoch 1: : 575it [41:52,  4.37s/it, v_num=, reduced_train_loss=1.000, global_step=1193.0, consumed_samples=76416.0, train_step_timing in s=3.350, batch_time=4.260, samples/sec=15.00]train loss calculated 1.004220962524414\n",
      "Epoch 1: : 576it [41:56,  4.37s/it, v_num=, reduced_train_loss=0.893, global_step=1194.0, consumed_samples=76480.0, train_step_timing in s=3.310, batch_time=4.350, samples/sec=14.70]train loss calculated 0.8926070332527161\n",
      "Epoch 1: : 577it [42:01,  4.37s/it, v_num=, reduced_train_loss=0.855, global_step=1195.0, consumed_samples=76544.0, train_step_timing in s=3.300, batch_time=4.300, samples/sec=14.90]train loss calculated 0.8547406196594238\n",
      "Epoch 1: : 578it [42:05,  4.37s/it, v_num=, reduced_train_loss=0.856, global_step=1196.0, consumed_samples=76608.0, train_step_timing in s=3.490, batch_time=4.380, samples/sec=14.60]train loss calculated 0.8564959764480591\n",
      "Epoch 1: : 579it [42:09,  4.37s/it, v_num=, reduced_train_loss=0.883, global_step=1197.0, consumed_samples=76672.0, train_step_timing in s=3.350, batch_time=4.510, samples/sec=14.20]train loss calculated 0.8826826810836792\n",
      "Epoch 1: : 580it [42:14,  4.37s/it, v_num=, reduced_train_loss=0.922, global_step=1198.0, consumed_samples=76736.0, train_step_timing in s=3.410, batch_time=4.270, samples/sec=15.00]train loss calculated 0.9217593669891357\n",
      "Epoch 1: : 581it [42:18,  4.37s/it, v_num=, reduced_train_loss=0.844, global_step=1199.0, consumed_samples=76800.0, train_step_timing in s=3.550, batch_time=4.340, samples/sec=14.80]train loss calculated 0.8439275026321411\n",
      "Epoch 1: : 582it [42:23,  4.37s/it, v_num=, reduced_train_loss=0.822, global_step=1200.0, consumed_samples=76864.0, train_step_timing in s=3.350, batch_time=4.550, samples/sec=14.10]train loss calculated 0.8222110271453857\n",
      "Epoch 1: : 583it [42:27,  4.37s/it, v_num=, reduced_train_loss=0.821, global_step=1201.0, consumed_samples=76928.0, train_step_timing in s=3.370, batch_time=4.320, samples/sec=14.80]train loss calculated 0.8209871649742126\n",
      "Epoch 1: : 584it [42:31,  4.37s/it, v_num=, reduced_train_loss=0.829, global_step=1202.0, consumed_samples=7.7e+4, train_step_timing in s=3.320, batch_time=4.410, samples/sec=14.50] train loss calculated 0.8286842107772827\n",
      "Epoch 1: : 585it [42:36,  4.37s/it, v_num=, reduced_train_loss=0.968, global_step=1203.0, consumed_samples=77056.0, train_step_timing in s=3.380, batch_time=4.330, samples/sec=14.80]train loss calculated 0.9683666229248047\n",
      "Epoch 1: : 586it [42:40,  4.37s/it, v_num=, reduced_train_loss=0.797, global_step=1204.0, consumed_samples=77120.0, train_step_timing in s=3.350, batch_time=4.350, samples/sec=14.70]train loss calculated 0.7974674701690674\n",
      "Epoch 1: : 587it [42:44,  4.37s/it, v_num=, reduced_train_loss=0.826, global_step=1205.0, consumed_samples=77184.0, train_step_timing in s=3.340, batch_time=4.340, samples/sec=14.80]train loss calculated 0.8257066011428833\n",
      "Epoch 1: : 588it [42:49,  4.37s/it, v_num=, reduced_train_loss=0.912, global_step=1206.0, consumed_samples=77248.0, train_step_timing in s=3.360, batch_time=4.410, samples/sec=14.50]train loss calculated 0.9121530652046204\n",
      "Epoch 1: : 589it [42:53,  4.37s/it, v_num=, reduced_train_loss=0.881, global_step=1207.0, consumed_samples=77312.0, train_step_timing in s=3.410, batch_time=4.350, samples/sec=14.70]train loss calculated 0.88130784034729\n",
      "Epoch 1: : 590it [42:58,  4.37s/it, v_num=, reduced_train_loss=0.830, global_step=1208.0, consumed_samples=77376.0, train_step_timing in s=3.360, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8299692869186401\n",
      "Epoch 1: : 591it [43:02,  4.37s/it, v_num=, reduced_train_loss=0.886, global_step=1209.0, consumed_samples=77440.0, train_step_timing in s=3.410, batch_time=4.380, samples/sec=14.60]train loss calculated 0.8862640857696533\n",
      "Epoch 1: : 592it [43:07,  4.37s/it, v_num=, reduced_train_loss=0.757, global_step=1210.0, consumed_samples=77504.0, train_step_timing in s=3.540, batch_time=4.420, samples/sec=14.50]train loss calculated 0.7567726969718933\n",
      "Epoch 1: : 593it [43:11,  4.37s/it, v_num=, reduced_train_loss=0.980, global_step=1211.0, consumed_samples=77568.0, train_step_timing in s=3.310, batch_time=4.520, samples/sec=14.20]train loss calculated 0.9804074764251709\n",
      "Epoch 1: : 594it [43:15,  4.37s/it, v_num=, reduced_train_loss=0.927, global_step=1212.0, consumed_samples=77632.0, train_step_timing in s=3.300, batch_time=4.330, samples/sec=14.80]train loss calculated 0.9272388815879822\n",
      "Epoch 1: : 595it [43:19,  4.37s/it, v_num=, reduced_train_loss=0.937, global_step=1213.0, consumed_samples=77696.0, train_step_timing in s=3.300, batch_time=4.270, samples/sec=15.00]train loss calculated 0.936908483505249\n",
      "Epoch 1: : 596it [43:24,  4.37s/it, v_num=, reduced_train_loss=0.827, global_step=1214.0, consumed_samples=77760.0, train_step_timing in s=3.320, batch_time=4.310, samples/sec=14.80]train loss calculated 0.8270081281661987\n",
      "Epoch 1: : 597it [43:28,  4.37s/it, v_num=, reduced_train_loss=0.973, global_step=1215.0, consumed_samples=77824.0, train_step_timing in s=3.430, batch_time=4.330, samples/sec=14.80]train loss calculated 0.9727742671966553\n",
      "Epoch 1: : 598it [43:33,  4.37s/it, v_num=, reduced_train_loss=0.808, global_step=1216.0, consumed_samples=77888.0, train_step_timing in s=3.330, batch_time=4.410, samples/sec=14.50]train loss calculated 0.8077811002731323\n",
      "Epoch 1: : 599it [43:37,  4.37s/it, v_num=, reduced_train_loss=0.902, global_step=1217.0, consumed_samples=7.8e+4, train_step_timing in s=3.350, batch_time=4.340, samples/sec=14.70] train loss calculated 0.901864767074585\n",
      "Epoch 1: : 600it [43:41,  4.37s/it, v_num=, reduced_train_loss=0.826, global_step=1218.0, consumed_samples=7.8e+4, train_step_timing in s=3.350, batch_time=4.290, samples/sec=14.90]train loss calculated 0.8260514140129089\n",
      "Epoch 1: : 601it [43:45,  4.37s/it, v_num=, reduced_train_loss=0.772, global_step=1219.0, consumed_samples=78080.0, train_step_timing in s=3.400, batch_time=4.310, samples/sec=14.90]train loss calculated 0.7721635103225708\n",
      "Epoch 1: : 602it [43:50,  4.37s/it, v_num=, reduced_train_loss=0.895, global_step=1220.0, consumed_samples=78144.0, train_step_timing in s=3.350, batch_time=4.380, samples/sec=14.60]train loss calculated 0.8949079513549805\n",
      "Epoch 1: : 603it [43:54,  4.37s/it, v_num=, reduced_train_loss=0.785, global_step=1221.0, consumed_samples=78208.0, train_step_timing in s=3.350, batch_time=4.310, samples/sec=14.90]train loss calculated 0.7854101657867432\n",
      "Epoch 1: : 604it [43:59,  4.37s/it, v_num=, reduced_train_loss=0.796, global_step=1222.0, consumed_samples=78272.0, train_step_timing in s=3.510, batch_time=4.350, samples/sec=14.70]train loss calculated 0.7963971495628357\n",
      "Epoch 1: : 605it [44:03,  4.37s/it, v_num=, reduced_train_loss=0.815, global_step=1223.0, consumed_samples=78336.0, train_step_timing in s=3.440, batch_time=4.480, samples/sec=14.30]train loss calculated 0.8148651719093323\n",
      "Epoch 1: : 606it [44:07,  4.37s/it, v_num=, reduced_train_loss=0.836, global_step=1224.0, consumed_samples=78400.0, train_step_timing in s=3.350, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8359217643737793\n",
      "Epoch 1: : 607it [44:12,  4.37s/it, v_num=, reduced_train_loss=0.824, global_step=1225.0, consumed_samples=78464.0, train_step_timing in s=3.470, batch_time=4.280, samples/sec=15.00]train loss calculated 0.8237515687942505\n",
      "Epoch 1: : 608it [44:16,  4.37s/it, v_num=, reduced_train_loss=0.845, global_step=1226.0, consumed_samples=78528.0, train_step_timing in s=3.430, batch_time=4.460, samples/sec=14.30]train loss calculated 0.8449801206588745\n",
      "Epoch 1: : 609it [44:20,  4.37s/it, v_num=, reduced_train_loss=0.769, global_step=1227.0, consumed_samples=78592.0, train_step_timing in s=3.350, batch_time=4.310, samples/sec=14.80]train loss calculated 0.7692952156066895\n",
      "Epoch 1: : 610it [44:25,  4.37s/it, v_num=, reduced_train_loss=0.939, global_step=1228.0, consumed_samples=78656.0, train_step_timing in s=3.320, batch_time=4.310, samples/sec=14.90]train loss calculated 0.938981831073761\n",
      "Epoch 1: : 611it [44:29,  4.37s/it, v_num=, reduced_train_loss=0.949, global_step=1229.0, consumed_samples=78720.0, train_step_timing in s=3.390, batch_time=4.330, samples/sec=14.80]train loss calculated 0.9491108655929565\n",
      "Epoch 1: : 612it [44:33,  4.37s/it, v_num=, reduced_train_loss=0.987, global_step=1230.0, consumed_samples=78784.0, train_step_timing in s=3.410, batch_time=4.350, samples/sec=14.70]train loss calculated 0.9869706630706787\n",
      "Epoch 1: : 613it [44:38,  4.37s/it, v_num=, reduced_train_loss=1.010, global_step=1231.0, consumed_samples=78848.0, train_step_timing in s=3.310, batch_time=4.420, samples/sec=14.50]train loss calculated 1.01332688331604\n",
      "Epoch 1: : 614it [44:42,  4.37s/it, v_num=, reduced_train_loss=0.837, global_step=1232.0, consumed_samples=78912.0, train_step_timing in s=3.630, batch_time=4.250, samples/sec=15.10]train loss calculated 0.8369039297103882\n",
      "Epoch 1: : 615it [44:47,  4.37s/it, v_num=, reduced_train_loss=0.971, global_step=1233.0, consumed_samples=7.9e+4, train_step_timing in s=3.650, batch_time=4.690, samples/sec=13.60] train loss calculated 0.9711479544639587\n",
      "Epoch 1: : 616it [44:51,  4.37s/it, v_num=, reduced_train_loss=0.773, global_step=1234.0, consumed_samples=7.9e+4, train_step_timing in s=3.410, batch_time=4.610, samples/sec=13.90]train loss calculated 0.773456335067749\n",
      "Epoch 1: : 617it [44:56,  4.37s/it, v_num=, reduced_train_loss=0.905, global_step=1235.0, consumed_samples=79104.0, train_step_timing in s=3.330, batch_time=4.400, samples/sec=14.50]train loss calculated 0.9051934480667114\n",
      "Epoch 1: : 618it [45:00,  4.37s/it, v_num=, reduced_train_loss=0.999, global_step=1236.0, consumed_samples=79168.0, train_step_timing in s=3.440, batch_time=4.320, samples/sec=14.80]train loss calculated 0.998913586139679\n",
      "Epoch 1: : 619it [45:04,  4.37s/it, v_num=, reduced_train_loss=0.826, global_step=1237.0, consumed_samples=79232.0, train_step_timing in s=3.350, batch_time=4.310, samples/sec=14.80]train loss calculated 0.8261573314666748\n",
      "Epoch 1: : 619it [45:05,  4.37s/it, v_num=, reduced_train_loss=0.826, global_step=1237.0, consumed_samples=79232.0, train_step_timing in s=3.350, batch_time=4.340, samples/sec=14.80]\n",
      "\t[2025-07-11 05:07:46,574] [INFO] [SageMaker] Training steps complete. Starting post training activities, may take a while!\t[2025-07-11 05:16:31,516] [INFO] [SageMaker] Training is complete!\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "source .env\n",
    "../cli_utility/01_manager/hyperpod_job_manager.sh \\\n",
    "    --job_name $JOB_NAME \\\n",
    "    --action monitor \\\n",
    "    --cluster-name $CLUSTER_NAME \\\n",
    "    --cluster-id $CLUSTER_ID \\\n",
    "    --rig-name $RIG_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a6729",
   "metadata": {},
   "source": [
    "## ^^ This job execution can take upto 20-30 mins based on dataset used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74232782",
   "metadata": {},
   "source": [
    "### Once the Job is finished we see a Manifest.json which contains path to trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7641c435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"checkpoint_s3_bucket\": \"s3://customer-escrow-905418197933-hp-66af7af3/demo-sft-hp-nova-micro-run-alnr5/outputs/checkpoints\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 cp \"${OUTPUT_PATH}${JOB_NAME}/manifest.json\" - | jq .\n",
    "ESCROW_BUCKET=$(aws s3 cp \"${OUTPUT_PATH}${JOB_NAME}/manifest.json\" - | jq '.checkpoint_s3_bucket')\n",
    " \n",
    " cat > escrow.env << EOF\n",
    "export ESCROW_BUCKET=$ESCROW_BUCKET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d8c0a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-905418197933/HP-SFT-RUNS/demo-sft-hp-nova-micro-run-alnr5/0/tensorboard/events.out.tfevents.1752679490.demo-sft-hp-nova-micro-run-alnr5-master-0.546.0 to tensorboard_logs/events.out.tfevents.1752679490.demo-sft-hp-nova-micro-run-alnr5-master-0.546.0\n",
      "TensorBoard started at http://localhost:6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "E0716 09:07:10.045220 8588691520 program.py:300] TensorBoard could not bind to port 6006, it was already in use\n",
      "ERROR: TensorBoard could not bind to port 6006, it was already in use\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Copy the entire tensorboard directory\n",
    "aws s3 cp \"${OUTPUT_PATH}${JOB_NAME}/0/tensorboard/\" ./tensorboard_logs/ --recursive\n",
    "\n",
    "# Start TensorBoard (this will run in background)\n",
    "tensorboard --logdir=./tensorboard_logs --port=6006 &\n",
    "\n",
    "echo \"TensorBoard started at http://localhost:6006\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49871a",
   "metadata": {},
   "source": [
    "![imgs/tb_board.png](imgs/tb_board.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbeb204",
   "metadata": {},
   "source": [
    "### Step 3: Model Evaluation\n",
    "\n",
    "Now we can run evaluation on the model similarly just like training job but change the neccessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa4d3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_RECIPE_PATH = \"evaluation/nova/nova_micro_p5_48xl_general_text_benchmark_eval\"\n",
    "INSTANCE = \"p5.48xlarge\"\n",
    "EVAL_CONTAINER = image_uri_map['eval']\n",
    "EVAL_RUN_NAME = \"my-eval-run\"\n",
    "EVAL_OUTPUT_PATH = 's3://sagemaker-us-east-1-905418197933/HP-Eavl-runs/'\n",
    "\n",
    "import os\n",
    "os.environ['EVAL_RECIPE_PATH'] = EVAL_RECIPE_PATH\n",
    "os.environ['EVAL_RUN_NAME'] = EVAL_RUN_NAME\n",
    "os.environ['INSTANCE'] = INSTANCE\n",
    "os.environ['EVAL_CONTAINER'] = EVAL_CONTAINER\n",
    "os.environ['EVAL_OUTPUT_PATH'] = EVAL_OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a9657",
   "metadata": {},
   "source": [
    "#### Job Runner for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c49d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source escrow.env\n",
    "cat << EOF > evaluator.sh\n",
    "hyperpod start-job --namespace ${NAMESPACE} --recipe ${EVAL_RECIPE_PATH} --override-parameters \\\\\n",
    "     '{\"instance_type\": \"${INSTANCE}\",\n",
    "       \"container\": \"${EVAL_CONTAINER}\", \n",
    "       \"recipes.run.name\": \"${EVAL_RUN_NAME}\",\n",
    "       \"recipes.run.output_s3_path\": \"${EVAL_OUTPUT_PATH}\",\n",
    "       \"recipes.run.model_name_or_path\": \"${ESCROW_BUCKET}\"}'\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a96a26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final command: python3 /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/NovaRecipeConfigs/src/hyperpod_cli/sagemaker_hyperpod_recipes/main.py recipes=evaluation/nova/nova_micro_p5_48xl_general_text_benchmark_eval cluster_type=k8s cluster=k8s base_results_dir=/Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/results cluster.pullPolicy=\"IfNotPresent\" cluster.restartPolicy=\"OnFailure\" cluster.namespace=\"kubeflow\" instance_type=\"p5.48xlarge\" container=\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-HP-Eval-latest\" recipes.run.name=\"my-eval-run\" recipes.run.output_s3_path=\"s3://sagemaker-us-east-1-905418197933/HP-Eavl-runs/\" recipes.run.model_name_or_path=\"s3://customer-escrow-905418197933-hp-66af7af3/demo-sft-hp-nova-micro-run-alnr5/outputs/checkpoints\"\n",
      "Prepared output directory at /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/results/my-eval-run-26f4a/k8s_templates\n",
      "Found credentials in shared credentials file: ~/.aws/credentials\n",
      "Helm script created at /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/results/my-eval-run-26f4a/my-eval-run-26f4a_launch.sh\n",
      "Running Helm script: /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/getting_started/results/my-eval-run-26f4a/my-eval-run-26f4a_launch.sh\n",
      "NAME: my-eval-run-26f4a\n",
      "LAST DEPLOYED: Wed Jul 16 09:40:23 2025\n",
      "NAMESPACE: kubeflow\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "Launcher successfully generated: /Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/NovaRecipeConfigs/src/hyperpod_cli/sagemaker_hyperpod_recipes/launcher/nova/k8s_templates/SFT\n",
      "\n",
      "{\n",
      " \"Console URL\": \"https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/cluster-management/hp-cluster\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!bash evaluator.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb9405c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest folder found: ./results/my-eval-run-26f4a/\n",
      "Successfully loaded: ./results/my-eval-run-26f4a/k8s_templates/config/my-eval-run-26f4a_hydra.yaml, converted to JSON and stored as a job_manifest variable\n"
     ]
    }
   ],
   "source": [
    "job_manifest = latest_job_manifest()\n",
    "\n",
    "unique_job_name = job_manifest['run']['name']\n",
    "os.environ['JOB_NAME'] = unique_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85e89647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-eval-run-26f4a\n"
     ]
    }
   ],
   "source": [
    "!echo $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56de0a",
   "metadata": {},
   "source": [
    "##### Monitoring results post-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e89860b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\u001b[0;34m   Job Status\u001b[0m\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\n",
      "{\n",
      "    \"jobs\": [\n",
      "        {\n",
      "            \"Name\": \"demo-sft-hp-nova-micro-run-alnr5\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-16T15:07:50Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-jdebk\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-10T22:52:47Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-vvee7\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-10T19:25:28Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"dewanup-nova-micro-lora-sft-fixed-wsd4j\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T15:47:22Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"eval-g0zj5\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-11T21:28:21Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"my-eval-run-26f4a\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-16T16:40:23Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"my-test-job-djr92\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-11T03:22:12Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"sft-hp-nova-micro-run-beqzh\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T22:12:14Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"test-dewanup-nova-micro-lora-sft-fixed-560ko\",\n",
      "            \"Namespace\": \"kubeflow\",\n",
      "            \"CreationTime\": \"2025-07-14T21:51:02Z\",\n",
      "            \"State\": \"Succeeded\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "\u001b[0;36m▶ Pod Status: my-eval-run-26f4a\u001b[0m\n",
      "\n",
      "{\n",
      " \"pods\": [\n",
      "  {\n",
      "   \"PodName\": \"my-eval-run-26f4a-master-0\",\n",
      "   \"Namespace\": \"kubeflow\",\n",
      "   \"Status\": \"Succeeded\",\n",
      "   \"CreationTime\": \"2025-07-16 16:40:23+00:00\"\n",
      "  }\n",
      " ]\n",
      "}\n",
      "\n",
      "\u001b[0;36m▶ Pod Details\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[0;36m▶ Pod: my-eval-run-26f4a-master-0\u001b[0m\n",
      "\n",
      "Name:             my-eval-run-26f4a-master-0\n",
      "Namespace:        kubeflow\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             hyperpod-i-0f98b43f2eb4bf47a/172.31.9.136\n",
      "Start Time:       Wed, 16 Jul 2025 09:40:26 -0700\n",
      "Labels:           training.kubeflow.org/job-name=my-eval-run-26f4a\n",
      "                  training.kubeflow.org/job-role=master\n",
      "                  training.kubeflow.org/operator-name=pytorchjob-controller\n",
      "                  training.kubeflow.org/replica-index=0\n",
      "                  training.kubeflow.org/replica-type=master\n",
      "Annotations:      <none>\n",
      "Status:           Succeeded\n",
      "IP:               172.31.33.84\n",
      "IPs:\n",
      "  IP:           172.31.33.84\n",
      "Controlled By:  PyTorchJob/my-eval-run-26f4a\n",
      "Init Containers:\n",
      "  init-container:\n",
      "    Container ID:   containerd://02186d3939a68f40a188c17bdff23e9b37c9494a1108a8312589be113c00c031\n",
      "    Image:          708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\n",
      "    Image ID:       708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo@sha256:c126d5d1632ba2ade3daf30624798339ee43159b1d840a20b5a0d8a23c267676\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Wed, 16 Jul 2025 09:40:27 -0700\n",
      "      Finished:     Wed, 16 Jul 2025 09:40:29 -0700\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /config from training-config (rw)\n",
      "      /sagemaker/fsx/init from fsx-path-init (rw)\n",
      "      /var/log/aws/clusters from logs (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-plt95 (ro)\n",
      "Containers:\n",
      "  pytorch:\n",
      "    Container ID:   containerd://cd9e4227be4c3224a0356f6db230fe5a0c525c9d39190b8dc94dbbc0c3583448\n",
      "    Image:          708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-HP-Eval-latest\n",
      "    Image ID:       708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo@sha256:e4655642df038f118bb9bb756cc33d678ea428313a4fc4ac3b65fa2d17966170\n",
      "    Port:           23456/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Terminated\n",
      "      Reason:       Completed\n",
      "      Exit Code:    0\n",
      "      Started:      Wed, 16 Jul 2025 09:40:31 -0700\n",
      "      Finished:     Wed, 16 Jul 2025 09:47:11 -0700\n",
      "    Ready:          False\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      nvidia.com/gpu:  8\n",
      "    Requests:\n",
      "      nvidia.com/gpu:  8\n",
      "    Environment:\n",
      "      X_AMZ_SOURCE_ACCOUNT:  905418197933\n",
      "      PYTHONUNBUFFERED:      1\n",
      "      MASTER_PORT:           23456\n",
      "      PET_MASTER_PORT:       23456\n",
      "      MASTER_ADDR:           my-eval-run-26f4a-master-0\n",
      "      PET_MASTER_ADDR:       my-eval-run-26f4a-master-0\n",
      "      WORLD_SIZE:            1\n",
      "      RANK:                  0\n",
      "      PET_NPROC_PER_NODE:    auto\n",
      "      PET_NODE_RANK:         0\n",
      "      PET_NNODES:            1\n",
      "    Mounts:\n",
      "      /config from training-config (rw)\n",
      "      /dev/shm from dshm (rw)\n",
      "      /sagemaker/fsx/main from fsx-path-main (rw)\n",
      "      /sagemaker/fsx/output_artifacts from fsx-path-output-artifacts (rw)\n",
      "      /tmp from tmp-storage (rw)\n",
      "      /var/log/aws/clusters from logs (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-plt95 (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   False \n",
      "  Initialized                 True \n",
      "  Ready                       False \n",
      "  ContainersReady             False \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  fsx-path-main:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/main\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  fsx-path-init:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/init\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  training-config:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      training-config-my-eval-run-26f4a\n",
      "    Optional:  false\n",
      "  dshm:\n",
      "    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n",
      "    Medium:     Memory\n",
      "    SizeLimit:  <unset>\n",
      "  logs:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /var/log/aws/clusters\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  fsx-path-output-artifacts:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /sagemaker/fsx/output_artifacts\n",
      "    HostPathType:  DirectoryOrCreate\n",
      "  tmp-storage:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /opt/dlami/nvme\n",
      "    HostPathType:  Directory\n",
      "  kube-api-access-plt95:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "                             nvidia.com/gpu:NoSchedule op=Exists\n",
      "                             sagemaker.amazonaws.com/RestrictedNode=Worker:NoSchedule\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  41m   default-scheduler  Successfully assigned kubeflow/my-eval-run-26f4a-master-0 to hyperpod-i-0f98b43f2eb4bf47a\n",
      "  Normal  Pulling    41m   kubelet            Pulling image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\"\n",
      "  Normal  Pulled     41m   kubelet            Successfully pulled image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/init-container-repo:latest\" in 148ms (148ms including waiting). Image size: 246304701 bytes.\n",
      "  Normal  Created    41m   kubelet            Created container init-container\n",
      "  Normal  Started    41m   kubelet            Started container init-container\n",
      "  Normal  Pulling    41m   kubelet            Pulling image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-HP-Eval-latest\"\n",
      "  Normal  Pulled     41m   kubelet            Successfully pulled image \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-HP-Eval-latest\" in 333ms (333ms including waiting). Image size: 12110890214 bytes.\n",
      "  Normal  Created    41m   kubelet            Created container pytorch\n",
      "  Normal  Started    41m   kubelet            Started container pytorch\n",
      "\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\u001b[0;34m   CloudWatch Logs\u001b[0m\n",
      "\u001b[0;34m════════════════════════════════════════════\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[0;36m▶ Log Configuration\u001b[0m\n",
      "\n",
      "Log Group: /aws/sagemaker/Clusters/hp-cluster/m9bbgk2h4tx7\n",
      "Stream prefix: SagemakerHyperPodTrainingJob/restricted-instance-group3\n",
      "\n",
      "\u001b[0;36m▶ Log Streams\u001b[0m\n",
      "\n",
      "\u001b[1;33m⚠ No streams found with prefix, trying broader search...\u001b[0m\n",
      "\u001b[0;32m✓ Found log streams:\u001b[0m\n",
      "  - SagemakerHyperPodTrainingJob/restricted-instance-group3/i-0f98b43f2eb4bf47a\n",
      "  - SagemakerHyperPodTrainingJob/restricted-instance-group3/i-0110f799aed1ff4f3\n",
      "  - \n",
      "\n",
      "\u001b[0;36m▶ Recent Logs (Last 100 lines per stream)\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[0;36m▶ Log Stream: SagemakerHyperPodTrainingJob/restricted-instance-group3/i-0f98b43f2eb4bf47a\u001b[0m\n",
      "\n",
      "Fetching recent logs...\n",
      "Creating parquet from Arrow format: 100%|##########| 1/1 [00:00<00:00, 628.64ba/s]\t[2025-07-11 21:36:27,126] Finished evaluation, Closing the inference server\t[2025-07-11 21:36:27,146] ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmprwerp4vb'> (in /usr/local/lib/python3.12/tempfile.py:936)n': {'name': 'eval-g0zj5', 'model_type': 'amazon.nova-micro-v1:0:128k', 'model_name_or_path': 's3://customer-escrow-905418197933-hp-66af7af3/dewanup-nova-micro-lora-sft-fixed-jdebk/outputs/checkpoints', 'replicas': 1, 'data_s3_path': '', 'tensorboard_s3_path': '', 'output_s3_path': 's3://905418197933-realm-data/HF/ace_reason_math/1750697426/output-artifacts/eval-results/'}, 'evaluation': {'task': 'mmlu', 'strategy': 'zs_cot', 'metric': 'accuracy', 'subtask': 'anatomy'}, 'inference': {'max_new_tokens': 8196, 'top_k': -1, 'top_p': 1.0, 'temperature': 0}}\t[2025-07-11 21:32:08,537] Run name: eval-g0zj5\t[2025-07-11 21:32:08,570] Found the model, Loading model weights\t[2025-07-11 21:32:08,570] Starting Inferencing server\t[2025-07-11 21:32:08,585] Inferencing server still starting...\t[2025-07-11 21:32:08,796] INFO 07-11 21:32:08 [importing.py:53] Triton module has been replaced with a placeholder.\t[2025-07-11 21:32:08,841] INFO 07-11 21:32:08 [__init__.py:239] Automatically detected platform cuda.\t[2025-07-11 21:32:38,587] Inferencing server still starting...\t[2025-07-11 21:33:08,589] Inferencing server still starting...\t[2025-07-11 21:33:38,590] Inferencing server still starting...\t[2025-07-11 21:34:08,592] Inferencing server still starting...\t[2025-07-11 21:34:38,594] Inferencing server still starting...\t[2025-07-11 21:35:10,595] Inferencing server still starting...\t[2025-07-11 21:36:14,598] Inferencing server has started.\t[2025-07-11 21:36:14,598] Starting evaluation with task mmlu\t[2025-07-11 21:36:17,720] INFO 07-11 21:36:17 [importing.py:53] Triton module has been replaced with a placeholder.\t[2025-07-11 21:36:17,870] INFO 07-11 21:36:17 [__init__.py:239] Automatically detected platform cuda.\t[2025-07-11 21:36:21,270] \n",
      "\n",
      "\u001b[0;36m▶ Log Stream: SagemakerHyperPodTrainingJob/restricted-instance-group3/i-0110f799aed1ff4f3\u001b[0m\n",
      "\n",
      "Fetching recent logs...\n",
      "Epoch 1: : 523it [38:06,  4.37s/it, v_num=, reduced_train_loss=0.874, global_step=1141.0, consumed_samples=73088.0, train_step_timing in s=3.340, batch_time=4.440, samples/sec=14.40]train loss calculated 0.8742266893386841\n",
      "Epoch 1: : 524it [38:10,  4.37s/it, v_num=, reduced_train_loss=0.888, global_step=1142.0, consumed_samples=73152.0, train_step_timing in s=3.320, batch_time=4.340, samples/sec=14.70]train loss calculated 0.8881533145904541\n",
      "Epoch 1: : 525it [38:15,  4.37s/it, v_num=, reduced_train_loss=0.857, global_step=1143.0, consumed_samples=73216.0, train_step_timing in s=3.380, batch_time=4.280, samples/sec=14.90]train loss calculated 0.8571255207061768\n",
      "Epoch 1: : 526it [38:19,  4.37s/it, v_num=, reduced_train_loss=0.961, global_step=1144.0, consumed_samples=73280.0, train_step_timing in s=3.290, batch_time=4.310, samples/sec=14.90]train loss calculated 0.9605405330657959\n",
      "Epoch 1: : 527it [38:23,  4.37s/it, v_num=, reduced_train_loss=0.827, global_step=1145.0, consumed_samples=73344.0, train_step_timing in s=3.210, batch_time=4.280, samples/sec=15.00]train loss calculated 0.8268367052078247\n",
      "Epoch 1: : 528it [38:27,  4.37s/it, v_num=, reduced_train_loss=0.834, global_step=1146.0, consumed_samples=73408.0, train_step_timing in s=3.310, batch_time=4.230, samples/sec=15.10]train loss calculated 0.8337791562080383\n",
      "Epoch 1: : 529it [38:32,  4.37s/it, v_num=, reduced_train_loss=0.788, global_step=1147.0, consumed_samples=73472.0, train_step_timing in s=3.340, batch_time=4.340, samples/sec=14.80]train loss calculated 0.78849196434021\n",
      "Epoch 1: : 530it [38:36,  4.37s/it, v_num=, reduced_train_loss=0.849, global_step=1148.0, consumed_samples=73536.0, train_step_timing in s=2.990, batch_time=4.150, samples/sec=15.40]train loss calculated 0.8493067026138306\n",
      "Epoch 1: : 531it [38:40,  4.37s/it, v_num=, reduced_train_loss=0.865, global_step=1149.0, consumed_samples=73600.0, train_step_timing in s=3.480, batch_time=4.020, samples/sec=15.90]train loss calculated 0.8653340339660645\n",
      "Epoch 1: : 532it [38:45,  4.37s/it, v_num=, reduced_train_loss=0.841, global_step=1150.0, consumed_samples=73664.0, train_step_timing in s=3.430, batch_time=4.500, samples/sec=14.20]train loss calculated 0.8407489061355591\n",
      "Epoch 1: : 533it [38:49,  4.37s/it, v_num=, reduced_train_loss=0.950, global_step=1151.0, consumed_samples=73728.0, train_step_timing in s=3.390, batch_time=4.370, samples/sec=14.60]train loss calculated 0.9502102136611938\n",
      "Epoch 1: : 534it [38:53,  4.37s/it, v_num=, reduced_train_loss=0.858, global_step=1152.0, consumed_samples=73792.0, train_step_timing in s=3.290, batch_time=4.440, samples/sec=14.40]train loss calculated 0.8581693172454834\n",
      "Epoch 1: : 535it [38:58,  4.37s/it, v_num=, reduced_train_loss=0.890, global_step=1153.0, consumed_samples=73856.0, train_step_timing in s=3.390, batch_time=4.340, samples/sec=14.70]train loss calculated 0.8895231485366821\n",
      "Epoch 1: : 536it [39:02,  4.37s/it, v_num=, reduced_train_loss=0.914, global_step=1154.0, consumed_samples=73920.0, train_step_timing in s=3.390, batch_time=4.370, samples/sec=14.60]train loss calculated 0.9141672849655151\n",
      "Epoch 1: : 537it [39:06,  4.37s/it, v_num=, reduced_train_loss=0.914, global_step=1155.0, consumed_samples=7.4e+4, train_step_timing in s=3.270, batch_time=4.360, samples/sec=14.70] train loss calculated 0.91437828540802\n",
      "Epoch 1: : 538it [39:11,  4.37s/it, v_num=, reduced_train_loss=0.877, global_step=1156.0, consumed_samples=7.4e+4, train_step_timing in s=3.410, batch_time=4.260, samples/sec=15.00]train loss calculated 0.8772664070129395\n",
      "Epoch 1: : 539it [39:15,  4.37s/it, v_num=, reduced_train_loss=0.908, global_step=1157.0, consumed_samples=74112.0, train_step_timing in s=3.400, batch_time=4.340, samples/sec=14.70]train loss calculated 0.9077670574188232\n",
      "Epoch 1: : 540it [39:19,  4.37s/it, v_num=, reduced_train_loss=0.875, global_step=1158.0, consumed_samples=74176.0, train_step_timing in s=3.380, batch_time=4.370, samples/sec=14.60]train loss calculated 0.8754771947860718\n",
      "Epoch 1: : 541it [39:24,  4.37s/it, v_num=, reduced_train_loss=0.818, global_step=1159.0, consumed_samples=74240.0, train_step_timing in s=3.380, batch_time=4.370, samples/sec=14.60]train loss calculated 0.8175592422485352\n",
      "Epoch 1: : 542it [39:28,  4.37s/it, v_num=, reduced_train_loss=0.927, global_step=1160.0, consumed_samples=74304.0, train_step_timing in s=3.350, batch_time=4.290, samples/sec=14.90]train loss calculated 0.9265929460525513\n",
      "Epoch 1: : 543it [39:32,  4.37s/it, v_num=, reduced_train_loss=0.827, global_step=1161.0, consumed_samples=74368.0, train_step_timing in s=3.290, batch_time=4.330, samples/sec=14.80]train loss calculated 0.8274103403091431\n",
      "Epoch 1: : 544it [39:37,  4.37s/it, v_num=, reduced_train_loss=0.941, global_step=1162.0, consumed_samples=74432.0, train_step_timing in s=3.290, batch_time=4.350, samples/sec=14.70]train loss calculated 0.9414460062980652\n",
      "Epoch 1: : 545it [39:41,  4.37s/it, v_num=, reduced_train_loss=0.807, global_step=1163.0, consumed_samples=74496.0, train_step_timing in s=3.400, batch_time=4.300, samples/sec=14.90]train loss calculated 0.8070048093795776\n",
      "Epoch 1: : 546it [39:45,  4.37s/it, v_num=, reduced_train_loss=0.820, global_step=1164.0, consumed_samples=74560.0, train_step_timing in s=3.310, batch_time=4.450, samples/sec=14.40]train loss calculated 0.820473313331604\n",
      "Epoch 1: : 547it [39:50,  4.37s/it, v_num=, reduced_train_loss=0.950, global_step=1165.0, consumed_samples=74624.0, train_step_timing in s=3.340, batch_time=4.310, samples/sec=14.80]train loss calculated 0.9501339197158813\n",
      "Epoch 1: : 548it [39:54,  4.37s/it, v_num=, reduced_train_loss=0.937, global_step=1166.0, consumed_samples=74688.0, train_step_timing in s=3.360, batch_time=4.320, samples/sec=14.80]train loss calculated 0.9365205764770508\n",
      "Epoch 1: : 549it [39:58,  4.37s/it, v_num=, reduced_train_loss=0.777, global_step=1167.0, consumed_samples=74752.0, train_step_timing in s=3.320, batch_time=4.390, samples/sec=14.60]train loss calculated 0.7774803638458252\n",
      "Epoch 1: : 550it [40:03,  4.37s/it, v_num=, reduced_train_loss=0.785, global_step=1168.0, consumed_samples=74816.0, train_step_timing in s=3.300, batch_time=4.300, samples/sec=14.90]train loss calculated 0.7846362590789795\n",
      "Epoch 1: : 551it [40:07,  4.37s/it, v_num=, reduced_train_loss=0.864, global_step=1169.0, consumed_samples=74880.0, train_step_timing in s=3.260, batch_time=4.290, samples/sec=14.90]train loss calculated 0.864311933517456\n",
      "Epoch 1: : 552it [40:11,  4.37s/it, v_num=, reduced_train_loss=0.770, global_step=1170.0, consumed_samples=74944.0, train_step_timing in s=3.350, batch_time=4.270, samples/sec=15.00]train loss calculated 0.7704010009765625\n",
      "Epoch 1: : 553it [40:16,  4.37s/it, v_num=, reduced_train_loss=0.938, global_step=1171.0, consumed_samples=7.5e+4, train_step_timing in s=3.380, batch_time=4.320, samples/sec=14.80] train loss calculated 0.9375468492507935\n",
      "Epoch 1: : 554it [40:20,  4.37s/it, v_num=, reduced_train_loss=0.951, global_step=1172.0, consumed_samples=75072.0, train_step_timing in s=3.440, batch_time=4.320, samples/sec=14.80]train loss calculated 0.9509410262107849\n",
      "Epoch 1: : 555it [40:24,  4.37s/it, v_num=, reduced_train_loss=0.777, global_step=1173.0, consumed_samples=75136.0, train_step_timing in s=3.450, batch_time=4.390, samples/sec=14.60]train loss calculated 0.7774874567985535\n",
      "Epoch 1: : 556it [40:29,  4.37s/it, v_num=, reduced_train_loss=0.895, global_step=1174.0, consumed_samples=75200.0, train_step_timing in s=3.360, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8945023417472839\n",
      "Epoch 1: : 557it [40:33,  4.37s/it, v_num=, reduced_train_loss=0.850, global_step=1175.0, consumed_samples=75264.0, train_step_timing in s=3.280, batch_time=4.330, samples/sec=14.80]train loss calculated 0.8498396277427673\n",
      "Epoch 1: : 558it [40:37,  4.37s/it, v_num=, reduced_train_loss=0.798, global_step=1176.0, consumed_samples=75328.0, train_step_timing in s=3.250, batch_time=4.340, samples/sec=14.70]train loss calculated 0.7983971238136292\n",
      "Epoch 1: : 559it [40:42,  4.37s/it, v_num=, reduced_train_loss=0.802, global_step=1177.0, consumed_samples=75392.0, train_step_timing in s=3.410, batch_time=4.240, samples/sec=15.10]train loss calculated 0.8024945259094238\n",
      "Epoch 1: : 560it [40:46,  4.37s/it, v_num=, reduced_train_loss=0.838, global_step=1178.0, consumed_samples=75456.0, train_step_timing in s=3.390, batch_time=4.380, samples/sec=14.60]train loss calculated 0.8378608226776123\n",
      "Epoch 1: : 561it [40:50,  4.37s/it, v_num=, reduced_train_loss=0.921, global_step=1179.0, consumed_samples=75520.0, train_step_timing in s=3.270, batch_time=4.370, samples/sec=14.70]train loss calculated 0.9207227230072021\n",
      "Epoch 1: : 562it [40:55,  4.37s/it, v_num=, reduced_train_loss=0.963, global_step=1180.0, consumed_samples=75584.0, train_step_timing in s=3.420, batch_time=4.280, samples/sec=15.00]train loss calculated 0.9632612466812134\n",
      "Epoch 1: : 563it [40:59,  4.37s/it, v_num=, reduced_train_loss=0.864, global_step=1181.0, consumed_samples=75648.0, train_step_timing in s=3.370, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8644891977310181\n",
      "Epoch 1: : 564it [41:04,  4.37s/it, v_num=, reduced_train_loss=0.947, global_step=1182.0, consumed_samples=75712.0, train_step_timing in s=3.430, batch_time=4.330, samples/sec=14.80]train loss calculated 0.94734787940979\n",
      "Epoch 1: : 565it [41:08,  4.37s/it, v_num=, reduced_train_loss=0.861, global_step=1183.0, consumed_samples=75776.0, train_step_timing in s=3.650, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8614346981048584\n",
      "Epoch 1: : 566it [41:13,  4.37s/it, v_num=, reduced_train_loss=0.898, global_step=1184.0, consumed_samples=75840.0, train_step_timing in s=3.350, batch_time=4.610, samples/sec=13.90]train loss calculated 0.8982287049293518\n",
      "Epoch 1: : 567it [41:17,  4.37s/it, v_num=, reduced_train_loss=0.864, global_step=1185.0, consumed_samples=75904.0, train_step_timing in s=3.300, batch_time=4.400, samples/sec=14.50]train loss calculated 0.8639969229698181\n",
      "Epoch 1: : 568it [41:21,  4.37s/it, v_num=, reduced_train_loss=0.812, global_step=1186.0, consumed_samples=7.6e+4, train_step_timing in s=3.420, batch_time=4.310, samples/sec=14.80] train loss calculated 0.8119203448295593\n",
      "Epoch 1: : 569it [41:26,  4.37s/it, v_num=, reduced_train_loss=1.050, global_step=1187.0, consumed_samples=7.6e+4, train_step_timing in s=3.750, batch_time=4.440, samples/sec=14.40]train loss calculated 1.0535669326782227\n",
      "Epoch 1: : 570it [41:30,  4.37s/it, v_num=, reduced_train_loss=0.852, global_step=1188.0, consumed_samples=76096.0, train_step_timing in s=3.360, batch_time=4.670, samples/sec=13.70]train loss calculated 0.851875901222229\n",
      "Epoch 1: : 571it [41:35,  4.37s/it, v_num=, reduced_train_loss=0.897, global_step=1189.0, consumed_samples=76160.0, train_step_timing in s=3.330, batch_time=4.330, samples/sec=14.80]train loss calculated 0.8968481421470642\n",
      "Epoch 1: : 572it [41:39,  4.37s/it, v_num=, reduced_train_loss=0.918, global_step=1190.0, consumed_samples=76224.0, train_step_timing in s=3.360, batch_time=4.370, samples/sec=14.60]train loss calculated 0.9178856611251831\n",
      "Epoch 1: : 573it [41:43,  4.37s/it, v_num=, reduced_train_loss=0.846, global_step=1191.0, consumed_samples=76288.0, train_step_timing in s=3.270, batch_time=4.300, samples/sec=14.90]train loss calculated 0.8460735082626343\n",
      "Epoch 1: : 574it [41:48,  4.37s/it, v_num=, reduced_train_loss=0.860, global_step=1192.0, consumed_samples=76352.0, train_step_timing in s=3.340, batch_time=4.290, samples/sec=14.90]train loss calculated 0.8600265979766846\n",
      "Epoch 1: : 575it [41:52,  4.37s/it, v_num=, reduced_train_loss=1.000, global_step=1193.0, consumed_samples=76416.0, train_step_timing in s=3.350, batch_time=4.260, samples/sec=15.00]train loss calculated 1.004220962524414\n",
      "Epoch 1: : 576it [41:56,  4.37s/it, v_num=, reduced_train_loss=0.893, global_step=1194.0, consumed_samples=76480.0, train_step_timing in s=3.310, batch_time=4.350, samples/sec=14.70]train loss calculated 0.8926070332527161\n",
      "Epoch 1: : 577it [42:01,  4.37s/it, v_num=, reduced_train_loss=0.855, global_step=1195.0, consumed_samples=76544.0, train_step_timing in s=3.300, batch_time=4.300, samples/sec=14.90]train loss calculated 0.8547406196594238\n",
      "Epoch 1: : 578it [42:05,  4.37s/it, v_num=, reduced_train_loss=0.856, global_step=1196.0, consumed_samples=76608.0, train_step_timing in s=3.490, batch_time=4.380, samples/sec=14.60]train loss calculated 0.8564959764480591\n",
      "Epoch 1: : 579it [42:09,  4.37s/it, v_num=, reduced_train_loss=0.883, global_step=1197.0, consumed_samples=76672.0, train_step_timing in s=3.350, batch_time=4.510, samples/sec=14.20]train loss calculated 0.8826826810836792\n",
      "Epoch 1: : 580it [42:14,  4.37s/it, v_num=, reduced_train_loss=0.922, global_step=1198.0, consumed_samples=76736.0, train_step_timing in s=3.410, batch_time=4.270, samples/sec=15.00]train loss calculated 0.9217593669891357\n",
      "Epoch 1: : 581it [42:18,  4.37s/it, v_num=, reduced_train_loss=0.844, global_step=1199.0, consumed_samples=76800.0, train_step_timing in s=3.550, batch_time=4.340, samples/sec=14.80]train loss calculated 0.8439275026321411\n",
      "Epoch 1: : 582it [42:23,  4.37s/it, v_num=, reduced_train_loss=0.822, global_step=1200.0, consumed_samples=76864.0, train_step_timing in s=3.350, batch_time=4.550, samples/sec=14.10]train loss calculated 0.8222110271453857\n",
      "Epoch 1: : 583it [42:27,  4.37s/it, v_num=, reduced_train_loss=0.821, global_step=1201.0, consumed_samples=76928.0, train_step_timing in s=3.370, batch_time=4.320, samples/sec=14.80]train loss calculated 0.8209871649742126\n",
      "Epoch 1: : 584it [42:31,  4.37s/it, v_num=, reduced_train_loss=0.829, global_step=1202.0, consumed_samples=7.7e+4, train_step_timing in s=3.320, batch_time=4.410, samples/sec=14.50] train loss calculated 0.8286842107772827\n",
      "Epoch 1: : 585it [42:36,  4.37s/it, v_num=, reduced_train_loss=0.968, global_step=1203.0, consumed_samples=77056.0, train_step_timing in s=3.380, batch_time=4.330, samples/sec=14.80]train loss calculated 0.9683666229248047\n",
      "Epoch 1: : 586it [42:40,  4.37s/it, v_num=, reduced_train_loss=0.797, global_step=1204.0, consumed_samples=77120.0, train_step_timing in s=3.350, batch_time=4.350, samples/sec=14.70]train loss calculated 0.7974674701690674\n",
      "Epoch 1: : 587it [42:44,  4.37s/it, v_num=, reduced_train_loss=0.826, global_step=1205.0, consumed_samples=77184.0, train_step_timing in s=3.340, batch_time=4.340, samples/sec=14.80]train loss calculated 0.8257066011428833\n",
      "Epoch 1: : 588it [42:49,  4.37s/it, v_num=, reduced_train_loss=0.912, global_step=1206.0, consumed_samples=77248.0, train_step_timing in s=3.360, batch_time=4.410, samples/sec=14.50]train loss calculated 0.9121530652046204\n",
      "Epoch 1: : 589it [42:53,  4.37s/it, v_num=, reduced_train_loss=0.881, global_step=1207.0, consumed_samples=77312.0, train_step_timing in s=3.410, batch_time=4.350, samples/sec=14.70]train loss calculated 0.88130784034729\n",
      "Epoch 1: : 590it [42:58,  4.37s/it, v_num=, reduced_train_loss=0.830, global_step=1208.0, consumed_samples=77376.0, train_step_timing in s=3.360, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8299692869186401\n",
      "Epoch 1: : 591it [43:02,  4.37s/it, v_num=, reduced_train_loss=0.886, global_step=1209.0, consumed_samples=77440.0, train_step_timing in s=3.410, batch_time=4.380, samples/sec=14.60]train loss calculated 0.8862640857696533\n",
      "Epoch 1: : 592it [43:07,  4.37s/it, v_num=, reduced_train_loss=0.757, global_step=1210.0, consumed_samples=77504.0, train_step_timing in s=3.540, batch_time=4.420, samples/sec=14.50]train loss calculated 0.7567726969718933\n",
      "Epoch 1: : 593it [43:11,  4.37s/it, v_num=, reduced_train_loss=0.980, global_step=1211.0, consumed_samples=77568.0, train_step_timing in s=3.310, batch_time=4.520, samples/sec=14.20]train loss calculated 0.9804074764251709\n",
      "Epoch 1: : 594it [43:15,  4.37s/it, v_num=, reduced_train_loss=0.927, global_step=1212.0, consumed_samples=77632.0, train_step_timing in s=3.300, batch_time=4.330, samples/sec=14.80]train loss calculated 0.9272388815879822\n",
      "Epoch 1: : 595it [43:19,  4.37s/it, v_num=, reduced_train_loss=0.937, global_step=1213.0, consumed_samples=77696.0, train_step_timing in s=3.300, batch_time=4.270, samples/sec=15.00]train loss calculated 0.936908483505249\n",
      "Epoch 1: : 596it [43:24,  4.37s/it, v_num=, reduced_train_loss=0.827, global_step=1214.0, consumed_samples=77760.0, train_step_timing in s=3.320, batch_time=4.310, samples/sec=14.80]train loss calculated 0.8270081281661987\n",
      "Epoch 1: : 597it [43:28,  4.37s/it, v_num=, reduced_train_loss=0.973, global_step=1215.0, consumed_samples=77824.0, train_step_timing in s=3.430, batch_time=4.330, samples/sec=14.80]train loss calculated 0.9727742671966553\n",
      "Epoch 1: : 598it [43:33,  4.37s/it, v_num=, reduced_train_loss=0.808, global_step=1216.0, consumed_samples=77888.0, train_step_timing in s=3.330, batch_time=4.410, samples/sec=14.50]train loss calculated 0.8077811002731323\n",
      "Epoch 1: : 599it [43:37,  4.37s/it, v_num=, reduced_train_loss=0.902, global_step=1217.0, consumed_samples=7.8e+4, train_step_timing in s=3.350, batch_time=4.340, samples/sec=14.70] train loss calculated 0.901864767074585\n",
      "Epoch 1: : 600it [43:41,  4.37s/it, v_num=, reduced_train_loss=0.826, global_step=1218.0, consumed_samples=7.8e+4, train_step_timing in s=3.350, batch_time=4.290, samples/sec=14.90]train loss calculated 0.8260514140129089\n",
      "Epoch 1: : 601it [43:45,  4.37s/it, v_num=, reduced_train_loss=0.772, global_step=1219.0, consumed_samples=78080.0, train_step_timing in s=3.400, batch_time=4.310, samples/sec=14.90]train loss calculated 0.7721635103225708\n",
      "Epoch 1: : 602it [43:50,  4.37s/it, v_num=, reduced_train_loss=0.895, global_step=1220.0, consumed_samples=78144.0, train_step_timing in s=3.350, batch_time=4.380, samples/sec=14.60]train loss calculated 0.8949079513549805\n",
      "Epoch 1: : 603it [43:54,  4.37s/it, v_num=, reduced_train_loss=0.785, global_step=1221.0, consumed_samples=78208.0, train_step_timing in s=3.350, batch_time=4.310, samples/sec=14.90]train loss calculated 0.7854101657867432\n",
      "Epoch 1: : 604it [43:59,  4.37s/it, v_num=, reduced_train_loss=0.796, global_step=1222.0, consumed_samples=78272.0, train_step_timing in s=3.510, batch_time=4.350, samples/sec=14.70]train loss calculated 0.7963971495628357\n",
      "Epoch 1: : 605it [44:03,  4.37s/it, v_num=, reduced_train_loss=0.815, global_step=1223.0, consumed_samples=78336.0, train_step_timing in s=3.440, batch_time=4.480, samples/sec=14.30]train loss calculated 0.8148651719093323\n",
      "Epoch 1: : 606it [44:07,  4.37s/it, v_num=, reduced_train_loss=0.836, global_step=1224.0, consumed_samples=78400.0, train_step_timing in s=3.350, batch_time=4.420, samples/sec=14.50]train loss calculated 0.8359217643737793\n",
      "Epoch 1: : 607it [44:12,  4.37s/it, v_num=, reduced_train_loss=0.824, global_step=1225.0, consumed_samples=78464.0, train_step_timing in s=3.470, batch_time=4.280, samples/sec=15.00]train loss calculated 0.8237515687942505\n",
      "Epoch 1: : 608it [44:16,  4.37s/it, v_num=, reduced_train_loss=0.845, global_step=1226.0, consumed_samples=78528.0, train_step_timing in s=3.430, batch_time=4.460, samples/sec=14.30]train loss calculated 0.8449801206588745\n",
      "Epoch 1: : 609it [44:20,  4.37s/it, v_num=, reduced_train_loss=0.769, global_step=1227.0, consumed_samples=78592.0, train_step_timing in s=3.350, batch_time=4.310, samples/sec=14.80]train loss calculated 0.7692952156066895\n",
      "Epoch 1: : 610it [44:25,  4.37s/it, v_num=, reduced_train_loss=0.939, global_step=1228.0, consumed_samples=78656.0, train_step_timing in s=3.320, batch_time=4.310, samples/sec=14.90]train loss calculated 0.938981831073761\n",
      "Epoch 1: : 611it [44:29,  4.37s/it, v_num=, reduced_train_loss=0.949, global_step=1229.0, consumed_samples=78720.0, train_step_timing in s=3.390, batch_time=4.330, samples/sec=14.80]train loss calculated 0.9491108655929565\n",
      "Epoch 1: : 612it [44:33,  4.37s/it, v_num=, reduced_train_loss=0.987, global_step=1230.0, consumed_samples=78784.0, train_step_timing in s=3.410, batch_time=4.350, samples/sec=14.70]train loss calculated 0.9869706630706787\n",
      "Epoch 1: : 613it [44:38,  4.37s/it, v_num=, reduced_train_loss=1.010, global_step=1231.0, consumed_samples=78848.0, train_step_timing in s=3.310, batch_time=4.420, samples/sec=14.50]train loss calculated 1.01332688331604\n",
      "Epoch 1: : 614it [44:42,  4.37s/it, v_num=, reduced_train_loss=0.837, global_step=1232.0, consumed_samples=78912.0, train_step_timing in s=3.630, batch_time=4.250, samples/sec=15.10]train loss calculated 0.8369039297103882\n",
      "Epoch 1: : 615it [44:47,  4.37s/it, v_num=, reduced_train_loss=0.971, global_step=1233.0, consumed_samples=7.9e+4, train_step_timing in s=3.650, batch_time=4.690, samples/sec=13.60] train loss calculated 0.9711479544639587\n",
      "Epoch 1: : 616it [44:51,  4.37s/it, v_num=, reduced_train_loss=0.773, global_step=1234.0, consumed_samples=7.9e+4, train_step_timing in s=3.410, batch_time=4.610, samples/sec=13.90]train loss calculated 0.773456335067749\n",
      "Epoch 1: : 617it [44:56,  4.37s/it, v_num=, reduced_train_loss=0.905, global_step=1235.0, consumed_samples=79104.0, train_step_timing in s=3.330, batch_time=4.400, samples/sec=14.50]train loss calculated 0.9051934480667114\n",
      "Epoch 1: : 618it [45:00,  4.37s/it, v_num=, reduced_train_loss=0.999, global_step=1236.0, consumed_samples=79168.0, train_step_timing in s=3.440, batch_time=4.320, samples/sec=14.80]train loss calculated 0.998913586139679\n",
      "Epoch 1: : 619it [45:04,  4.37s/it, v_num=, reduced_train_loss=0.826, global_step=1237.0, consumed_samples=79232.0, train_step_timing in s=3.350, batch_time=4.310, samples/sec=14.80]train loss calculated 0.8261573314666748\n",
      "Epoch 1: : 619it [45:05,  4.37s/it, v_num=, reduced_train_loss=0.826, global_step=1237.0, consumed_samples=79232.0, train_step_timing in s=3.350, batch_time=4.340, samples/sec=14.80]\n",
      "\t[2025-07-11 05:07:46,574] [INFO] [SageMaker] Training steps complete. Starting post training activities, may take a while!\t[2025-07-11 05:16:31,516] [INFO] [SageMaker] Training is complete!\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "source .env\n",
    "../cli_utility/01_manager/hyperpod_job_manager.sh \\\n",
    "    --job_name $JOB_NAME \\\n",
    "    --action monitor \\\n",
    "    --cluster-name $CLUSTER_NAME \\\n",
    "    --cluster-id $CLUSTER_ID \\\n",
    "    --rig-name $RIG_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1767a53b",
   "metadata": {},
   "source": [
    "### Step 4: Inference\n",
    "\n",
    "\n",
    "Now, once we have evaluation done we can host this on Bedrock using bedrock Model Inferecne "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
