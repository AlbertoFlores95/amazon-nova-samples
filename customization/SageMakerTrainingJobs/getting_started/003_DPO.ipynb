{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {},
   "source": [
    "# Direct Preference Alignment (DPO) of Amazon Nova using Amazon SageMaker Training Job\n",
    "\n",
    "You can customize Amazon Nova models through base recipes using Amazon SageMaker training jobs. These recipes support Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), with both Full-Rank and Low-Rank Adaptation (LoRA) options.\n",
    "\n",
    "The end-to-end customization workflow involves stages like model training, model evaluation, and deployment for inference. This model customization approach on SageMaker AI provides greater flexibility and control to fine-tune its supported Amazon Nova models, optimize hyperparameters with precision, and implement techniques including LoRA Parameter-Efficient Fine-Tuning (PEFT), Full-Rank Supervised Fine-Tuning, and Direct Preference Optimization (DPO).\n",
    "\n",
    "\n",
    "This notebook demonstrates Direct Preference Optimization (DPO) of Amazon Nova using Amazon SageMaker Training Job. DPO is a technique that allows fine-tuning language models based on human preferences, enabling the model to better align with human values and preferences.\n",
    "\n",
    "\n",
    "> **Note:** This notebook demonstrates fine-tuning using Nova Lite, but the same techniques can be applied to Nova Pro or Nova Micro models with appropriate adjustments to the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2234305",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "\n",
    "The first cell installs the required Python packages for this notebook. For more details on other pre-requisites needed check out [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-general-prerequisites.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014c9168-59b5-4707-8d67-71339389e5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./sagemaker-2.245.1.dev0.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ./requirements.txt (line 2)) (1.39.3)\n",
      "Requirement already satisfied: datasets==3.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ./requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: sagemaker-mlflow==0.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ./requirements.txt (line 4)) (0.1.0)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ./requirements.txt (line 5)) (1.6.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ./requirements.txt (line 6)) (7.0.0)\n",
      "Requirement already satisfied: py7zr in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r ./requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: attrs<26,>=24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.115.12)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (4.24.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: packaging<25,>=23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (4.3.8)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (5.29.5)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (1.0.37)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: uvicorn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.34.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==3.6.0->-r ./requirements.txt (line 3)) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==3.6.0->-r ./requirements.txt (line 3)) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==3.6.0->-r ./requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==3.6.0->-r ./requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==3.6.0->-r ./requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==3.6.0->-r ./requirements.txt (line 3)) (0.33.2)\n",
      "Requirement already satisfied: mlflow>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn==1.6.1->-r ./requirements.txt (line 5)) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn==1.6.1->-r ./requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn==1.6.1->-r ./requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3->-r ./requirements.txt (line 2)) (1.39.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3->-r ./requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3->-r ./requirements.txt (line 2)) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.40.0,>=1.39.3->boto3->-r ./requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (3.12.7)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (4.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (3.22.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from omegaconf<3,>=2.2->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.3->boto3->-r ./requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2.9.2)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (14.0.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.25.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: texttable in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from py7zr->-r ./requirements.txt (line 7)) (3.23.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: pyzstd>=0.16.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from py7zr->-r ./requirements.txt (line 7)) (0.17.0)\n",
      "Requirement already satisfied: pyppmd<1.3.0,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.0.6)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from py7zr->-r ./requirements.txt (line 7)) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from py7zr->-r ./requirements.txt (line 7)) (1.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0->-r ./requirements.txt (line 3)) (1.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: mlflow-skinny==3.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: Flask<4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.16.2)\n",
      "Requirement already satisfied: gunicorn<24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (23.0.0)\n",
      "Requirement already satisfied: matplotlib<4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.10.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.0.41)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (8.1.8)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.57.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.44)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.34.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: Mako in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.3.10)\n",
      "Requirement already satisfied: tomli in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.40.3)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.46.2)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (4.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.55b1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: greenlet>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow==0.1.0->-r ./requirements.txt (line 4)) (3.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from starlette<0.47.0,>=0.40.0->fastapi->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (1.7.7)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.245.1.dev0->-r ./requirements.txt (line 1)) (0.3.6)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.245.1.dev0-py3-none-any.whl size=1651861 sha256=a3200be025a9fd1e2aba119b05d84e5372ca6ffa71734d74198a911e63ebdbe6\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/df/ad/4f/d0e921991270f56ea2eb5a16205186ec9076ae1f25fa66df41\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.245.1.dev0\n",
      "    Uninstalling sagemaker-2.245.1.dev0:\n",
      "      Successfully uninstalled sagemaker-2.245.1.dev0\n",
      "Successfully installed sagemaker-2.245.1.dev0\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c9e5c-c57c-42cd-baf4-e139422cc147",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b6105-ecec-4213-b56d-589238844dca",
   "metadata": {},
   "source": [
    "## Step 0: Prerequisites\n",
    "\n",
    "This section sets up the necessary AWS credentials and SageMaker session to run the notebook. You'll need proper IAM permissions to use SageMaker.\n",
    "\n",
    "\n",
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n",
    "The code initializes a SageMaker session, sets up the IAM role, and configures the S3 bucket for storing training data and model artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce51663-0171-4d54-b16e-f85e3cadb692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Unable to load JumpStart region config.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/jumpstart/constants.py\", line 69, in _load_region_config\n",
      "    with open(filepath) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/jumpstart/region_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::905418197933:role/service-role/AmazonSageMaker-ExecutionRole-20240807T161358\n",
      "sagemaker bucket: sagemaker-us-east-1-905418197933\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b95b61-8666-4015-bf2e-fcf68ce38c5b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9cbc4",
   "metadata": {},
   "source": [
    "### Direct Prefernce Optimization\n",
    "#### Preference optimization\n",
    "\n",
    "Direct Preference Optimization (DPO) is an efficient fine-tuning method for Large Language Models (LLMs) that uses paired comparison data to align model outputs with human preferences. This approach enables direct optimization of model behavior based on human feedback about which responses are more desirable.\n",
    "\n",
    "#### Why preference optimization matters\n",
    "LLMs trained on large-scale data often generate outputs that may be factually correct but fail to align with specific user needs, organizational values, or safety requirements. Preference optimization addresses this gap by allowing organizations to:\n",
    "• Fine-tune models toward desired behavior patterns\n",
    "• Reduce unwanted outputs or harmful responses\n",
    "• Align model responses with brand voice and communication guidelines\n",
    "• Improve response quality based on domain expert feedback\n",
    "\n",
    "#### How DPO works\n",
    "DPO uses paired examples where human evaluators have indicated which of two possible responses is preferred. The model learns to maximize the likelihood of generating preferred responses while minimizing undesired ones. You can implement DPO using either:\n",
    "• Full-rank DPO: Updates all model parameters to optimize for preferences\n",
    "• LoRA-based DPO: Uses lightweight adapters to learn preference alignments, requiring fewer computational resources\n",
    "\n",
    "#### When to choose preference optimization\n",
    "We recommend using DPO under the following circumstances:\n",
    "• When optimizing for subjective outputs that require alignment with specific human preferences\n",
    "• When you need to adjust the model’s tone, style, or content characteristics to match desired response patterns\n",
    "• When making targeted improvements to an existing model based on user feedback and error analysis\n",
    "• When you need to maintain consistent output quality across different use cases\n",
    "• When implementing safety guardrails through preferred response patterns\n",
    "\n",
    "DPO is particularly effective for iterative refinement of model behavior through carefully curated preference datasets that demonstrate desired versus undesired outputs. The method’s flexibility in supporting both full-rank and LoRA-based approaches allows organizations to choose the most suitable implementation based on their computational resources and specific requirements.\n",
    "\n",
    "![imgs/dpo_sft.png](imgs/dpo_sft.png)\n",
    "\n",
    "Source: https://arxiv.org/pdf/2305.18290\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82089d28-b97a-4956-83fb-d8c46d44fdb5",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the dataset\n",
    "\n",
    "In this example, we are going to load [nvidia/When2Call](https://huggingface.co/datasets/nvidia/When2Call) dataset, an open-source dataset and model suite focused on enabling and improving function calling capabilities for large language models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a70b2",
   "metadata": {},
   "source": [
    "### Step 1.1: Data Loading\n",
    "\n",
    "This code loads the When2Call dataset from Hugging Face, specifically the \"train_pref\" split which contains preference data needed for DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d481791d-9c86-4d32-a39a-918aff5e432f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc56d4987d9b4ac58b566edd0bbfd105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05de65d0578946259cdfd963793dada5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "when2call_train_pref.jsonl:   0%|          | 0.00/17.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fc429edb6e459ebe75cc8771972e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tools', 'messages', 'chosen_response', 'rejected_response'],\n",
       "    num_rows: 9000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nvidia/When2Call\", \"train_pref\", split=\"train\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8e2e6",
   "metadata": {},
   "source": [
    "Converting the dataset to a pandas DataFrame makes it easier to work with and manipulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731e78ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tools</th>\n",
       "      <th>messages</th>\n",
       "      <th>chosen_response</th>\n",
       "      <th>rejected_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{\"name\": \"get_ico_calendar\", \"description\": \"...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Show me complete...</td>\n",
       "      <td>{'role': 'assistant', 'content': '&lt;TOOLCALL&gt;[{...</td>\n",
       "      <td>{'role': 'assistant', 'content': 'Which langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{\"name\": \"monthly_mortgage_payment\", \"descrip...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Find the 99% con...</td>\n",
       "      <td>{'role': 'assistant', 'content': 'Apologies, b...</td>\n",
       "      <td>{'role': 'assistant', 'content': 'To calculate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{\"name\": \"rgb_to_cmyk\", \"description\": \"Conve...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Convert RGB colo...</td>\n",
       "      <td>{'role': 'assistant', 'content': '&lt;TOOLCALL&gt;[{...</td>\n",
       "      <td>{'role': 'assistant', 'content': 'The CMYK equ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{\"name\": \"whole_foods_order\", \"description\": ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Place a small or...</td>\n",
       "      <td>{'role': 'assistant', 'content': '&lt;TOOLCALL&gt;[{...</td>\n",
       "      <td>{'role': 'assistant', 'content': 'Which type o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{\"name\": \"get_channels\", \"description\": \"Retr...</td>\n",
       "      <td>[{'role': 'user', 'content': 'I am interested ...</td>\n",
       "      <td>{'role': 'assistant', 'content': 'To assist yo...</td>\n",
       "      <td>{'role': 'assistant', 'content': '&lt;TOOLCALL&gt;[{...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tools  \\\n",
       "0  [{\"name\": \"get_ico_calendar\", \"description\": \"...   \n",
       "1  [{\"name\": \"monthly_mortgage_payment\", \"descrip...   \n",
       "2  [{\"name\": \"rgb_to_cmyk\", \"description\": \"Conve...   \n",
       "3  [{\"name\": \"whole_foods_order\", \"description\": ...   \n",
       "4  [{\"name\": \"get_channels\", \"description\": \"Retr...   \n",
       "\n",
       "                                            messages  \\\n",
       "0  [{'role': 'user', 'content': 'Show me complete...   \n",
       "1  [{'role': 'user', 'content': 'Find the 99% con...   \n",
       "2  [{'role': 'user', 'content': 'Convert RGB colo...   \n",
       "3  [{'role': 'user', 'content': 'Place a small or...   \n",
       "4  [{'role': 'user', 'content': 'I am interested ...   \n",
       "\n",
       "                                     chosen_response  \\\n",
       "0  {'role': 'assistant', 'content': '<TOOLCALL>[{...   \n",
       "1  {'role': 'assistant', 'content': 'Apologies, b...   \n",
       "2  {'role': 'assistant', 'content': '<TOOLCALL>[{...   \n",
       "3  {'role': 'assistant', 'content': '<TOOLCALL>[{...   \n",
       "4  {'role': 'assistant', 'content': 'To assist yo...   \n",
       "\n",
       "                                   rejected_response  \n",
       "0  {'role': 'assistant', 'content': 'Which langua...  \n",
       "1  {'role': 'assistant', 'content': 'To calculate...  \n",
       "2  {'role': 'assistant', 'content': 'The CMYK equ...  \n",
       "3  {'role': 'assistant', 'content': 'Which type o...  \n",
       "4  {'role': 'assistant', 'content': '<TOOLCALL>[{...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eec66a",
   "metadata": {},
   "source": [
    "### Step 1.2: Train/Val/Test Split\n",
    "\n",
    "The dataset is split into training (72%), validation (18%), and test (10%) sets to properly evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df908a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train elements:  7938\n",
      "Number of test elements:  900\n",
      "Number of val elements:  162\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.02, random_state=42)\n",
    "\n",
    "print(\"Number of train elements: \", len(train))\n",
    "print(\"Number of test elements: \", len(test))\n",
    "print(\"Number of val elements: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368c020-e9a3-48b3-a53b-45404bba9482",
   "metadata": {},
   "source": [
    "### Understanding the Nova Format for PPO\n",
    "\n",
    "Let's format the dataset by using the prompt style for Amazon Nova:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": [{\"text\": Content of the System prompt}],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\"text\": Content of the user prompt]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\"text\": Content of the answer]\n",
    "        },\n",
    "        ...\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"candidates\": [\n",
    "                {\n",
    "                    \"content\": [\"text\": Content of the answer, \"preferenceLabel\": \"preferred\"],\n",
    "                    \"content\": [\"text\": Content of the answer, \"preferenceLabel\": \"non-preferred\"]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8d4b2",
   "metadata": {},
   "source": [
    "### Step 1.3: Data Preprocessing \n",
    "The notebook defines utility functions to clean the dataset content by removing prefixes and handling special cases:\n",
    "\n",
    "```python\n",
    "def clean_prefix(content):\n",
    "    # Removes prefixes like \"USER:\", \"ASSISTANT:\", etc.\n",
    "    ...\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "\n",
    "def clean_numbered_conversation(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5416d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_prefix(content):\n",
    "    \"\"\"Remove prefixes from content, according to Nova data_validator\"\"\"\n",
    "    prefixes = [\n",
    "        \"SYSTEM:\",\n",
    "        \"System:\",\n",
    "        \"USER:\",\n",
    "        \"User:\",\n",
    "        \"ASSISTANT:\",\n",
    "        \"Assistant:\",\n",
    "        \"Bot:\",\n",
    "        \"BOT:\",\n",
    "    ]\n",
    "\n",
    "    # Handle array case (list of content items)\n",
    "    if hasattr(content, \"__iter__\") and not isinstance(content, str):\n",
    "        for i, item in enumerate(content):\n",
    "            if isinstance(item, dict) and \"text\" in item:\n",
    "                text = item[\"text\"]\n",
    "                if isinstance(text, str):\n",
    "                    # Clean line by line for multi-line text\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    cleaned_lines = []\n",
    "                    for line in lines:\n",
    "                        cleaned_line = line.strip()\n",
    "                        for prefix in prefixes:\n",
    "                            if cleaned_line.startswith(prefix):\n",
    "                                cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                                break\n",
    "                        cleaned_lines.append(cleaned_line)\n",
    "                    item[\"text\"] = \"\\n\".join(cleaned_lines)\n",
    "        return content\n",
    "\n",
    "    # Handle string case\n",
    "    if isinstance(content, str):\n",
    "        lines = content.split(\"\\n\")\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            cleaned_line = line.strip()\n",
    "            for prefix in prefixes:\n",
    "                if cleaned_line.startswith(prefix):\n",
    "                    cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                    break\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    \"\"\"Clean message list from None values and convert to list of dicts if needed.\"\"\"\n",
    "    if isinstance(message_list, str):\n",
    "        message_list = json.loads(message_list)\n",
    "\n",
    "    tmp_cleaned = []\n",
    "    for msg in message_list:\n",
    "        new_msg = {}\n",
    "        for key, value in msg.items():\n",
    "            if key in [\"candidates\", \"content\"]:\n",
    "                if value is None or str(value).lower() == \"None\":\n",
    "                    continue\n",
    "            new_msg[key] = value\n",
    "        tmp_cleaned.append(new_msg)\n",
    "\n",
    "    cleaned = []\n",
    "    for item in tmp_cleaned:\n",
    "        if item[\"role\"] == \"assistant\":\n",
    "            # Clean prefixes from candidates content\n",
    "            if \"candidates\" in item:\n",
    "                candidates = item[\"candidates\"]\n",
    "                for candidate in candidates:\n",
    "                    if isinstance(candidate, dict) and \"content\" in candidate:\n",
    "                        content = candidate[\"content\"]\n",
    "                        for content_item in content:\n",
    "                            if (\n",
    "                                isinstance(content_item, dict)\n",
    "                                and \"text\" in content_item\n",
    "                            ):\n",
    "                                # First clean numbered conversation format\n",
    "                                text = clean_numbered_conversation(content_item[\"text\"])\n",
    "                                # Then clean regular prefixes\n",
    "                                content_item[\"text\"] = clean_prefix(text)\n",
    "            cleaned.append({\"role\": item[\"role\"], \"candidates\": item[\"candidates\"]})\n",
    "        else:\n",
    "            content = item[\"content\"]\n",
    "            for content_item in content:\n",
    "                if isinstance(content_item, dict) and \"text\" in content_item:\n",
    "                    text = clean_numbered_conversation(content_item[\"text\"])\n",
    "                    content_item[\"text\"] = clean_prefix(text)\n",
    "            cleaned.append({\"role\": item[\"role\"], \"content\": content})\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Additional function to specifically handle the numbered conversation format\n",
    "def clean_numbered_conversation(text):\n",
    "    \"\"\"Clean numbered conversation format like '1. User: ...'\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Pattern to match numbered items with User: or Assistant: prefixes\n",
    "    pattern = r\"(\\d+\\.\\s*)(User:|Assistant:)\\s*\"\n",
    "\n",
    "    # Replace the pattern, keeping the number but removing the role prefix\n",
    "    cleaned_text = re.sub(pattern, r\"\\1\", text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44dcd6",
   "metadata": {},
   "source": [
    "Now let's define the functions to parse the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a083a",
   "metadata": {},
   "source": [
    "### Dataset Parsing Functions\n",
    "\n",
    "These functions transform the dataset into the format required by Nova models, handling tool calls and formatting:\n",
    "\n",
    "```python\n",
    "def transform_tool_format(tool):\n",
    "    # Transforms tool format to Nova's expected format\n",
    "    ...\n",
    "\n",
    "def extract_toolcall_content(text):\n",
    "    # Extracts content between <TOOLCALL> tags\n",
    "    ...\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    # Prepares dataset in the required format for Nova models\n",
    "    ...\n",
    "\n",
    "def prepare_dataset_test(sample):\n",
    "    # Formats validation dataset for evaluation\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86eb8edd-35c0-4cf1-82d3-54417bdabd6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:01.435195Z",
     "start_time": "2023-09-03T00:02:01.429794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    \"\"\"Transform tool from old format to Nova format.\"\"\"\n",
    "    if isinstance(tool, str):\n",
    "        tool = json.loads(tool)\n",
    "\n",
    "    return {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": tool[\"name\"],\n",
    "            \"description\": tool[\"description\"],\n",
    "            \"inputSchema\": {\"json\": tool[\"parameters\"]},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_toolcall_content(text):\n",
    "    \"\"\"Extract content between <TOOLCALL> tags if present.\"\"\"\n",
    "    if isinstance(text, dict):\n",
    "        if text.get(\"content\"):\n",
    "            text = text.get(\"content\")\n",
    "\n",
    "    if \"<TOOLCALL>\" in text and \"</TOOLCALL>\" in text:\n",
    "        pattern = r\"<TOOLCALL>(.*?)</TOOLCALL>\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            tool_calls_text = []\n",
    "            if isinstance(match.group(1), str):\n",
    "                tools = json.loads(match.group(1))\n",
    "\n",
    "            for tool_call in tools:\n",
    "                arguments = (\n",
    "                    json.loads(tool_call[\"arguments\"])\n",
    "                    if isinstance(tool_call[\"arguments\"], str)\n",
    "                    else tool_call[\"arguments\"]\n",
    "                )\n",
    "                tool_call_json = {\n",
    "                    \"name\": tool_call[\"name\"],\n",
    "                    \"parameters\": arguments,\n",
    "                }\n",
    "                tool_calls_text.append(json.dumps(tool_call_json))\n",
    "\n",
    "            return \"\".join(tool_calls_text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    \"\"\"Prepare dataset in the required format for Nova models\"\"\"\n",
    "    # Add user messages\n",
    "    result = {\"system\": [], \"messages\": []}\n",
    "\n",
    "    if isinstance(sample[\"tools\"], str):\n",
    "        tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    else:\n",
    "        tools = sample[\"tools\"]\n",
    "\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    # Add system message with tools if tools exist\n",
    "    if transformed_tools:\n",
    "        tools_dict = {\"tools\": transformed_tools}\n",
    "        system_text = (\n",
    "            \"You may call one or more functions to assist with the user query.\\n\\n\"\n",
    "            \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "            \"<tools>\\n\"\n",
    "            f\"{json.dumps(tools_dict)}\\n\"\n",
    "            \"</tools>\\n\\n\"\n",
    "            \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "            '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "        )\n",
    "        result[\"system\"] = [{\"text\": system_text}]\n",
    "\n",
    "    for message in sample.get(\"messages\", []):\n",
    "        result[\"messages\"].append(\n",
    "            {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": [{\"text\": extract_toolcall_content(message[\"content\"])}],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    chosen = extract_toolcall_content(sample[\"chosen_response\"])\n",
    "    rejected = extract_toolcall_content(sample[\"rejected_response\"])\n",
    "\n",
    "    result[\"messages\"].append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"candidates\": [\n",
    "                {\"content\": [{\"text\": chosen}], \"preferenceLabel\": \"preferred\"},\n",
    "                {\"content\": [{\"text\": rejected}], \"preferenceLabel\": \"non-preferred\"},\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db326040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_test(sample):\n",
    "    \"\"\"Parse sample and format it for validation dataset.\"\"\"\n",
    "    # Process tools upfront\n",
    "    if isinstance(sample[\"tools\"], str):\n",
    "        tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    else:\n",
    "        tools = sample[\"tools\"]\n",
    "\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    # Initialize variables\n",
    "    system_content = \"\"\n",
    "    current_input = \"\"\n",
    "\n",
    "    # Add system message with tools if tools exist\n",
    "    if transformed_tools:\n",
    "        system_content = (\n",
    "            \"You may call one or more functions to assist with the user query.\\n\\n\"\n",
    "            \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "            \"<tools>\\n\"\n",
    "            f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "            \"</tools>\\n\\n\"\n",
    "            \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "            '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "        )\n",
    "\n",
    "    for message in sample.get(\"messages\", []):\n",
    "        if message[\"role\"] == \"user\":\n",
    "            current_input = (\n",
    "                current_input\n",
    "                + \"\\n##User: \"\n",
    "                + extract_toolcall_content(message[\"content\"])\n",
    "            )\n",
    "        else:\n",
    "            current_input = (\n",
    "                current_input\n",
    "                + \"\\n##Assistant: \"\n",
    "                + extract_toolcall_content(message[\"content\"])\n",
    "            )\n",
    "\n",
    "    if current_input.startswith(\"\\n\"):\n",
    "        current_input = current_input[1:]\n",
    "\n",
    "    current_input = current_input.strip()\n",
    "\n",
    "    chosen = extract_toolcall_content(sample[\"chosen_response\"])\n",
    "\n",
    "    return {\"system\": system_content, \"query\": current_input, \"response\": chosen}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a745a0",
   "metadata": {},
   "source": [
    "### Step 1.4: Data Preperation in Converse Format for Train and Validation Datasets\n",
    "\n",
    "The notebook applies the functions to transform the datasets into the required formats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e9cbedd-7403-467e-8cc6-1d2550d8b8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:10.364459Z",
     "start_time": "2023-09-03T00:02:09.672705Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f81a2acb4c4422a0cd285586c6d3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7938 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"messages\":[{\"role\":\"user\",\"content\":[{\"text\":\"Count the occurrences of each fruit in the given list.\"}]},{\"role\":\"assistant\",\"candidates\":[{\"content\":[{\"text\":\"To assist you better, could you please provide me with the list of fruits?\"}],\"preferenceLabel\":\"preferred\"},{\"content\":[{\"text\":\"I'm sorry for the inconvenience, but I'm currently unable to perform tasks such as counting occurrences in a list.\"}],\"preferenceLabel\":\"non-preferred\"}]}],\"system\":[{\"text\":\"You may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools><\\/tools> XML tags:\\n<tools>\\n{\\\"tools\\\": [{\\\"toolSpec\\\": {\\\"name\\\": \\\"get_range\\\", \\\"description\\\": \\\"Helper function to format the range string.\\\", \\\"inputSchema\\\": {\\\"json\\\": {\\\"type\\\": \\\"dict\\\", \\\"properties\\\": {\\\"start\\\": {\\\"description\\\": \\\"The start of the range.\\\", \\\"type\\\": \\\"int\\\"}, \\\"end\\\": {\\\"description\\\": \\\"The end of the range.\\\", \\\"type\\\": \\\"int\\\"}}}}}}, {\\\"toolSpec\\\": {\\\"name\\\": \\\"count_occurrences\\\", \\\"description\\\": \\\"Counts the occurrences of each element in a list and returns a dictionary with the counts.\\\", \\\"inputSchema\\\": {\\\"json\\\": {\\\"type\\\": \\\"dict\\\", \\\"properties\\\": {\\\"lst\\\": {\\\"description\\\": \\\"The input list.\\\", \\\"type\\\": \\\"List\\\"}}}}}}, {\\\"toolSpec\\\": {\\\"name\\\": \\\"calculate_grade\\\", \\\"description\\\": \\\"Calculates the weighted average grade based on scores and their corresponding weights.\\\", \\\"inputSchema\\\": {\\\"json\\\": {\\\"type\\\": \\\"dict\\\", \\\"properties\\\": {\\\"scores\\\": {\\\"description\\\": \\\"A list of scores.\\\", \\\"type\\\": \\\"List[float]\\\"}, \\\"weights\\\": {\\\"description\\\": \\\"A list of weights corresponding to each score.\\\", \\\"type\\\": \\\"List[float]\\\"}}}}}}]}\\n<\\/tools>\\n\\nFor each function call, return a json object with function name and parameters:\\n{\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}\"}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2750c2b7274546a18230e8bcc00da133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from random import randint\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\"train\": train_dataset, \"test\": test_dataset, \"val\": val_dataset}\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset, remove_columns=train_dataset.features\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.to_pandas()\n",
    "\n",
    "train_dataset[\"messages\"] = train_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(train_dataset.iloc[randint(0, len(train_dataset))].to_json())\n",
    "\n",
    "val_dataset = dataset[\"val\"].map(\n",
    "    prepare_dataset, remove_columns=test_dataset.features\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.to_pandas()\n",
    "\n",
    "val_dataset[\"messages\"] = val_dataset[\"messages\"].apply(clean_message_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ef3bb",
   "metadata": {},
   "source": [
    "### Step 1.5: Data Preperation on test data for Offline Evaluation post fine tuning\n",
    "\n",
    "Let's format the test dataset in the format:\n",
    "\n",
    "Required Fields:\n",
    "\n",
    "* query: String containing the question or instruction that needs an answer\n",
    "* response: String containing the expected model output\n",
    "\n",
    "Optional Fields:\n",
    "\n",
    "* system: String containing the system prompt that sets the behavior, role, or personality of the AI model before it processes the query\n",
    "\n",
    "Example Entry\n",
    "```\n",
    "\n",
    "{\n",
    "   \"system\":\"You are a english major with top marks in class who likes to give minimal word responses: \",\n",
    "   \"query\":\"What is the symbol that ends the sentence as a question\",\n",
    "   \"response\":\"?\"\n",
    "}\n",
    "{\n",
    "   \"system\":\"You are a pattern analysis specialist that provides succinct answers: \",\n",
    "   \"query\":\"What is the next number in this series? 1, 2, 4, 8, 16, ?\",\n",
    "   \"response\":\"32\"\n",
    "}\n",
    "{\n",
    "   \"system\":\"You have great attention to detail that follows instructions accurately: \",\n",
    "   \"query\":\"Repeat only the last two words of the following: I ate a hamburger today and it was kind of dry\",\n",
    "   \"response\":\"of dry\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da049f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc4d49890f84d9fa1944bca620bd73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'You may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"tools\": [{\"toolSpec\": {\"name\": \"search_gifs\", \"description\": \"Search for gifs based on a query using the Humor API.\", \"inputSchema\": {\"json\": {\"type\": \"dict\", \"properties\": {\"query\": {\"description\": \"The search query to find matching gifs.\", \"type\": \"str\", \"default\": \"cat\"}, \"number\": {\"description\": \"The number of results to retrieve, between 1 and 10. Defaults to 3.\", \"type\": \"int, optional\", \"default\": 3}}}}}}]}\\n</tools>\\n\\nFor each function call, return a json object with function name and parameters:\\n{\"name\": function name, \"parameters\": dictionary of argument name and its value}', 'query': '##User: I need 5 gifs of cute puppies.', 'response': '{\"name\": \"search_gifs\", \"parameters\": {\"query\": \"cute puppies\", \"number\": 5}}'}\n"
     ]
    }
   ],
   "source": [
    "test_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset_test, remove_columns=test_dataset.features\n",
    ")\n",
    "\n",
    "print(test_dataset[randint(0, len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e667af-8197-4d2f-8432-82db6a1d3006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:46:36.592759Z",
     "iopub.status.busy": "2024-12-17T16:46:36.591798Z",
     "iopub.status.idle": "2024-12-17T16:46:36.603128Z",
     "shell.execute_reply": "2024-12-17T16:46:36.598965Z",
     "shell.execute_reply.started": "2024-12-17T16:46:36.592728Z"
    }
   },
   "source": [
    "### Step 1.6: Upload all 3 curated datasets (train, test, val) to Amazon S3\n",
    "\n",
    "The notebook applies the functions to transform the datasets into the required formats\n",
    "\n",
    "\n",
    "The processed datasets are saved locally and then uploaded to Amazon S3 for use in SageMaker training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d97f29e5-4aed-4939-8d51-ad3c5268299f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db05863-3acb-483b-8e34-2aacbdbc68a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/nova-dpo\"\n",
    "else:\n",
    "    input_path = f\"datasets/nova-dpo\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.jsonl\"\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/dataset.jsonl\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/gen_qa.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "064d0321-1bd5-4c62-845a-bb1b9a3891a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1ab38319b447c5b5ba179b4d331666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-905418197933/datasets/nova-dpo/train/dataset.jsonl\n",
      "s3://sagemaker-us-east-1-905418197933/datasets/nova-dpo/test/gen_qa.jsonl\n",
      "s3://sagemaker-us-east-1-905418197933/datasets/nova-dpo/val/dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save datasets to s3\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "os.makedirs(\"./data/test\", exist_ok=True)\n",
    "os.makedirs(\"./data/val\", exist_ok=True)\n",
    "\n",
    "train_dataset.to_json(\"./data/train/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "val_dataset.to_json(\"./data/val/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "test_dataset.to_json(\"./data/test/gen_qa.jsonl\")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/train/dataset.jsonl\", bucket_name, f\"{input_path}/train/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/val/dataset.jsonl\", bucket_name, f\"{input_path}/val/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/test/gen_qa.jsonl\", bucket_name, f\"{input_path}/test/gen_qa.jsonl\"\n",
    ")\n",
    "\n",
    "shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(val_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c237-28bd-474e-9444-94aaea8e6979",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457beda-117d-4782-9f04-0680c199e98a",
   "metadata": {},
   "source": [
    "## Step 2: Model fine-tuning\n",
    "\n",
    "We now define the PyTorch estimator to run the supervised fine-tuning on a tool-calling dataset for our Amazon Nova model\n",
    "\n",
    "This section sets up and runs the fine-tuning job using SageMaker. It uses Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) to efficiently train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4bda7",
   "metadata": {},
   "source": [
    "#### Instance Type and Count\n",
    "\n",
    "P5 instances are optimized for deep learning workloads, providing high-performance GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8cecfd-e640-4527-99d4-cb3cec9093b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml.p5.48xlarge'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_type = \"ml.p5.48xlarge\"\n",
    "instance_count = 2\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ec8be",
   "metadata": {},
   "source": [
    "#### Image URI\n",
    "\n",
    "This specifies the pre-built container for SFT fine-tuning, which is different from the DPO container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5df7700-7c66-4af8-aea0-da0e5af493bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-TJ-DPO-latest'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nova-fine-tune-repo:SM-TJ-DPO-latest\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ed30c",
   "metadata": {},
   "source": [
    "#### Configuring the Model and Recipe\n",
    "\n",
    "This specifies which model to fine-tune and the recipe to use. The recipe includes \"lora\" indicating parameter-efficient fine-tuning, and \"sft\" indicating supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d100cecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"nova-micro/prod\"\n",
    "recipe = \"fine-tuning/nova/nova_micro_p5_gpu_dpo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68a358",
   "metadata": {},
   "source": [
    "#### PyTorch Estimator\n",
    "\n",
    "This creates a PyTorch estimator with the configuration to run the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b95841fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using instance_count argument to estimator to set number of nodes. Ignoring run -> replicas in recipe.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-dpo\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"replicas\": instance_count,  # Required\n",
    "    },\n",
    "    \"training_config\": {\"trainer\": {\"max_epochs\": 1}},\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a9cb9",
   "metadata": {},
   "source": [
    "#### Configuring the Data Channels\n",
    "\n",
    "Configure the Data Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a386bd9-172c-485c-af45-ebc1d126470b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    s3_data=train_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")\n",
    "\n",
    "val_input = TrainingInput(\n",
    "    s3_data=val_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b25e13aa-1df2-43fc-bae4-15f5b7113191",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: train-nova-micro-dpo-2025-07-08-22-02-05-132\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(inputs={\"train\": train_input, \"validation\": val_input}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6772b70c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Job Name:  train-nova-micro-dpo-2025-07-08-22-02-05-132\n"
     ]
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print('Training Job Name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90df6baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/train-nova-micro-dpo-2025-07-08-22-02-05-132\">Training Job</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=train-nova-micro-dpo-2025-07-08-22-02-05-132;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-905418197933/train-nova-micro-dpo-2025-07-08-22-02-05-132/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, Markdown, Image\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(\"us-east-1\", training_job_name)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import HTML, Markdown, Image\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(\"us-east-1\", training_job_name)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import HTML, Markdown, Image\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket_name, training_job_name, \"us-east-1\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be572ce1",
   "metadata": {},
   "source": [
    "### Reading the Output Content after training job completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae475e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_uri = estimator.model_data\n",
    "print(model_s3_uri)\n",
    "\n",
    "output_s3_uri = \"/\".join(model_s3_uri.split(\"/\")[:-1])+\"/output.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adbd9e",
   "metadata": {},
   "source": [
    "### Downloading and Extracting the Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a966b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp/train_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $output_s3_uri ./tmp/train_output/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./tmp/train_output/output.tar.gz -C ./tmp/train_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac693550",
   "metadata": {},
   "outputs": [],
   "source": [
    "escrow_model_uri = json.load(open('./tmp/train_output/manifest.json'))['checkpoint_s3_bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "escrow_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7235f97",
   "metadata": {},
   "source": [
    "### Plotting the Train/Loss Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV files\n",
    "train_df = pd.read_csv('./tmp/train_output/step_wise_training_metrics.csv')\n",
    "val_df = pd.read_csv('./tmp/train_output/validation_metrics.csv')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_df['step_number'], train_df['training_loss'], label='Training Loss', color='blue')\n",
    "plt.plot(val_df['step_number'], val_df['validation_loss'], label='Validation Loss', color='red')\n",
    "\n",
    "plt.xlabel('Step Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d9b8d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f527c76",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31573633",
   "metadata": {},
   "source": [
    "Create minimal recipe for `gen_qa` evaluation. With `gen_qa` evaluation, we bring our own dataset for evaluation, and measure the following metrics:\n",
    "\n",
    "* rouge1\n",
    "* rouge2\n",
    "* rougeL\n",
    "* exact_match\n",
    "* quasi_exact_match\n",
    "* f1_score\n",
    "* f1_score_quasi\n",
    "* bleu\n",
    "\n",
    "Your fine-tuned model checkpoints are accessible through the `manifest.json` in the output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6024f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_job_name = \"nova-lite-gen_qa-eval-job\"\n",
    "\n",
    "recipe_content = f\"\"\"\n",
    "run:\n",
    "  name: {recipe_job_name}\n",
    "  model_type: amazon.nova-lite-v1:0:300k\n",
    "  model_name_or_path: {escrow_model_uri}\n",
    "  data_s3_path: \"\" # Empty string\n",
    "\n",
    "evaluation:\n",
    "  task: gen_qa\n",
    "  strategy: gen_qa\n",
    "  metric: all\n",
    "\n",
    "inference:\n",
    "  max_new_tokens: 4096\n",
    "  top_p: 0.9\n",
    "  temperature: 0.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"eval-recipe.yaml\", \"w\") as f:\n",
    "  f.write(recipe_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb19b27",
   "metadata": {},
   "source": [
    "### Instance count and Instance Type\n",
    "\n",
    "Defines the Instance type and count to use for Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654130e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\" # Override the instance type if you want to get a different container version\n",
    "instance_count = 1\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f7c12",
   "metadata": {},
   "source": [
    "#### Image URI for Evaluation\n",
    "\n",
    "This specifies the pre-built container for Evaluation, which is different from the SFT container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af413cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-latest\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4a853",
   "metadata": {},
   "source": [
    "#### Configuring the Model and Recipe\n",
    "\n",
    "This specifies which model evaluation to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c359e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nova-lite/prod\"\n",
    "recipe = \"./eval-recipe.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d19059",
   "metadata": {},
   "source": [
    "#### PyTorch Estimator\n",
    "\n",
    "This creates a PyTorch estimator with the configuration to run the evaluation job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-dpo-eval\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"replicas\": instance_count,  # Required\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee888ed5",
   "metadata": {},
   "source": [
    "### Configuring the Data Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e37de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "eval_input = TrainingInput(\n",
    "    s3_data=test_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "eval_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107342c8",
   "metadata": {},
   "source": [
    "### Starting the Training Job\n",
    "This starts the training job with the configured estimator and datasets. Note that it uses the test dataset for validation during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "eval_estimator.fit(inputs={\"train\": eval_input}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad97ca",
   "metadata": {},
   "source": [
    "### Viewing the Evaluation Artifacts \n",
    "Downloading the artifact from Evaluation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = eval_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '/'.join(output.split(\"/\")[:-1]) +\"/output.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp $output ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a182d",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "The notebook defines a function to visualize the evaluation metrics in a bar chart:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7028860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "tarfile.open('output.tar.gz', 'r:gz').extractall('output_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60656e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"output_folder/\" + recipe_job_name +\"/eval_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_metrics(results):\n",
    "    # Extract metrics and their standard errors\n",
    "    metrics = {}\n",
    "    for key, value in results.items():\n",
    "        if not key.endswith(\"_stderr\"):\n",
    "            metrics[key] = {\"value\": value, \"stderr\": results.get(f\"{key}_stderr\", 0)}\n",
    "\n",
    "    # Sort metrics by value for better visualization\n",
    "    sorted_metrics = dict(\n",
    "        sorted(metrics.items(), key=lambda x: x[1][\"value\"], reverse=True)\n",
    "    )\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    labels = list(sorted_metrics.keys())\n",
    "    values = [sorted_metrics[label][\"value\"] for label in labels]\n",
    "    errors = [sorted_metrics[label][\"stderr\"] for label in labels]\n",
    "\n",
    "    # Normalize BLEU score to be on the same scale as other metrics (0-1)\n",
    "    bleu_index = labels.index(\"bleu\") if \"bleu\" in labels else -1\n",
    "    if bleu_index >= 0:\n",
    "        values[bleu_index] /= 100\n",
    "        errors[bleu_index] /= 100\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create bar chart\n",
    "    x = np.arange(len(labels))\n",
    "    bars = ax.bar(\n",
    "        x,\n",
    "        values,\n",
    "        yerr=errors,\n",
    "        align=\"center\",\n",
    "        alpha=0.7,\n",
    "        capsize=5,\n",
    "        color=\"skyblue\",\n",
    "        ecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Evaluation Metrics\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        # Convert BLEU back to its original scale for display\n",
    "        display_value = values[i] * 100 if labels[i] == \"bleu\" else values[i]\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{display_value:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # Add a note about BLEU\n",
    "    if bleu_index >= 0:\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.15,\n",
    "            \"Note: BLEU score shown as percentage (original: {:.2f})\".format(\n",
    "                values[bleu_index] * 100\n",
    "            ),\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"center\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1510a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def find_json_files(path):\n",
    "    return glob.glob(os.path.join(path, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa81439",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_path = find_json_files(results_path)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67bc309",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50485e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(evaluation_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fig = plot_metrics(data[\"results\"][\"all\"])\n",
    "\n",
    "output_file = os.path.join(\"./\", 'evaluation_metrics.png')\n",
    "fig.savefig(output_file, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ad62a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281b2d7",
   "metadata": {},
   "source": [
    "## Model deployment and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67adae29",
   "metadata": {},
   "source": [
    "After training and evaluating our model, we want to make it available for inference. Amazon Bedrock provides a serverless endpoint for model deployment, allowing us to serve the model without managing infrastructure.\n",
    "\n",
    "The Bedrock Custom Model feature of Amazon Bedrock lets us import our fine-tuned model and access it through the same API as other foundation models. This provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(\"bedrock\", region_name=sess.boto_region_name)\n",
    "\n",
    "\n",
    "model_path = escrow_model_uri\n",
    "\n",
    "# Define name for imported model\n",
    "imported_model_name = \"nova-lite-sagemaker-dpo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f4936",
   "metadata": {},
   "source": [
    "### Creating the Bedrock Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_params = {\n",
    "    \"modelName\": imported_model_name,\n",
    "    \"modelSourceConfig\": {\"s3DataSource\": {\"s3Uri\": model_path}},\n",
    "    \"roleArn\": role,\n",
    "    \"clientRequestToken\": \"NovaRecipeSageMaker\",\n",
    "}\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_custom_model(**request_params)\n",
    "\n",
    "model_arn = response[\"modelArn\"]\n",
    "\n",
    "# Output the model ARN\n",
    "print(f\"Model import job created with ARN: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23652b",
   "metadata": {},
   "source": [
    "### Monitoring the Model status\n",
    "\n",
    "After initiating the model import, we need to monitor its progress. The status goes through several states:\n",
    "\n",
    "* CREATING: Model is being imported\n",
    "* ACTIVE: Import successful\n",
    "* FAILED: Import encountered errors\n",
    "\n",
    "This cell polls the Bedrock API every 60 seconds to check the status of the model import, continuing until it reaches a terminal state (ACTIVE or FAILED). Once the import completes successfully, we'll have the model ARN which we can use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.list_custom_models(sortBy=\"CreationTime\", sortOrder=\"Descending\")\n",
    "    model_summaries = response[\"modelSummaries\"]\n",
    "    status = \"\"\n",
    "    for model in model_summaries:\n",
    "        if model[\"modelName\"] == imported_model_name:\n",
    "            status = model[\"modelStatus\"].upper()\n",
    "            model_arn = model[\"modelArn\"]\n",
    "            print(f'{model[\"modelStatus\"].upper()} {model[\"modelArn\"]} ...')\n",
    "            if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "                break\n",
    "    if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "    \n",
    "    model_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571fde7",
   "metadata": {},
   "source": [
    "##### ⚠️ After the model is ACTIVE, create provisioned throughput before running the inference!\n",
    "\n",
    "Please refer to the official [AWS Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-purchase.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3adf1",
   "metadata": {},
   "source": [
    "### Testing the Deployed Model\n",
    "\n",
    "Now that our model is deployed to Amazon Bedrock, we can invoke it for inference. We'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "Inference Setup Components:\n",
    "* Bedrock Runtime Client: AWS SDK client for making inference calls\n",
    "* Helper Function: To handle retry logic and properly format requests\n",
    "The generate function we're defining:\n",
    "\n",
    "Applies the proper chat template to user messages\n",
    "* Handles retry logic for robustness\n",
    "* Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=sess.boto_region_name,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,  # 5 minutes\n",
    "        retries={\"max_attempts\": 3},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model_id,\n",
    "    messages,\n",
    "    system_prompt=None,\n",
    "    tools=None,\n",
    "    temperature=0.3,\n",
    "    max_tokens=4096,\n",
    "    top_p=0.9,\n",
    "    max_retries=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        model_id (str): ID of the model to use\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        system_prompt (str, optional): System prompt to guide the model\n",
    "        tools (dict, optional): Tool configuration for the model\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    # Prepare base parameters for the API call\n",
    "    kwargs = {\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add optional parameters if provided\n",
    "    if tools:\n",
    "        kwargs[\"toolConfig\"] = tools\n",
    "    if system_prompt:\n",
    "        kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return client.converse(modelId=model_id, messages=messages, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to get response.\")\n",
    "                print(str(e))\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_arn = \"<PROVISIONED_THROUGHPUT_ARN>\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a helpful AI assistant that can answer questions and provide information.\n",
    "You can use tools to help you with your tasks.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "<tools>\n",
    "{{tools}}\n",
    "</tools>\n",
    "For each function call, return a json object with function name and parameters:\n",
    "\n",
    "{{{{\\\"name\\\": \\\"function name\\\", \\\"parameters\\\": \\\"dictionary of argument name and its value\\\"}}}}\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"calculate_bmi\",\n",
    "            \"description\": \"Calculate BMI given weight in kg and height in meters\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"weight_kg\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Property weight_kg\",\n",
    "                        },\n",
    "                        \"height_m\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Property height_m\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"weight_kg\", \"height_m\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"fetch_weather\",\n",
    "            \"description\": 'Fetch weather information\\n\\nArgs:\\nquery: The weather query (e.g., \"weather in New York\")\\nnum_results: Number of results to return (default: 1)\\n\\nReturns:\\nJSON string containing weather information',\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"query\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Property query\",\n",
    "                            },\n",
    "                            \"num_results\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Property num_results\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"query\"],\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "system_prompt = system_prompt.format(tools=json.dumps({\"tools\": tools}))\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": \"What is the weather in Rome, Italy?\"}]},\n",
    "]\n",
    "\n",
    "response = generate(\n",
    "    model_id=model_arn,\n",
    "    system_prompt=system_prompt,\n",
    "    messages=messages,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response[\"output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
