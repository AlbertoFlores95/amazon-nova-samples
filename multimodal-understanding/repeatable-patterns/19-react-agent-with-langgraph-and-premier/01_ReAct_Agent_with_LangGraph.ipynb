{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Agent with LangGraph: Academic Paper Analysis Agent\n",
    "\n",
    "This notebook implements a LangGraph ReAct agent that can analyze academic papers (PDF files) using LangGraph and Amazon Nova. The agent has specific tools to:\n",
    "1. Summarize papers\n",
    "2. Extract research questions\n",
    "3. Extract key results\n",
    "4. Identify research gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-aws langgraph pypdf boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import pypdf\n",
    "from typing import Dict, List, Any, Optional, Union, Literal, TypedDict\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain and LangGraph imports\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, FunctionMessage, ToolMessage\n",
    "#from langchain_aws import BedrockChat\n",
    "from langchain.tools import BaseTool, tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting model_id and AWS region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "model_id = \"us.amazon.nova-premier-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Extraction Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = pypdf.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def get_paper_sections(text):\n",
    "    \"\"\"Simple function to split paper into sections based on common headers\"\"\"\n",
    "    # This is a very simplified approach - real implementation would be more sophisticated\n",
    "    sections = {}\n",
    "    \n",
    "    # Common section headers in academic papers\n",
    "    section_markers = [\n",
    "        \"abstract\", \"introduction\", \"related work\", \"background\", \"methodology\", \n",
    "        \"methods\", \"experimental setup\", \"results\", \"discussion\", \n",
    "        \"conclusion\", \"future work\", \"references\"\n",
    "    ]\n",
    "    \n",
    "    # Split by common section headers (very naive approach)\n",
    "    lines = text.split('\\n')\n",
    "    current_section = \"preamble\"\n",
    "    sections[current_section] = []\n",
    "    \n",
    "    for line in lines:\n",
    "        lower_line = line.lower().strip()\n",
    "        if any(lower_line.startswith(marker) or lower_line == marker for marker in section_markers):\n",
    "            current_section = lower_line\n",
    "            sections[current_section] = []\n",
    "        else:\n",
    "            sections[current_section].append(line)\n",
    "    \n",
    "    # Convert lists of strings to single strings\n",
    "    for section in sections:\n",
    "        sections[section] = \"\\n\".join(sections[section])\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AWS Bedrock setup\n",
    "# You need to have AWS credentials configured\n",
    "# Note: Replace with your AWS credentials and region if you are not using IAM role\n",
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,  # Change to your preferred region\n",
    "    # Use the lines below if you are not using IAM ROLE - remember to never share your credentials or save them in code repos\n",
    "    # aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "    # aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    ")\n",
    "\n",
    "# Create Bedrock LLM\n",
    "llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    client=bedrock_client,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 4000,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing our setup\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State and Paper Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PaperContext:\n",
    "    \"\"\"Class to hold the context of the paper being analyzed\"\"\"\n",
    "    def __init__(self, paper_path=None):\n",
    "        self.paper_path = paper_path\n",
    "        self.full_text = \"\"\n",
    "        self.sections = {}\n",
    "        self.title = \"\"\n",
    "        self.authors = []\n",
    "        self.abstract = \"\"\n",
    "        \n",
    "        if paper_path and os.path.exists(paper_path):\n",
    "            self.load_paper(paper_path)\n",
    "    \n",
    "    def load_paper(self, paper_path):\n",
    "        \"\"\"Load and parse a paper from a PDF file\"\"\"\n",
    "        self.paper_path = paper_path\n",
    "        self.full_text = extract_text_from_pdf(paper_path)\n",
    "        self.sections = get_paper_sections(self.full_text)\n",
    "        \n",
    "        # Extract basic metadata (very naive approach)\n",
    "        lines = self.full_text.split('\\n')\n",
    "        self.title = lines[0] if lines else \"\"\n",
    "        \n",
    "        # Get abstract if it exists\n",
    "        if \"abstract\" in self.sections:\n",
    "            self.abstract = self.sections[\"abstract\"]\n",
    "    \n",
    "    def get_section(self, section_name):\n",
    "        \"\"\"Get a specific section of the paper\"\"\"\n",
    "        # Case-insensitive section lookup\n",
    "        for key in self.sections:\n",
    "            if section_name.lower() in key.lower():\n",
    "                return self.sections[key]\n",
    "        return \"\"\n",
    "\n",
    "# Create a global paper context object\n",
    "paper_context = PaperContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def load_paper(file_path: str) -> str:\n",
    "    \"\"\"Load an academic paper from a PDF file path\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"Error: File {file_path} does not exist.\"\n",
    "    \n",
    "    try:\n",
    "        paper_context.load_paper(file_path)\n",
    "        return f\"Successfully loaded paper: {paper_context.title}. Paper has {len(paper_context.sections)} sections.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error loading paper: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_paper_summary() -> str:\n",
    "    \"\"\"Generate a comprehensive summary of the loaded academic paper\"\"\"\n",
    "    if not paper_context.full_text:\n",
    "        return \"No paper loaded. Please load a paper first.\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Create a comprehensive summary of this academic paper. Focus on the main contributions,\n",
    "    methodology, and key findings. The summary should be around 500 words.\n",
    "    \n",
    "    Paper Title: {paper_context.title}\n",
    "    Abstract: {paper_context.abstract}\n",
    "    \n",
    "    Paper content:\n",
    "    {paper_context.full_text[:10000]}  # Take the first 10000 chars for summary\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "@tool\n",
    "def extract_research_questions() -> List[str]:\n",
    "    \"\"\"Extract and list the research questions addressed in the paper\"\"\"\n",
    "    if not paper_context.full_text:\n",
    "        return \"No paper loaded. Please load a paper first.\"\n",
    "    \n",
    "    # Get intro and potentially methods sections for research questions\n",
    "    intro = paper_context.get_section(\"introduction\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Identify the main research questions or hypotheses addressed in this academic paper.\n",
    "    Extract them as an explicit list. If research questions are not explicitly stated,\n",
    "    infer them from the goals and objectives of the paper.\n",
    "    \n",
    "    Paper Title: {paper_context.title}\n",
    "    Abstract: {paper_context.abstract}\n",
    "    Introduction: {intro[:5000]}  # Use intro section if available\n",
    "    \n",
    "    Format your response as a JSON list of research questions, like this:\n",
    "    ```json\n",
    "    [\n",
    "      \"Research question 1\",\n",
    "      \"Research question 2\"\n",
    "    ]\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse the response to extract the JSON list\n",
    "    try:\n",
    "        # Look for JSON in triple backticks\n",
    "        import re\n",
    "        json_match = re.search(r'```(?:json)?\\n(.+?)\\n```', response.content, re.DOTALL)\n",
    "        if json_match:\n",
    "            questions = json.loads(json_match.group(1))\n",
    "        else:\n",
    "            # Try to find a JSON array directly\n",
    "            json_match = re.search(r'\\[.+\\]', response.content, re.DOTALL)\n",
    "            if json_match:\n",
    "                questions = json.loads(json_match.group(0))\n",
    "            else:\n",
    "                # If all else fails, return the raw response\n",
    "                return response.content\n",
    "        \n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        # If JSON parsing fails, return the raw text\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_key_results() -> Dict[str, Any]:\n",
    "    \"\"\"Extract the key results and findings from the paper\"\"\"\n",
    "    if not paper_context.full_text:\n",
    "        return \"No paper loaded. Please load a paper first.\"\n",
    "    \n",
    "    # Get results and discussion sections\n",
    "    results = paper_context.get_section(\"results\")\n",
    "    discussion = paper_context.get_section(\"discussion\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Extract the key results and findings from this academic paper.\n",
    "    Focus on quantitative results, main findings, and their significance.\n",
    "    \n",
    "    Paper Title: {paper_context.title}\n",
    "    Abstract: {paper_context.abstract}\n",
    "    Results section: {results[:5000]}\n",
    "    Discussion section: {discussion[:5000]}\n",
    "    \n",
    "    Format your response as a JSON object with these keys:\n",
    "    1. main_findings: A list of the main findings (each 1-2 sentences)\n",
    "    2. quantitative_results: Key numerical results and metrics\n",
    "    3. significance: The significance of these findings in the field\n",
    "    \n",
    "    Example format:\n",
    "    ```json\n",
    "    {{\n",
    "      \"main_findings\": [\n",
    "        \"Finding 1\",\n",
    "        \"Finding 2\"\n",
    "      ],\n",
    "      \"quantitative_results\": {{\n",
    "        \"metric1\": \"value1\",\n",
    "        \"metric2\": \"value2\"\n",
    "      }},\n",
    "      \"significance\": \"Description of why these results matter\"\n",
    "    }}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse the response to extract the JSON object\n",
    "    try:\n",
    "        # Look for JSON in triple backticks\n",
    "        import re\n",
    "        json_match = re.search(r'```(?:json)?\\n(.+?)\\n```', response.content, re.DOTALL)\n",
    "        if json_match:\n",
    "            results_dict = json.loads(json_match.group(1))\n",
    "        else:\n",
    "            # Try to find a JSON object directly\n",
    "            json_match = re.search(r'\\{.+\\}', response.content, re.DOTALL)\n",
    "            if json_match:\n",
    "                results_dict = json.loads(json_match.group(0))\n",
    "            else:\n",
    "                # If all else fails, return the raw response\n",
    "                return response.content\n",
    "        \n",
    "        return results_dict\n",
    "    except Exception as e:\n",
    "        # If JSON parsing fails, return the raw text\n",
    "        return response.content\n",
    "\n",
    "@tool\n",
    "def identify_research_gaps() -> List[str]:\n",
    "    \"\"\"Identify research gaps and potential future work based on the paper\"\"\"\n",
    "    if not paper_context.full_text:\n",
    "        return \"No paper loaded. Please load a paper first.\"\n",
    "    \n",
    "    # Get discussion, conclusion, and future work sections\n",
    "    discussion = paper_context.get_section(\"discussion\")\n",
    "    conclusion = paper_context.get_section(\"conclusion\")\n",
    "    future_work = paper_context.get_section(\"future work\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Identify research gaps and potential future work based on this academic paper.\n",
    "    Look for:\n",
    "    1. Limitations explicitly mentioned by the authors\n",
    "    2. Areas where the authors suggest future work\n",
    "    3. Implicit gaps that could be addressed in future research\n",
    "    \n",
    "    Paper Title: {paper_context.title}\n",
    "    Abstract: {paper_context.abstract}\n",
    "    Discussion: {discussion[:3000]}\n",
    "    Conclusion: {conclusion[:3000]}\n",
    "    Future Work: {future_work[:3000]}\n",
    "    \n",
    "    Format your response as a JSON list of research gaps, with each gap including a description\n",
    "    and whether it was explicitly mentioned by the authors or inferred.\n",
    "    \n",
    "    Example format:\n",
    "    ```json\n",
    "    [\n",
    "      {{\n",
    "        \"description\": \"Description of research gap 1\",\n",
    "        \"type\": \"explicit\" or \"implicit\",\n",
    "        \"potential_direction\": \"Potential research direction to address this gap\"\n",
    "      }},\n",
    "      {{\n",
    "        \"description\": \"Description of research gap 2\",\n",
    "        \"type\": \"explicit\" or \"implicit\",\n",
    "        \"potential_direction\": \"Potential research direction to address this gap\"\n",
    "      }}\n",
    "    ]\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse the response to extract the JSON list\n",
    "    try:\n",
    "        # Look for JSON in triple backticks\n",
    "        import re\n",
    "        json_match = re.search(r'```(?:json)?\\n(.+?)\\n```', response.content, re.DOTALL)\n",
    "        if json_match:\n",
    "            gaps = json.loads(json_match.group(1))\n",
    "        else:\n",
    "            # Try to find a JSON array directly\n",
    "            json_match = re.search(r'\\[.+\\]', response.content, re.DOTALL)\n",
    "            if json_match:\n",
    "                gaps = json.loads(json_match.group(0))\n",
    "            else:\n",
    "                # If all else fails, return the raw response\n",
    "                return response.content\n",
    "        \n",
    "        return gaps\n",
    "    except Exception as e:\n",
    "        # If JSON parsing fails, return the raw text\n",
    "        return response.content\n",
    "\n",
    "@tool\n",
    "def get_section_content(section_name: str) -> str:\n",
    "    \"\"\"Get the content of a specific section from the paper\"\"\"\n",
    "    if not paper_context.full_text:\n",
    "        return \"No paper loaded. Please load a paper first.\"\n",
    "    \n",
    "    content = paper_context.get_section(section_name)\n",
    "    if not content:\n",
    "        return f\"Section '{section_name}' not found in the paper.\"\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Combine all tools\n",
    "tools = [load_paper, get_paper_summary, extract_research_questions, \n",
    "         extract_key_results, identify_research_gaps, get_section_content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    messages: List[Union[HumanMessage, AIMessage, SystemMessage, FunctionMessage]]\n",
    "    paper_loaded: bool\n",
    "    paper_title: str\n",
    "    paper_summary: Optional[str]\n",
    "    research_questions: Optional[List[str]]\n",
    "    key_results: Optional[Dict[str, Any]]\n",
    "    research_gaps: Optional[List[Dict[str, str]]]\n",
    "    final_report: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define system prompt for the academic paper analysis agent\n",
    "system_prompt = \"\"\"You are an academic research assistant specializing in analyzing scientific papers.\n",
    "Your purpose is to help researchers understand papers by extracting key information like:\n",
    "- Summarizing the paper's content\n",
    "- Identifying the main research questions\n",
    "- Extracting key results and findings\n",
    "- Identifying research gaps and future directions\n",
    "\n",
    "You have access to tools that can:\n",
    "1. Load academic papers from PDF files\n",
    "2. Generate comprehensive summaries\n",
    "3. Extract research questions\n",
    "4. Extract key results and findings\n",
    "5. Identify research gaps and potential future work\n",
    "6. Get content from specific sections of the paper\n",
    "\n",
    "Think step-by-step about what tools you need to use to fulfill the user's request.\n",
    "Always start by loading the paper if the user has provided a file path.\n",
    "Use academic language and be precise in your responses.\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# Create the agent using LangGraph\n",
    "paper_agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools, \n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Default state\n",
    "def create_initial_state(user_input: str) -> AgentState:\n",
    "    return {\n",
    "        \"user_input\": user_input,\n",
    "        \"messages\": [HumanMessage(content=user_input)],\n",
    "        \"paper_loaded\": False,\n",
    "        \"paper_title\": \"\",\n",
    "        \"paper_summary\": None,\n",
    "        \"research_questions\": None,\n",
    "        \"key_results\": None,\n",
    "        \"research_gaps\": None,\n",
    "        \"final_report\": None\n",
    "    }\n",
    "\n",
    "# Function to process the agent's action\n",
    "def run_agent(state: AgentState) -> dict:\n",
    "    \"\"\"Run the agent on the current state and update with results\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = paper_agent.invoke({\"messages\": messages})\n",
    "    \n",
    "    updated_state = state.copy()\n",
    "    updated_state[\"messages\"] = messages + [response]\n",
    "    \n",
    "    # Check if the most recent message is a tool response\n",
    "    if isinstance(response, ToolMessage):\n",
    "        # Store tool outputs in appropriate state fields\n",
    "        if response.name == \"get_paper_summary\":\n",
    "            updated_state[\"paper_summary\"] = response.content\n",
    "        elif response.name == \"extract_research_questions\":\n",
    "            try:\n",
    "                # May need to parse JSON if the content is a string representation of JSON\n",
    "                updated_state[\"research_questions\"] = response.content\n",
    "            except:\n",
    "                pass\n",
    "        elif response.name == \"extract_key_results\":\n",
    "            try:\n",
    "                updated_state[\"key_results\"] = response.content\n",
    "            except:\n",
    "                pass\n",
    "        elif response.name == \"identify_research_gaps\":\n",
    "            try:\n",
    "                updated_state[\"research_gaps\"] = response.content\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Update paper information if loaded\n",
    "    if not state[\"paper_loaded\"] and paper_context.full_text:\n",
    "        updated_state[\"paper_loaded\"] = True\n",
    "        updated_state[\"paper_title\"] = paper_context.title\n",
    "    \n",
    "    return updated_state\n",
    "\n",
    "# Build the workflow graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    lambda state: END if not any(hasattr(msg, \"tool_calls\") and msg.tool_calls \n",
    "                                for msg in state[\"messages\"][-1:]) else \"agent\"\n",
    ")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_paper_agent(query: str):\n",
    "    \"\"\"Run the paper agent with a custom query and return the final response\"\"\"\n",
    "    initial_state = create_initial_state(query)\n",
    "    response = app.invoke(initial_state)\n",
    "    #debug\n",
    "    print(\"RAW RESPONSE:\")\n",
    "    pprint(response)\n",
    "    \n",
    "    # Extract the final message content\n",
    "    final_message = None\n",
    "    thinking_contents = []\n",
    "    \n",
    "    # Parse the nested message structure\n",
    "    for message_item in response[\"messages\"]:\n",
    "        # Check if this item contains a nested messages list\n",
    "        if isinstance(message_item, dict) and \"messages\" in message_item:\n",
    "            for sub_message in message_item[\"messages\"]:\n",
    "                # Look for AIMessages with content as a list\n",
    "                if hasattr(sub_message, \"content\") and isinstance(sub_message.content, list):\n",
    "                    # Extract thinking content\n",
    "                    for content_item in sub_message.content:\n",
    "                        if isinstance(content_item, dict) and content_item.get('type') == 'text' and '<thinking>' in content_item.get('text', ''):\n",
    "                            thinking_contents.append(content_item.get('text'))\n",
    "                \n",
    "                # If this is a string content, it might be the final response\n",
    "                if hasattr(sub_message, \"content\") and isinstance(sub_message.content, str):\n",
    "                    final_message = sub_message.content\n",
    "        \n",
    "        # Also check direct messages\n",
    "        if hasattr(message_item, \"content\") and isinstance(message_item.content, list):\n",
    "            for content_item in message_item.content:\n",
    "                if isinstance(content_item, dict) and content_item.get('type') == 'text' and '<thinking>' in content_item.get('text', ''):\n",
    "                    thinking_contents.append(content_item.get('text'))\n",
    "        \n",
    "        if hasattr(message_item, \"content\") and isinstance(message_item.content, str):\n",
    "            final_message = message_item.content\n",
    "    return {\n",
    "        \"paper_loaded\": response[\"paper_loaded\"],\n",
    "        \"paper_title\": response[\"paper_title\"],\n",
    "        \"final_response\": final_message,\n",
    "        \"thinking_contents\": thinking_contents\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Paper Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is our paper, update the path if you are using a different directory.\n",
    "paper_path = \"data/a-shopping-agent-for-addressing-subjective-product-needs.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lets test our tools with different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW RESPONSE:\n",
      "{'final_report': None,\n",
      " 'key_results': None,\n",
      " 'messages': [HumanMessage(content='what research questions it addresses for the paper? data/a-shopping-agent-for-addressing-subjective-product-needs.pdf', additional_kwargs={}, response_metadata={}, id='23c97460-2bde-42f4-800b-c38697a2e4cc'),\n",
      "              {'messages': [HumanMessage(content='what research questions it addresses for the paper? data/a-shopping-agent-for-addressing-subjective-product-needs.pdf', additional_kwargs={}, response_metadata={}, id='23c97460-2bde-42f4-800b-c38697a2e4cc'),\n",
      "                            AIMessage(content=[{'type': 'text', 'text': '<thinking>\\nTo extract the research questions from the provided paper, I need to first load the paper and then use the tool designed to extract research questions.\\n</thinking>\\n'}, {'type': 'tool_use', 'name': 'load_paper', 'input': {'file_path': 'data/a-shopping-agent-for-addressing-subjective-product-needs.pdf'}, 'id': 'tooluse_wuAdHutXQkWHmHqKpfvogw'}, {'type': 'tool_use', 'name': 'extract_research_questions', 'input': {}, 'id': 'tooluse_tzp1raPASryWMsmnEUbzWA'}], additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': '01b3b823-dc4f-46c7-8f50-5cd2cc71e2b7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 21 Apr 2025 06:31:11 GMT', 'content-type': 'application/json', 'content-length': '639', 'connection': 'keep-alive', 'x-amzn-requestid': '01b3b823-dc4f-46c7-8f50-5cd2cc71e2b7'}, 'RetryAttempts': 4}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': [2965]}, 'model_name': 'us.amazon.nova-premier-v1:0'}, id='run-1f9579bd-e192-463b-9b95-3a8718001e9d-0', tool_calls=[{'name': 'load_paper', 'args': {'file_path': 'data/a-shopping-agent-for-addressing-subjective-product-needs.pdf'}, 'id': 'tooluse_wuAdHutXQkWHmHqKpfvogw', 'type': 'tool_call'}, {'name': 'extract_research_questions', 'args': {}, 'id': 'tooluse_tzp1raPASryWMsmnEUbzWA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 791, 'output_tokens': 217, 'total_tokens': 1008, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}),\n",
      "                            ToolMessage(content='Successfully loaded paper: A Shopping Agent for Addressing Subjective Product Needs. Paper has 4 sections.', name='load_paper', id='20aaab45-8ad0-4f67-8d54-1479535ffa5f', tool_call_id='tooluse_wuAdHutXQkWHmHqKpfvogw'),\n",
      "                            ToolMessage(content='No paper loaded. Please load a paper first.', name='extract_research_questions', id='45dbcf1a-2da5-41ba-9eac-a60cc6c0f7a2', tool_call_id='tooluse_tzp1raPASryWMsmnEUbzWA'),\n",
      "                            AIMessage(content=[{'type': 'text', 'text': '<thinking>\\nIt seems that the tool to extract research questions did not recognize the loaded paper. I will try loading the paper again and then extracting the research questions to ensure the process is completed correctly.\\n</thinking> '}, {'type': 'tool_use', 'name': 'load_paper', 'input': {'file_path': 'data/a-shopping-agent-for-addressing-subjective-product-needs.pdf'}, 'id': 'tooluse_2TtStzCNSyalMh4Cip_Rpg'}, {'type': 'tool_use', 'name': 'extract_research_questions', 'input': {}, 'id': 'tooluse_ibuAPYSGTR67biJm5XBZ4w'}], additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'b98c91ab-d0df-4e2b-9e18-06c0c080fd0e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 21 Apr 2025 06:31:14 GMT', 'content-type': 'application/json', 'content-length': '699', 'connection': 'keep-alive', 'x-amzn-requestid': 'b98c91ab-d0df-4e2b-9e18-06c0c080fd0e'}, 'RetryAttempts': 0}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': [3234]}, 'model_name': 'us.amazon.nova-premier-v1:0'}, id='run-c83f917b-7364-406f-a34a-4349d4a9b534-0', tool_calls=[{'name': 'load_paper', 'args': {'file_path': 'data/a-shopping-agent-for-addressing-subjective-product-needs.pdf'}, 'id': 'tooluse_2TtStzCNSyalMh4Cip_Rpg', 'type': 'tool_call'}, {'name': 'extract_research_questions', 'args': {}, 'id': 'tooluse_ibuAPYSGTR67biJm5XBZ4w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 947, 'output_tokens': 270, 'total_tokens': 1217, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}),\n",
      "                            ToolMessage(content='Successfully loaded paper: A Shopping Agent for Addressing Subjective Product Needs. Paper has 4 sections.', name='load_paper', id='55651006-f359-4a0f-99a0-7d37b77f9f1e', tool_call_id='tooluse_2TtStzCNSyalMh4Cip_Rpg'),\n",
      "                            ToolMessage(content='[\"How can an autonomous shopping agent effectively address subjective product needs in e-commerce scenarios?\", \"What are the key facets of subjective product needs that the agent should consider?\", \"How can the agent reduce user effort and cognitive load during the product exploration and selection process?\", \"What computational, conversational, browsing, and generative actions can the agent perform to streamline the shopping experience?\", \"How can customer reviews be utilized to enhance the agent\\'s ability to meet subjective needs?\", \"What is the impact of the agent on the time saved and decision fatigue for users?\"]', name='extract_research_questions', id='d7b7114e-f2ad-4f6e-9bfe-52a31f98f60e', tool_call_id='tooluse_ibuAPYSGTR67biJm5XBZ4w'),\n",
      "                            AIMessage(content=\"The paper addresses the following research questions:\\n\\n1. How can an autonomous shopping agent effectively address subjective product needs in e-commerce scenarios?\\n2. What are the key facets of subjective product needs that the agent should consider?\\n3. How can the agent reduce user effort and cognitive load during the product exploration and selection process?\\n4. What computational, conversational, browsing, and generative actions can the agent perform to streamline the shopping experience?\\n5. How can customer reviews be utilized to enhance the agent's ability to meet subjective needs?\\n6. What is the impact of the agent on the time saved and decision fatigue for users?\", additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'ed9f7690-16e3-42a5-b8c7-6a5904dc75db', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 21 Apr 2025 06:31:27 GMT', 'content-type': 'application/json', 'content-length': '873', 'connection': 'keep-alive', 'x-amzn-requestid': 'ed9f7690-16e3-42a5-b8c7-6a5904dc75db'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [5933]}, 'model_name': 'us.amazon.nova-premier-v1:0'}, id='run-ea3dd1c0-b9c4-42ff-97c7-ce3a9cbdc4c9-0', usage_metadata={'input_tokens': 1221, 'output_tokens': 132, 'total_tokens': 1353, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})]}],\n",
      " 'paper_loaded': True,\n",
      " 'paper_summary': None,\n",
      " 'paper_title': 'A Shopping Agent for Addressing Subjective Product Needs',\n",
      " 'research_gaps': None,\n",
      " 'research_questions': None,\n",
      " 'user_input': 'what research questions it addresses for the paper? '\n",
      "               'data/a-shopping-agent-for-addressing-subjective-product-needs.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Just extract research questions\n",
    "query1 = f\"what research questions it addresses for the paper? {paper_path}\"\n",
    "\n",
    "# Our agent will first return the raw response\n",
    "agent_response = run_paper_agent(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print the key points of the response just to make it easy to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'final_response': 'The paper addresses the following research questions:\\n'\n",
      "                   '\\n'\n",
      "                   '1. How can an autonomous shopping agent effectively '\n",
      "                   'address subjective product needs in e-commerce scenarios?\\n'\n",
      "                   '2. What are the key facets of subjective product needs '\n",
      "                   'that the agent should consider?\\n'\n",
      "                   '3. How can the agent reduce user effort and cognitive load '\n",
      "                   'during the product exploration and selection process?\\n'\n",
      "                   '4. What computational, conversational, browsing, and '\n",
      "                   'generative actions can the agent perform to streamline the '\n",
      "                   'shopping experience?\\n'\n",
      "                   '5. How can customer reviews be utilized to enhance the '\n",
      "                   \"agent's ability to meet subjective needs?\\n\"\n",
      "                   '6. What is the impact of the agent on the time saved and '\n",
      "                   'decision fatigue for users?',\n",
      " 'paper_loaded': True,\n",
      " 'paper_title': 'A Shopping Agent for Addressing Subjective Product Needs',\n",
      " 'thinking_contents': ['<thinking>\\n'\n",
      "                       'To extract the research questions from the provided '\n",
      "                       'paper, I need to first load the paper and then use the '\n",
      "                       'tool designed to extract research questions.\\n'\n",
      "                       '</thinking>\\n',\n",
      "                       '<thinking>\\n'\n",
      "                       'It seems that the tool to extract research questions '\n",
      "                       'did not recognize the loaded paper. I will try loading '\n",
      "                       'the paper again and then extracting the research '\n",
      "                       'questions to ensure the process is completed '\n",
      "                       'correctly.\\n'\n",
      "                       '</thinking> ']}\n"
     ]
    }
   ],
   "source": [
    "# Here are the key points from the response.\n",
    "\n",
    "pprint(agent_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the thinking block:\n",
    "`To extract the research questions from the paper, I need to first load the paper and then use the tool designed to extract research questions.`\n",
    "\n",
    "As you can see above, our agent invoked our model 3 times:\n",
    "1. Executed `load_paper` tool to load the paper.\n",
    "2. Executed `extract_research_questions` tool to extract research questions.\n",
    "3. Generated the final response.\n",
    "\n",
    "Time to send another couple of queries, we want to check the key results/findings and also a comprehensive report on the paper including summary, research questions, key findings and research gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW RESPONSE:\n",
      "{'final_report': None,\n",
      " 'key_results': None,\n",
      " 'messages': [HumanMessage(content='What are the key results and findings? data/a-shopping-agent-for-addressing-subjective-product-needs.pdf', additional_kwargs={}, response_metadata={}, id='6f39f51c-9c42-4350-a007-eabf8399db04'),\n",
      "              {'messages': [HumanMessage(content='What are the key results and findings? data/a-shopping-agent-for-addressing-subjective-product-needs.pdf', additional_kwargs={}, response_metadata={}, id='6f39f51c-9c42-4350-a007-eabf8399db04'),\n",
      "                            AIMessage(content=[{'type': 'text', 'text': '<thinking>\\nTo extract the key results and findings from the paper, I need to first load the paper and then use the tool designed to extract key results and findings.\\n</thinking>\\n'}, {'type': 'tool_use', 'name': 'load_paper', 'input': {'file_path': 'data/a-shopping-agent-for-addressing-subjective-product-needs.pdf'}, 'id': 'tooluse_qn87slJBTwm-sMehYHtf8g'}, {'type': 'tool_use', 'name': 'extract_key_results', 'input': {}, 'id': 'tooluse_a7UJ2Ge4QZ-X3rW_6f6p_Q'}], additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'a06e4451-2641-4e50-b986-ac9822046fa2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 21 Apr 2025 06:33:01 GMT', 'content-type': 'application/json', 'content-length': '635', 'connection': 'keep-alive', 'x-amzn-requestid': 'a06e4451-2641-4e50-b986-ac9822046fa2'}, 'RetryAttempts': 0}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': [3600]}, 'model_name': 'us.amazon.nova-premier-v1:0'}, id='run-22dfa8d6-b3f4-4026-b5ac-c391f6570810-0', tool_calls=[{'name': 'load_paper', 'args': {'file_path': 'data/a-shopping-agent-for-addressing-subjective-product-needs.pdf'}, 'id': 'tooluse_qn87slJBTwm-sMehYHtf8g', 'type': 'tool_call'}, {'name': 'extract_key_results', 'args': {}, 'id': 'tooluse_a7UJ2Ge4QZ-X3rW_6f6p_Q', 'type': 'tool_call'}], usage_metadata={'input_tokens': 790, 'output_tokens': 220, 'total_tokens': 1010, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}),\n",
      "                            ToolMessage(content='Successfully loaded paper: A Shopping Agent for Addressing Subjective Product Needs. Paper has 4 sections.', name='load_paper', id='f9409eba-c7d5-4e30-9737-e25b35d3241f', tool_call_id='tooluse_qn87slJBTwm-sMehYHtf8g'),\n",
      "                            ToolMessage(content='{\"main_findings\": [\"The proposed shopping agent effectively reduces user effort by autonomously executing various actions such as vagueness detection, subjective product needs extraction, and browsing actions.\", \"The agent can save users significant time by processing and ranking numerous reviews, thereby reducing decision fatigue and promoting the exploration of a wider range of products.\"], \"quantitative_results\": {\"time_saved\": \"2.67 hours\", \"number_of_reviews_processed\": \"400\", \"average_words_per_review\": \"100\", \"user_reading_speed\": \"250 words per minute\"}, \"significance\": \"The findings demonstrate the potential of autonomous agents to handle subjective queries in e-commerce, enhancing personalization, product exploration, and selection in a user-centric manner. This approach not only reduces the cognitive burden on users but also facilitates the exploration of a wider range of products, significantly improving the shopping experience.\"}', name='extract_key_results', id='62f50025-1dce-4881-ab50-fed44ff7b89b', tool_call_id='tooluse_a7UJ2Ge4QZ-X3rW_6f6p_Q'),\n",
      "                            AIMessage(content='The key results and findings from the paper \"A Shopping Agent for Addressing Subjective Product Needs\" are as follows:\\n\\n### Main Findings:\\n1. **Effectiveness in Reducing User Effort**: The proposed shopping agent effectively reduces user effort by autonomously executing various actions such as vagueness detection, subjective product needs extraction, and browsing actions.\\n2. **Time Savings**: The agent can save users significant time by processing and ranking numerous reviews, thereby reducing decision fatigue and promoting the exploration of a wider range of products.\\n\\n### Quantitative Results:\\n- **Time Saved**: The agent can save users approximately 2.67 hours.\\n- **Number of Reviews Processed**: The agent processes around 400 reviews.\\n- **Average Words per Review**: Each review contains an average of 100 words.\\n- **User Reading Speed**: The average user reading speed is 250 words per minute.\\n\\n### Significance:\\nThe findings demonstrate the potential of autonomous agents to handle subjective queries in e-commerce, enhancing personalization, product exploration, and selection in a user-centric manner. This approach not only reduces the cognitive burden on users but also facilitates the exploration of a wider range of products, significantly improving the shopping experience.\\n\\nThese results highlight the effectiveness and efficiency of the proposed shopping agent in addressing subjective product needs, making it a valuable tool for enhancing the online shopping experience.', additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'e43b5497-2b5c-4686-8431-4e540ad343cf', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 21 Apr 2025 06:33:24 GMT', 'content-type': 'application/json', 'content-length': '1700', 'connection': 'keep-alive', 'x-amzn-requestid': 'e43b5497-2b5c-4686-8431-4e540ad343cf'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [11878]}, 'model_name': 'us.amazon.nova-premier-v1:0'}, id='run-1c059cb2-089c-4c78-b4a7-056cd5ea1f1f-0', usage_metadata={'input_tokens': 1133, 'output_tokens': 292, 'total_tokens': 1425, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})]}],\n",
      " 'paper_loaded': True,\n",
      " 'paper_summary': None,\n",
      " 'paper_title': 'A Shopping Agent for Addressing Subjective Product Needs',\n",
      " 'research_gaps': None,\n",
      " 'research_questions': None,\n",
      " 'user_input': 'What are the key results and findings? '\n",
      "               'data/a-shopping-agent-for-addressing-subjective-product-needs.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Extract key results findings\n",
    "query2 = f\"What are the key results and findings? {paper_path}\"\n",
    "\n",
    "# Our agent is starting with clean state.\n",
    "agent_response = run_paper_agent(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'final_response': 'The key results and findings from the paper \"A Shopping '\n",
      "                   'Agent for Addressing Subjective Product Needs\" are as '\n",
      "                   'follows:\\n'\n",
      "                   '\\n'\n",
      "                   '### Main Findings:\\n'\n",
      "                   '1. **Effectiveness in Reducing User Effort**: The proposed '\n",
      "                   'shopping agent effectively reduces user effort by '\n",
      "                   'autonomously executing various actions such as vagueness '\n",
      "                   'detection, subjective product needs extraction, and '\n",
      "                   'browsing actions.\\n'\n",
      "                   '2. **Time Savings**: The agent can save users significant '\n",
      "                   'time by processing and ranking numerous reviews, thereby '\n",
      "                   'reducing decision fatigue and promoting the exploration of '\n",
      "                   'a wider range of products.\\n'\n",
      "                   '\\n'\n",
      "                   '### Quantitative Results:\\n'\n",
      "                   '- **Time Saved**: The agent can save users approximately '\n",
      "                   '2.67 hours.\\n'\n",
      "                   '- **Number of Reviews Processed**: The agent processes '\n",
      "                   'around 400 reviews.\\n'\n",
      "                   '- **Average Words per Review**: Each review contains an '\n",
      "                   'average of 100 words.\\n'\n",
      "                   '- **User Reading Speed**: The average user reading speed '\n",
      "                   'is 250 words per minute.\\n'\n",
      "                   '\\n'\n",
      "                   '### Significance:\\n'\n",
      "                   'The findings demonstrate the potential of autonomous '\n",
      "                   'agents to handle subjective queries in e-commerce, '\n",
      "                   'enhancing personalization, product exploration, and '\n",
      "                   'selection in a user-centric manner. This approach not only '\n",
      "                   'reduces the cognitive burden on users but also facilitates '\n",
      "                   'the exploration of a wider range of products, '\n",
      "                   'significantly improving the shopping experience.\\n'\n",
      "                   '\\n'\n",
      "                   'These results highlight the effectiveness and efficiency '\n",
      "                   'of the proposed shopping agent in addressing subjective '\n",
      "                   'product needs, making it a valuable tool for enhancing the '\n",
      "                   'online shopping experience.',\n",
      " 'paper_loaded': True,\n",
      " 'paper_title': 'A Shopping Agent for Addressing Subjective Product Needs',\n",
      " 'thinking_contents': ['<thinking>\\n'\n",
      "                       'To extract the key results and findings from the '\n",
      "                       'paper, I need to first load the paper and then use the '\n",
      "                       'tool designed to extract key results and findings.\\n'\n",
      "                       '</thinking>\\n']}\n"
     ]
    }
   ],
   "source": [
    "# Here are the key points from the response.\n",
    "pprint(agent_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the thinking block again:\n",
    "`To extract the key results and findings from the paper, I need to first load the paper and then use the tool designed to extract key results and findings.`\n",
    "\n",
    "This time the agent executed the following steps:\n",
    "1. `load_paper` tool again (remember we are not keeping history so we can see the whole workflow).\n",
    "2. `main_findings` tool.\n",
    "3. Generated the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example 3: Generatea comprehensive report, including a summary, research questions, key findings and research gaps.\n",
    "query3 = f\"Please generate a comprehensive report on the paper, including a summary, research questions, key findings, and research gaps. Here is the paper: {paper_path}\"\n",
    "\n",
    "agent_response = run_paper_agent(query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here are the key points from the response.\n",
    "pprint(agent_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a few other tools in the output above, the thinking block was more complex as well. Looking at the thinking block we can see the agent planing:\n",
    "```\n",
    "To generate a comprehensive report on the paper, I need to follow these steps:\n",
    "1. Load the paper from the provided file path.\n",
    "2. Generate a comprehensive summary of the paper.\n",
    "3. Extract the research questions addressed in the paper.\n",
    "4. Extract the key results and findings from the paper.\\n5. Identify research gaps and potential future work based on the paper.\\n\\nI will start by loading the paper and then proceed with the other steps.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
