{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3b87af2-6de5-4055-b410-237fff246f98",
   "metadata": {},
   "source": [
    "# Long Context Instruction Following and Understanding with Amazon Nova Premier\n",
    "\n",
    "Large Language Models (LLMs) with extended context lengths, particularly up to 1 million tokens, represent a significant advancement in natural language processing, enabling the handling of vast amounts of information in a single input.\n",
    "\n",
    "This large context enables and simplifies cases requiring extended context lengths.\n",
    "\n",
    "This notebook demonstrates how to use long context with up to 1M context window for Amazon Nova Premier, by analysing Amazon Financial Reports over multiple years in one request.\n",
    "\n",
    "## When is long context useful?\n",
    "\n",
    "* **Document Processing and Summarization:** LLMs can analyze and summarize extensive documents, such as legal contracts, regulatory texts, or academic papers. For instance, law firms can use these models to reduce manual review time, minimizing the risk of overlooking critical information, while banks can process lengthy regulatory documents for compliance\n",
    "\n",
    "* **Enhanced Conversation Memory:** In customer service and chat applications, a 1 million token context enables LLMs to retain extensive conversation histories, improving coherence and relevance. This is particularly beneficial for handling long chat threads or email exchanges, enhancing user satisfaction by ensuring the model remembers previous interactions. For example, chatbots can maintain context over hours of dialogue, a significant improvement over models with shorter memory spans.\n",
    "\n",
    "* **Complex Task Handling:** Tasks requiring long-term planning or integration of multiple data sources benefit from extended contexts. This includes academic research, where LLMs can summarize large volumes of literature and generate hypotheses, and financial analysis, where they can process complex datasets for decision-making. Additionally, LLMs can manage large codebases, supporting developers in debugging or generating code for projects with over 30,000 lines.\n",
    "\n",
    "## Implementation Example\n",
    "\n",
    "This notebook walks through a simple financial document analyser:\n",
    "\n",
    "* Download financial documents over multiple years\n",
    "* Stack the PDF documents as part of the prompt.\n",
    "* Request analysis over time for these reports to get insights on trends.\n",
    "* Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fce233",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"boto3>=1.28.57\" \"awscli>=1.29.57\" \"botocore>=1.31.57\" \"requests>=2.32.3\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e297b-94c9-4d89-9401-e02bdbf8a604",
   "metadata": {},
   "source": [
    "## Use case: Multi year financial report analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491654d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from botocore.config import Config\n",
    "import requests\n",
    "\n",
    "PREMIER_MODEL_ID = \"us.amazon.nova-premier-v1:0\"\n",
    "PRO_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "LITE_MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "MICRO_MODEL_ID = \"us.amazon.nova-micro-v1:0\"\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,     # 5 minutes\n",
    "        retries={'max_attempts': 1}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": []\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f79ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions =\"\"\"\n",
    "## Task:\n",
    "Analyze Amazon's financial reports across multiple years to identify significant performance trends, segment growth patterns, and strategic shifts.\n",
    "\n",
    "## Context information:\n",
    "- You have access to Amazon's annual financial reports (10-K) for multiple fiscal years in PDF format\n",
    "- These reports contain comprehensive financial data including income statements, balance sheets, cash flow statements, and management discussions\n",
    "- The analysis should focus on year-over-year comparisons to identify meaningful trends\n",
    "- Amazon operates multiple business segments including North America retail, International retail, Amazon Web Services (AWS), advertising, and subscription services\n",
    "\n",
    "## Model Instructions:\n",
    "- FIRST extract key financial metrics from each year's reports\n",
    "- THEN organize data chronologically to identify meaningful trends\n",
    "- DO compare segment performance across the five-year period\n",
    "- DO identify significant strategic shifts or investments mentioned in management discussions\n",
    "- DO NOT make speculative predictions beyond what is supported by the data\n",
    "- ALWAYS note any changes in accounting practices or reporting methodologies that might affect year-over-year comparisons\n",
    "\n",
    "## Response style and format requirements:\n",
    "- Respond in markdown\n",
    "- Structure the analysis with clear headings and subheadings\n",
    "- Present key financial metrics in tabular format showing all five years side-by-side\n",
    "- Include percentage changes year-over-year for all major metrics\n",
    "- Create a section dedicated to visualizing the most significant trends (with descriptions of what would be shown in charts)\n",
    "- Limit the executive summary to 250 words maximum\n",
    "- Format segment analysis as separate sections with consistent metrics across all segments\n",
    "- MUST include a \"Key Insights\" bullet-pointed list at the end of each major section\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"text\": instructions\n",
    "}\n",
    "\n",
    "messages[0]['content'].append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e767655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "# reports from: https://ir.aboutamazon.com/annual-reports-proxies-and-shareholder-letters/default.aspx\n",
    "reports = [\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2025/ar/Amazon-2024-Annual-Report.pdf\",\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Annual-Report.pdf\",\n",
    "    # \"https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/Amazon-2022-Annual-Report.pdf\",\n",
    "    # \"https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/Amazon-2021-Annual-Report.pdf\",\n",
    "]\n",
    "\n",
    "for report_url in reports:\n",
    "    file_name = report_url.split(\"/\")[-1].split('.')[0]\n",
    "    # download_file(report_url, file_name)\n",
    "    print(file_name)\n",
    "\n",
    "    response = requests.get(report_url)\n",
    "    response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        \n",
    "    # Create a file-like object from the response content\n",
    "    pdf_file_object = response.content\n",
    "    obj = \\\n",
    "    { \n",
    "        \"document\": {\n",
    "            \"name\": file_name,\n",
    "            \"format\": \"pdf\",\n",
    "            \"source\": {\n",
    "                \"bytes\": pdf_file_object\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    messages[0]['content'].append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9723d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your system prompt(s).\n",
    "system_prompt = [\n",
    "    {\n",
    "        \"text\": \"You are an expert analyst that can critically analyse financial reports.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"maxTokens\": 10000, \"topP\": 0.1, \"temperature\": 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b020a-cf10-44fb-b9e4-7be25b5e84c1",
   "metadata": {},
   "source": [
    "## Converse API\n",
    "\n",
    "You can use the Amazon Bedrock [Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) to create conversational applications that send and receive messages to and from an Amazon Bedrock model. To use the Converse API, you use the Converse or ConverseStream (for streaming responses) operations to send messages to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6736574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON, Markdown\n",
    "\n",
    "model_id=PREMIER_MODEL_ID\n",
    "\n",
    "model_response = client.converse(\n",
    "    modelId=model_id, system=system_prompt, messages=messages, inferenceConfig=inf_params,\n",
    ")\n",
    "\n",
    "content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "JSON(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0620e9-ae12-4098-b57a-762f94196dae",
   "metadata": {},
   "source": [
    "Render response text as markdown in Jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa9634-70a1-4516-89c2-efb4718cc92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON, Markdown\n",
    "\n",
    "Markdown(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b880d60-3a41-4dfc-88de-f0114186e827",
   "metadata": {},
   "source": [
    "## Converse Stream API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac9071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_response = client.converse_stream(\n",
    "    modelId=model_id, messages=messages, system=system_prompt, inferenceConfig=inf_params\n",
    ")\n",
    "\n",
    "stream = model_response.get('stream')\n",
    "if stream:\n",
    "    for event in stream:\n",
    "\n",
    "        if 'messageStart' in event:\n",
    "            print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "        if 'contentBlockDelta' in event:\n",
    "            print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "        if 'messageStop' in event:\n",
    "            print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "        if 'metadata' in event:\n",
    "            metadata = event['metadata']\n",
    "            if 'usage' in metadata:\n",
    "                print(\"\\nToken usage\")\n",
    "                print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                print(\n",
    "                    f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "            if 'metrics' in event['metadata']:\n",
    "                print(\n",
    "                    f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423052e-d2bf-4576-bc5e-5c264b1f7d74",
   "metadata": {},
   "source": [
    "## Asking questions about the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadec2b0-7773-40fd-8175-051432315730",
   "metadata": {},
   "source": [
    "Let's take a look at Amazon Nova's capability to answer questions based on the information hidden somewhere in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a2d0f-7f7d-4ddb-9dbd-6f9e3cda17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"text\": \"How many employees worked at Amazon at the end of 2023?\"\n",
    "}\n",
    "\n",
    "messages[0]['content'].append(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67116d6-1746-4130-ac75-9623e14a15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = client.converse(\n",
    "    modelId=model_id, system=system_prompt, messages=messages, inferenceConfig=inf_params,\n",
    ")\n",
    "\n",
    "content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "print(\"\\n[Full Response]\")\n",
    "JSON(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc719193-677d-487a-a9d9-d10e9ad7f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(content_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae3bf18c-e87e-47a5-91a8-c781fd785e63",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We showed how we can download and easily analyse multiple years of financial reports in PDF using a simple prompt and Amazon Nova Premier with up to 1M context window. This enables new cases and simplifies many existing cases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
