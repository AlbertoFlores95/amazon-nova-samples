{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb6374e",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Batch inference with Amazon Nova FMs to summarize call transcripts\n",
    "\n",
    "With [Amazon Bedrock Batch Inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html), you can provide a set of prompts as a single input file and receive responses as a single output file, allowing you to run asynchronous large-scale inference. The responses are processed and stored in your Amazon S3 bucket so you can access them at a later time. Amazon Bedrock offers support for Amazon Nova FMs for batch inference at a 50% lower price compared to on-demand inference pricing. Please refer to model list [here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html).\n",
    "\n",
    "## Introduction\n",
    "Call center transcript summarization is a crucial task for businesses seeking to extract valuable insights from customer interactions. As the volume of call data grows, traditional analysis methods struggle to keep pace, creating a demand for scalable solutions. Batch Inference for Amazon Bedrock provides a powerful tool to address this challenge by enabling organizations to process large volumes of data efficiently.\n",
    "\n",
    "This notebook demonstrates how to leverage batch inference for summarizing call center transcripts at scale by processing substantial volumes of text transcripts in batches. Though we are using example of call transcript summarization here, you can really apply this to any other use case that does not need a real time output.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure that you have the following prerequisites in place:\n",
    "1. You must have permissions to invoke `create_model_invocation_job` API. Refer to the documentation to learn about [required permissions for batch inference job](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-prereq.html#batch-inference-permissions).\n",
    "2. Permission to read and write data on Amazon S3 bucket.\n",
    "3. Bedrock Batch Inference requires a service role so that it can access and write to S3 on your behalf. You can create the service role manually [see here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html) or use the AWS Console workflow which can create one for you [here](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/batch-inference/create). We also provide a quick way to create a service role in the code below.\n",
    "4. Call transcript dataset:\n",
    "    * This notebook was built using synthetic call transcripts in `.txt` files. If you want to try it with your own dataset, upload your call transcripts to an Amazon S3 bucket in `.txt` format. Each text file in the S3 bucket should contain only one call transcript.\n",
    "5. Ensure that you are in a AWS region that is supported for Batch Inference. Refer [here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html) for documentation.\n",
    "6. The default maximum size of a single file (in GB) submitted for batch inference for Nova models is 1 GB. However, you can request an increase [here](https://us-east-1.console.aws.amazon.com/servicequotas/home/services/bedrock/quotas/L-68FC8D47) as needed.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc94b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet \"boto3>=1.35.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d35565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel if needed if you installed fresh from previous cell\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bd0f2-0a15-46cd-a840-3fadc449f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "# from botocore.config import Config\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "import re\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(region_name=\"us-east-1\")\n",
    "\n",
    "# Bedrock client for batch inference job\n",
    "bedrock = session.client(service_name=\"bedrock\")\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = session.client('s3')\n",
    "region = session.region_name\n",
    "\n",
    "# Set the S3 bucket name and prefix for the text files\n",
    "bucket_name = 'batchnovabucket' # Update your bucket name here\n",
    "raw_data_prefix = 'novabatchinf/raw'\n",
    "output_prefix = 'output'\n",
    " \n",
    "# Batch API parameters:\n",
    "jobName = 'novabatchinf-job' + str(int(datetime.now().timestamp()))\n",
    "job_s3_prefix = 'novabatchinf/' + jobName\n",
    "modelId = \"amazon.nova-pro-v1:0\"  # or use other model from here: https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html#w78aab7c19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5ba61",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before initiating a batch inference job for call center transcript summarization, it's crucial to properly format and upload your data to an S3 bucket. Learn more about data format requirments in our [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-data.html).\n",
    "\n",
    "### Formatting Input Data\n",
    "\n",
    "The input data should be in JSONL format, with each line representing a single transcript for summarization. Each line in your JSONL file should follow this structure:\n",
    "\n",
    "```json\n",
    "{\"recordId\": \"11 character alphanumeric string\", \"modelInput\": {JSON body}}\n",
    "```\n",
    "\n",
    "Here, `recordId` is an 11-character alphanumeric string, working as a unique identifier for each entry. If you omit this field, the batch inference job will automatically add it in the output.\n",
    "\n",
    "The format of the `modelInput` JSON object should match the body field for the model you are using in the `InvokeModel` request. Since we are using the Amazon Nova Pro model on Amazon Bedrock, your model input might look like the following:\n",
    "\n",
    "```python\n",
    "message_list = [{\"role\": \"user\", \"content\": [{\"text\": original_text}]}]\n",
    "inf_params = {\"max_new_tokens\": 500, \"top_p\": 0.9, \"top_k\": 20, \"temperature\": 0.7}\n",
    "system_list = [\n",
    "    {\n",
    "        \"text\": \"Write an accurate 250 word gender-neutral summary of the following text without adding preamble or additonal information not present in the original text\"\n",
    "    }\n",
    "]\n",
    "\n",
    "record = {\n",
    "    \"recordId\": \"CALL0000001\", \n",
    "    \"modelInput\": {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"system\": system_list,\n",
    "        \"messages\": message_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef383ed",
   "metadata": {},
   "source": [
    "#### Download the sample synthetic dataset\n",
    "If you do not have a dataset but want to try out Batch Inference for Amazon Bedrock, you can use the synthetic call data available in the dataset directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ec54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the zip file\n",
    "zip_file_path = './dataset/synthetic_call_transcript_data.zip'\n",
    "\n",
    "# Set the path to the destination folder\n",
    "dest_folder_path = './unzipped_transcripts'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(dest_folder_path):\n",
    "    os.makedirs(dest_folder_path)\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all files to the destination folder\n",
    "    zip_ref.extractall(dest_folder_path)\n",
    "\n",
    "print(f\"Files extracted to {dest_folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9dcac4",
   "metadata": {},
   "source": [
    "Let's now construct the jsonl file with the synthetic dataset in the appropriate format that Nova models can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_for_batch_inference(input_folder, output_file):\n",
    "    inf_params = {\"max_new_tokens\": 500, \"top_p\": 0.9, \"top_k\": 20, \"temperature\": 0.7}\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": \"Write an accurate 250 word gender-neutral summary of the following text without adding preamble or additional information not present in the original text\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for filename in tqdm(os.listdir(input_folder)):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(input_folder, filename)\n",
    "                record_id = os.path.splitext(filename)[0]\n",
    "\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                        content = infile.read().strip()\n",
    "\n",
    "                    message_list = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"text\": content\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "                    # https://docs.aws.amazon.com/nova/latest/userguide/complete-request-schema.html\n",
    "                    json_object = {\n",
    "                        \"recordId\": record_id,\n",
    "                        \"modelInput\": {\n",
    "                            \"schemaVersion\": \"messages-v1\", \n",
    "                            \"system\": system_list,\n",
    "                            \"messages\": message_list,\n",
    "                            \"inferenceConfig\": inf_params,\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    json.dump(json_object, outfile, ensure_ascii=False)\n",
    "                    outfile.write('\\n')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586283be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = './unzipped_transcripts/textual' # the folder that contains the\n",
    "jsonl_file = \"raw.jsonl\"\n",
    "create_jsonl_for_batch_inference(input_folder, jsonl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d5bf1-4ad3-4491-a8b2-4703bc830c69",
   "metadata": {},
   "source": [
    "### Uploading to Amazon S3\n",
    "\n",
    "The `upload_to_s3` function uploads a file or directory to an AWS S3 bucket. It takes three arguments:\n",
    "\n",
    "1. `path`: The path to the file or directory to be uploaded.\n",
    "2. `bucket_name`: The name of the S3 bucket.\n",
    "3. `bucket_subfolder` (optional): The name of the subfolder within the S3 bucket where the prepared data should be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cbedd-f505-495d-b40a-0ecf572fd5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(path, bucket_name, bucket_subfolder=None):\n",
    "    # check if the path is a file\n",
    "    if os.path.isfile(path):\n",
    "        # If the path is a file, upload it directly\n",
    "        object_name = os.path.basename(path) if bucket_subfolder is None else f\"{bucket_subfolder}/{os.path.basename(path)}\"\n",
    "        try:\n",
    "            s3.upload_file(path, bucket_name, object_name)\n",
    "            print(f\"Successfully uploaded {path} to {bucket_name}/{object_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {path} to S3: {e}\")\n",
    "            return False\n",
    "    elif os.path.isdir(path):\n",
    "        # If the path is a directory, recursively upload all files within it\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, path)\n",
    "                object_name = relative_path if bucket_subfolder is None else f\"{bucket_subfolder}/{relative_path}\"\n",
    "                try:\n",
    "                    s3.upload_file(file_path, bucket_name, object_name)\n",
    "                    # print(f\"Successfully uploaded {file_path} to {bucket_name}/{object_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading {file_path} to S3: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"{path} is not a file or directory.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b688b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploads the data from local to S3 bucket for batch inference\n",
    "upload_to_s3(path=f\"./{jsonl_file}\", \n",
    "             bucket_name=bucket_name, \n",
    "             bucket_subfolder=job_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586aacfb",
   "metadata": {},
   "source": [
    "## Creating the Batch Inference Job\n",
    "\n",
    "Once the data is prepared and uploaded to an Amazon S3 bucket, you can create the batch inference job.\n",
    "\n",
    "### Configuring Input and Output Data\n",
    "\n",
    "Before submitting the batch inference job, you need to configure the input and output data locations in Amazon S3. This is done using the `inputDataConfig` and `outputDataConfig` parameters.\n",
    "\n",
    "The `inputDataConfig` specifies the Amazon S3 URI where the prepared input data (JSONL file) is stored and, the `outputDataConfig` specifies the Amazon S3 URI where the processed output data will be stored by the batch inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a50314",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{bucket_name}/{job_s3_prefix}/{jsonl_file}\"\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{bucket_name}/{job_s3_prefix}/{output_prefix}/\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd1951",
   "metadata": {},
   "source": [
    "Sanity check the Input and Output configs to ensure the S3Uri Paths are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f10c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputDataConfig)\n",
    "print(outputDataConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61761f",
   "metadata": {},
   "source": [
    "### Create the Amazon Bedrock Batch Service Role\n",
    "Bedrock Batch Inference requires a service role so that it can access and write to S3 on your behalf. You can create the service role manually [see here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html). You may use the below code or use the AWS Console workflow which can create one for you [here](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/batch-inference/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52391c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS account ID\n",
    "sts_client = session.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "# Create IAM client\n",
    "iam_client = session.client('iam')\n",
    "\n",
    "# Define the role name\n",
    "role_name = \"AmazonNovaBedrockBatchServiceRole\"\n",
    "\n",
    "# Define the policy document\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{bucket_name}\",\n",
    "                f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "                # Add another 2 lines if your output bucket is different from input bucket see here for reference: https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html\n",
    "            ],\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:ResourceAccount\": account_id\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define the trust relationship\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": account_id\n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-invocation-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Check if the role already exists\n",
    "try:\n",
    "    existing_role = iam_client.get_role(RoleName=role_name)\n",
    "    print(f\"Role '{role_name}' already exists. Skipping creation.\")\n",
    "    roleArn = existing_role['Role']['Arn']\n",
    "except iam_client.exceptions.NoSuchEntityException:\n",
    "    # Create the role\n",
    "    try:\n",
    "        create_role_response = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_relationship),\n",
    "            Description=\"Service role for Amazon Nova Bedrock Batch inference\"\n",
    "        )\n",
    "        \n",
    "        # Attach the inline policy to the role\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=\"NovaBedrockBatchServiceRolePolicy\",\n",
    "            PolicyDocument=json.dumps(policy_document)\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully created role: {create_role_response['Role']['Arn']}\")\n",
    "        roleArn = create_role_response['Role']['Arn']\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating role: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06129a4a",
   "metadata": {},
   "source": [
    "### Submitting the Batch Inference Job\n",
    "\n",
    "To submit the batch inference job, you use the `create_model_invocation_job` API from the Amazon Bedrock client. This API requires the following parameters:\n",
    "\n",
    "- `roleArn`: The Amazon Resource Name (ARN) of the IAM role with permissions to invoke the batch inference API for Amazon Bedrock.\n",
    "- `modelId`: The ID of the model you want to use for batch inference (e.g., `amazon.nova-lite-v1:0` or other modelIds from [here](https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html)).\n",
    "- `jobName`: A name for your batch inference job.\n",
    "- `inputDataConfig`: The configuration for the input data, as defined in the previous step.\n",
    "- `outputDataConfig`: The configuration for the output data, as defined in the previous step.\n",
    "\n",
    "The API call returns a response containing the ARN of the submitted batch inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=bedrock.create_model_invocation_job(\n",
    "    roleArn=roleArn,\n",
    "    modelId=modelId,\n",
    "    jobName=jobName,\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d3b15",
   "metadata": {},
   "source": [
    "### Monitoring Job Status\n",
    "\n",
    "After submitting the batch inference job, you can monitor its status using the `get_model_invocation_job` API from the Amazon Bedrock client. This API requires the `jobIdentifier` parameter, which is the ARN of the submitted job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ed4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobArn = response.get('jobArn')\n",
    "job_id = jobArn.split('/')[1]\n",
    "\n",
    "print(jobName)\n",
    "\n",
    "status = ''\n",
    "while status not in ['Completed', 'Failed']:\n",
    "    job_response = bedrock.get_model_invocation_job(jobIdentifier=jobArn)\n",
    "    status = job_response['status']\n",
    "    if status == 'Failed':\n",
    "        print(job_response)\n",
    "    elif status == 'Completed':\n",
    "        print(datetime.now(), \": \", status)\n",
    "        break\n",
    "    else: \n",
    "        print(datetime.now(), \": \", status)\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20b17a-ba72-48d8-91d0-5ab51e741764",
   "metadata": {},
   "source": [
    "## Adding helper functions to work with the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63b716-534d-4fab-a94d-48e84b16f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "def pretty_llm_print(text_output, title=None):\n",
    "    \"\"\"Generates formatted output showing the prompt once and the outpus.\"\"\"\n",
    "    # Header styling\n",
    "    header = \"\"\n",
    "    if title:\n",
    "        header = f\"\"\"<div style='border: 2px solid #000000; \n",
    "            padding: 10px; \n",
    "            border-radius: 5px; \n",
    "            max-width: fit-content; \n",
    "            margin: 0 auto; \n",
    "            text-align: center; \n",
    "            font-weight: bold;'>{title}</div>\\n\"\"\"\n",
    "\n",
    "    body_parts = [header]\n",
    "    prompt_texts = text_output['output']['message']['role']\n",
    "    body_parts.append(f\"\\n**{prompt_texts.capitalize()}**\\n\\n\")\n",
    "    assistant_content = text_output['output']['message']['content']\n",
    "\n",
    "    for content in assistant_content:\n",
    "        if 'text' in content:\n",
    "            processed = process_content_string(content['text'])\n",
    "            body_parts.append(processed)\n",
    "\n",
    "    usage = text_output.get('usage')\n",
    "    if usage:\n",
    "        pretty_usage = json.dumps(usage, indent=2)\n",
    "        # Add a header and a Markdown code block for the usage info\n",
    "        processed = formatted_usage = process_usage_dict(usage)\n",
    "        body_parts.append(processed)\n",
    "\n",
    "    # Final styling\n",
    "    styled_markdown = f\"\"\"\n",
    "<div style=\"border: 2px solid #FFC000; \n",
    "    padding: 10px; \n",
    "    border-radius: 5px; \n",
    "    max-width: 100%;\">\n",
    "{''.join(body_parts)}\n",
    "</div>\"\"\"\n",
    "    display(Markdown(styled_markdown))\n",
    "\n",
    "def process_content_string(text):\n",
    "    \"\"\"Format thinking/answer blocks\"\"\"\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    \n",
    "    answer_style = \"\"\"<div style=\"background-color: #e8f5e9; \n",
    "        border-left: 4px solid #43a047; \n",
    "        padding: 10px; \n",
    "        margin: 10px 0; \n",
    "        border-radius: 4px;\">\n",
    "        <strong style=\"color: #43a047;\">Summary</strong>\n",
    "        <div style=\"margin-top: 8px; white-space: pre-wrap;\">{}</div>\n",
    "    </div>\"\"\"\n",
    "\n",
    "    # Convert <answer> tags to summary blocks\n",
    "    text = re.sub(r'<answer>(.*?)</answer>', \n",
    "                 lambda m: answer_style.format(m.group(1)), \n",
    "                 text, \n",
    "                 flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_usage_dict(text):\n",
    "    \"\"\"Format thinking/answer blocks or usage blocks\"\"\"\n",
    "    # Format usage dictionary to have each metric on a new line with thinking style\n",
    "    thinking_style = \"\"\"<div style=\"background-color: #f8f9fa; \n",
    "        border-left: 4px solid #4a90e2; \n",
    "        padding: 10px; \n",
    "        margin: 10px 0; \n",
    "        border-radius: 4px;\">\n",
    "        <strong style=\"color: #4a90e2;\">Usage</strong>\n",
    "        <div style=\"margin-top: 8px; white-space: pre-wrap;\">{}</div>\n",
    "    </div>\"\"\"\n",
    "    if isinstance(text, dict):\n",
    "        usage_str = '\\n'.join([f'{k}: {v}' for k, v in text.items()])\n",
    "    else:\n",
    "        usage_str = str(text)\n",
    "    return thinking_style.format(usage_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62690e19",
   "metadata": {},
   "source": [
    "## Retrieving and Analyzing Output\n",
    "\n",
    "When your batch inference job is complete, Amazon Bedrock creates a dedicated folder in the specified S3 bucket, using the job ID as the folder name. This folder contains a summary of the batch inference job, along with the processed inference data in JSONL format.\n",
    "\n",
    "### Accessing and Understanding Output Format\n",
    "\n",
    "The output files contain the processed text, observability data, and the parameters used for inference. The format of the output data will depend on the model you used for batch inference. The notebook provides an example of how to access and process this information from the output JSONL file for Amazon Nova models.\n",
    "\n",
    "Additionally, in the output location specified for your batch inference job, you'll find a `manifest.json.out` file that provides a summary of the processed records. This file includes information such as the total number of records processed, the number of successfully processed records, the number of records with errors, and the total input and output token counts. For example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"totalRecordCount\": 1100,\n",
    "  \"processedRecordCount\": 1100,\n",
    "  \"successRecordCount\": 1100,\n",
    "  \"errorRecordCount\": 0,\n",
    "  \"inputTokenCount\": 1362960,\n",
    "  \"outputTokenCount\": 142037\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec4371",
   "metadata": {},
   "source": [
    "Let's download the final output from the batch job and read the first line as a sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c27f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the S3 bucket name and prefix for the text files. \n",
    "# Last part in the path is the batch job's job id\n",
    "prefix = f\"{job_s3_prefix}/{output_prefix}/{job_id}/\"\n",
    "\n",
    "# Set local file path\n",
    "local_file_path = f\"./{jsonl_file}.out\"\n",
    "\n",
    "# Download the JSON file from S3\n",
    "try:\n",
    "    object_key = f\"{prefix}{jsonl_file}.out\"\n",
    "    s3.download_file(bucket_name, object_key, local_file_path)\n",
    "    print(f\"Successfully downloaded file from S3 to {local_file_path}\")\n",
    "\n",
    "    # Read and print only the first line of the file\n",
    "    with open(local_file_path, 'r') as file:\n",
    "        first_line = file.readline().strip()\n",
    "        try:\n",
    "            data = json.loads(first_line)\n",
    "            model_output = data.get('modelOutput', {})\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON in the first line: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f209d0e-62d7-4192-a63f-33ae83495cb7",
   "metadata": {},
   "source": [
    "Display output accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc295a7-3e81-4a3f-948e-969f767d6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_llm_print(model_output, title=\"Call center transcript summarization by \" + modelId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f16b7",
   "metadata": {},
   "source": [
    "### Integrating with Existing Workflows\n",
    "\n",
    "After retrieving the processed output data, you can integrate it into your existing workflows or analytics systems for further analysis or downstream processing. For example, you could:\n",
    "\n",
    "- Store the summarized transcripts in a database for easy access and querying.\n",
    "- Perform sentiment analysis or topic modeling on the summarized transcripts to gain additional insights.\n",
    "- Categorize the summarizes into actionable business buckets and develop anomaly detection.\n",
    "- Develop dashboards or reports to visualize and analyze the summarized data.\n",
    "\n",
    "The specific integration steps will depend on your existing workflows and systems, but the processed output data from the batch inference job can be easily incorporated into various data pipelines and analytics processes.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The notebook covers the entire process, from data preparation and formatting to job submission, output retrieval, and integration with existing workflows. By implementing batch inference for call transcript summarization, you can streamline your analysis processes and gain a competitive edge in understanding customer needs and improving your call center operations.\n",
    "\n",
    "Feel free to adapt and extend this notebook to suit your specific requirements, and explore other use cases where batch inference can be applied to optimize your interactions with foundation models at scale."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
