# Amazon Nova Citations 

## Overview

Citations are crucial data points to ensure relevance, accuracy and reliability of the content generated by large language models.
In the example below, we demonstrate how Nova can be prompted to provide citations. The code includes a sample PDF that is being passed into the context.

**Filename:** `nova_client.py`

To evaluate the behavior of this code, you can utilize the below notebook.
**Filename:** `evaluate_with_create_evaluation_job.ipynb`


## Goals
- Provide an example usage with a standalone python code that can be tested in local development environments as well
- Provide mechanism to evaluate the effectiveness of the custom prompting using Bedrock Evaluation jobs

## Repository Contents
- `nova_client.py`: A standalone python script which uploads the [pdf](./All Amazon Shareholder Letters-1997) to an S3 bucket and generates output with citations from the pdf for the question "What contributed to net sales and decline of stock price?".
- `screenshots/`: Folder shows the screen shots from Bedrock Console
- `README.md`: This file, which explains this project and how to use it


## Usage
To use nova_client.py, ensure you set the below ENV Variables
1. DATA_BUCKET = <set it to the bucket where you would like to upload the PDF files>
2. If you intend to change the user prompt, update the variable "question_with_template" in main()
3. Toggle the value of MODEL_TO_TEST to the model you would like to test with. The current setting works with "amazon.nova-pro-v1:0"

To use the evaluation notebook(evaluate_with_create_evaluation_job.ipynb)
1. Upload the [evaluation notebook](evaluate_with_create_evaluation_job.ipynb) and the [prompts](eval_dataset.jsonl) to SageMaker AI -> Juypter Space
2. Ensure pre-requisites are met
3. The last cell in the notebook can be run multiple times to get the status. 
Optionally you can get the details of the job through [console](./screenshots/Bedrock-create-evaluation-job-console.png)
Once the evaluation completes, you can observe the various evaluation metrics either as a csv output or within the bedrock console.
The console also provides a convenient graph showing the performance of the model against various benchmarks.
Particularly for citations, our focus is on the [faitfulness metric](./screenshots/PromptResponseAndFaithFulnessEvidence.png)