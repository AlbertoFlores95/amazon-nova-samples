{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83cd93c5",
   "metadata": {},
   "source": [
    "# Nova Prompt Optimizer\n",
    "\n",
    "Nova Prompt Optimizer automatically optimizes your prompts for Amazon Nova models using your own datasets. This revolutionary solution transforms the lengthy manual migration process into an efficient, data-driven workflowâ€”helping you unlock Nova's full potential faster than ever before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6e0e5-e71f-4eed-adbb-98546efded15",
   "metadata": {},
   "source": [
    "## Section 1: Setup Nova Prompt Optimizer\n",
    "\n",
    "### Setup AWS Credentials\n",
    "To execute the SDK, you will need AWS credentials configured. Take a look at the [AWS CLI configuration documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#config-settings-and-precedence) for details on the various ways to configure credentials. An easy way to try out the SDK is to populate the following environment variables with your AWS API credentials. Take a look at this guide for [Authenticating with short-term credentials for the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-authentication-short-term.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Setup your AWS Access Key and Secret Key as environment variables.\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"Your Access Key\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"Your Secret Key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db810145",
   "metadata": {},
   "source": [
    "### Install the NovaPromptOptimizer SDK\n",
    "\n",
    "pip install the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5668aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nova-prompt-optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc38c9-7b03-4c7b-824c-dac92201aa15",
   "metadata": {},
   "source": [
    "## Section 2: Initialize the Input Adapters\n",
    "\n",
    "![adapters](docs/adapters.png)\n",
    "\n",
    "### Dataset Adapter\n",
    "\n",
    "Initialize the Dataset Adapter that takes the input_columns and output_columns. We use the JSONDatasetAdapter to read a `.jsonl` file and adapt it to the standardized format. Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da047488-0220-4e49-b8d0-361c5afcdfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.dataset_adapter import JSONDatasetAdapter\n",
    "\n",
    "input_columns = {\"input\"}\n",
    "output_columns = {\"answer\"}\n",
    "\n",
    "dataset_adapter = JSONDatasetAdapter(input_columns, output_columns)\n",
    "\n",
    "# Adapt\n",
    "dataset_adapter.adapt(\"data/FacilitySupportAnalyzer.jsonl\")\n",
    "\n",
    "# Create Train and Test Splits. Train Set is used for Optimization, Test Set is used for Evaluation\n",
    "train_set, test_set = dataset_adapter.split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22305416-5912-429f-b47b-04592a0966da",
   "metadata": {},
   "source": [
    "### Prompt Adapter\n",
    "\n",
    "Initialize the Prompt Adapter for the Original Prompt. For this example, we use the FacilitySupportAnalyzer Original Prompt in the `.txt` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621dc01d-6896-4a37-ad64-63f27c597f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.prompt_adapter import TextPromptAdapter\n",
    "\n",
    "prompt_variables = input_columns\n",
    "\n",
    "prompt_adapter = TextPromptAdapter()\n",
    "\n",
    "prompt_adapter.set_user_prompt(file_path=\"original_prompt/user_prompt_template.txt\", variables=prompt_variables)\n",
    "\n",
    "# Adapt\n",
    "prompt_adapter.adapt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349aeee-4f9a-49e0-94cb-029f2c4fb05f",
   "metadata": {},
   "source": [
    "### Metric Adapter\n",
    "\n",
    "Initialize the Metric Adapter for evaluating this prompt for certain optimizers. For this example, we build a Custom Metric for the Facility Support Analyzer Dataset. The metric adapter requires the use of the `apply` [For single row evaluation] or `batch_apply` [For evaluating the whole dataset together] function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093a481-31a0-4871-99a0-670a60d67b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.metric_adapter import MetricAdapter\n",
    "from typing import List, Any, Dict\n",
    "import re\n",
    "import json\n",
    "\n",
    "class FacilitySupportAnalyzerMetric(MetricAdapter):\n",
    "    def parse_json(self, input_string: str):\n",
    "        \"\"\"\n",
    "        Attempts to parse the given string as JSON. If direct parsing fails,\n",
    "        it tries to extract a JSON snippet from code blocks formatted as:\n",
    "            ```json\n",
    "            ... JSON content ...\n",
    "            ```\n",
    "        or any code block delimited by triple backticks and then parses that content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json.loads(input_string)\n",
    "        except json.JSONDecodeError as err:\n",
    "            error = err\n",
    "\n",
    "        patterns = [\n",
    "            re.compile(r\"```json\\s*(.*?)\\s*```\", re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r\"```(.*?)```\", re.DOTALL)\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = pattern.search(input_string)\n",
    "            if match:\n",
    "                json_candidate = match.group(1).strip()\n",
    "                try:\n",
    "                    return json.loads(json_candidate)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        raise error\n",
    "\n",
    "    def _calculate_metrics(self, y_pred: Any, y_true: Any) -> Dict:\n",
    "        strict_json = False\n",
    "        result = {\n",
    "            \"is_valid_json\": False,\n",
    "            \"correct_categories\": 0.0,\n",
    "            \"correct_sentiment\": False,\n",
    "            \"correct_urgency\": False,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            y_true = y_true if isinstance(y_true, dict) else (json.loads(y_true) if strict_json else self.parse_json(y_true))\n",
    "            y_pred = y_pred if isinstance(y_pred, dict) else (json.loads(y_pred) if strict_json else self.parse_json(y_pred))\n",
    "        except json.JSONDecodeError:\n",
    "            result[\"total\"] = 0\n",
    "            return result  # Return result with is_valid_json = False\n",
    "        else:\n",
    "            result[\"is_valid_json\"] = True\n",
    "\n",
    "            categories_true = y_true.get(\"categories\", {})\n",
    "            categories_pred = y_pred.get(\"categories\", {})\n",
    "\n",
    "            if isinstance(categories_true, dict) and isinstance(categories_pred, dict):\n",
    "                correct = sum(\n",
    "                    categories_true.get(k, False) == categories_pred.get(k, False)\n",
    "                    for k in categories_true\n",
    "                )\n",
    "                result[\"correct_categories\"] = correct / len(categories_true) if categories_true else 0.0\n",
    "            else:\n",
    "                result[\"correct_categories\"] = 0.0  # or raise an error if you prefer\n",
    "\n",
    "            result[\"correct_sentiment\"] = y_pred.get(\"sentiment\", \"\") == y_true.get(\"sentiment\", \"\")\n",
    "            result[\"correct_urgency\"] = y_pred.get(\"urgency\", \"\") == y_true.get(\"urgency\", \"\")\n",
    "\n",
    "        # Compute overall metric score\n",
    "        result[\"total\"] = sum(\n",
    "            float(result[k]) for k in [\"correct_categories\", \"correct_sentiment\", \"correct_urgency\"]\n",
    "        ) / 3.0\n",
    "\n",
    "        return result\n",
    "\n",
    "    def apply(self, y_pred: Any, y_true: Any):\n",
    "        return self._calculate_metrics(y_pred, y_true)\n",
    "\n",
    "    def batch_apply(self, y_preds: List[Any], y_trues: List[Any]):\n",
    "        evals = [self.apply(y_pred, y_true) for y_pred, y_true in zip(y_preds, y_trues)]\n",
    "        float_keys = [k for k, v in evals[0].items() if isinstance(v, (int, float, bool))]\n",
    "        return {k: sum(e[k] for e in evals) / len(evals) for k in float_keys}\n",
    "\n",
    "metric_adapter = FacilitySupportAnalyzerMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852c904-42e8-46a2-ad9e-a2e88d8eba6a",
   "metadata": {},
   "source": [
    "### Inference Adapter\n",
    "Initialize the InferenceAdapter to choose the backend Inference. Currently, we only support BedrockInferenceAdapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea654ddd-59d0-495e-8e56-29fe0ed6dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.inference.adapter import BedrockInferenceAdapter\n",
    "\n",
    "inference_adapter = BedrockInferenceAdapter(region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb20883-793c-4b38-8e2c-7f2d461cc2fb",
   "metadata": {},
   "source": [
    "## Section 3: Evaluate the Original Prompt\n",
    "\n",
    "### Evaluator\n",
    "\n",
    "The Evaluator can use the metric_adapter, prompt_adapter, and dataset_adapter to evaluate the prompt given the `model_id` to produce an evaluation score. The Evaluator internally uses the `InferenceRunner` to first generate inference results and then evaluate the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb5ac6",
   "metadata": {},
   "source": [
    "#### Base Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f158430-ed74-470c-8657-2b49b57ae79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.evaluation import Evaluator\n",
    "\n",
    "evaluator = Evaluator(prompt_adapter, test_set, metric_adapter, inference_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33974a07-c556-4238-a39c-12601fa01e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_prompt_score = evaluator.aggregate_score(model_id=\"us.amazon.nova-lite-v1:0\")\n",
    "\n",
    "print(f\"Original Prompt Evaluation Score = {original_prompt_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa60377",
   "metadata": {},
   "source": [
    "## Section 4: Optimize the Prompt\n",
    "\n",
    "### Create a Numerical Metric using the Custom Metric\n",
    "\n",
    "Optimization run requires numerical metrics to quantify the optimization and produce the best results. We slightly update our metrics `apply` function to provide us a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46adefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacilitySupportAnalyzerNovaPromptOptimizerMetric(FacilitySupportAnalyzerMetric):\n",
    "    def apply(self, y_pred: Any, y_true: Any):\n",
    "        # Requires to return a value and not a JSON payload\n",
    "        return self._calculate_metrics(y_pred, y_true)[\"total\"]\n",
    "        \n",
    "    def batch_apply(self, y_preds: List[Any], y_trues: List[Any]):\n",
    "        pass\n",
    "    \n",
    "nova_prompt_optimizer_metric_adapter = FacilitySupportAnalyzerNovaPromptOptimizerMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab372d4-8ffc-4104-a5f6-30160a48f9f6",
   "metadata": {},
   "source": [
    "### Optimization Adapter\n",
    "\n",
    "We can now define the Optimization Functions. The Optimization function takes as input the Prompt Adapter and Inference Adapter and Optionally a Dataset Adapter and Metric Adapter. The optimization function optimizes the prompt and returns a Prompt Adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbc2fe-e95e-4cc0-9087-eaf0b111aa37",
   "metadata": {},
   "source": [
    "### Nova Prompt Optimizer\n",
    "\n",
    "NovaPromptOptimizer is a combination of Meta Prompting using the Nova Guide on prompting and DSPy's MIPROv2 Optimizer using Nova Prompting Tips. NovaPromptOptimizer first runs a meta prompter to identify system instructions and user template from the prompt adapter. Then MIPROv2 is run on top of this to optimize system instructions and identify few-shot samples that need to be added. The few shot samples are added as converse format so they are added as User/Assistant turns.\n",
    "\n",
    "**Requirements:** NovaPromptOptimizer requires Prompt Adapter, Dataset Adapter, Metric Adapter and Inference Adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.optimizers import NovaPromptOptimizer\n",
    "\n",
    "# We provide the train_set here\n",
    "nova_prompt_optimizer = NovaPromptOptimizer(prompt_adapter=prompt_adapter, inference_adapter=inference_adapter, dataset_adapter=train_set, metric_adapter=nova_prompt_optimizer_metric_adapter)\n",
    "\n",
    "# Since we are using Nova Lite, we will use \"lite\" mode for optimization\n",
    "optimized_prompt_adapter = nova_prompt_optimizer.optimize(mode=\"lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbfc1f",
   "metadata": {},
   "source": [
    "### Optimized System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt_adapter.system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8c6b5",
   "metadata": {},
   "source": [
    "### Optimized User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt_adapter.user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb22d1",
   "metadata": {},
   "source": [
    "### Few Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Few-Shot Examples = {len(optimized_prompt_adapter.few_shot_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only the first example\n",
    "print(optimized_prompt_adapter.few_shot_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44cf06",
   "metadata": {},
   "source": [
    "### Save the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt_adapter.save(\"optimized_prompt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30a347",
   "metadata": {},
   "source": [
    "## Section 5: Evaluate the Optimized Prompt\n",
    "\n",
    "### Evaluator\n",
    "\n",
    "Now we evaluate the Nova Prompt Optimizers Optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65493b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.evaluation import Evaluator\n",
    "\n",
    "evaluator = Evaluator(optimized_prompt_adapter, test_set, metric_adapter, inference_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nova_prompt_optimizer_eval_score = evaluator.aggregate_score(model_id=\"us.amazon.nova-lite-v1:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimized Prompt Evaluation Score = {nova_prompt_optimizer_eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b345963-5516-49c8-9b7c-91eccbfe348c",
   "metadata": {},
   "source": [
    "### Save your Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2acd687-d68d-4b09-8bd3-7ae88626c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.save(\"evals/nova_lite/nova_prompt_optimizer_eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7af64",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Congratulations! You have completed the workshop on Nova Prompt Optimization using our new Nova Prompt Optimizer SDK\n",
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "## Next Steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
